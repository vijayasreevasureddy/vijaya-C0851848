{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VIJAYA_C0851848_800_Neurons.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e907af-cb61-40f2-8ba2-577a9a578456"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null\n",
        "!pip install JSAnimation >/dev/null\n",
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 7.4 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=8d3f08f4452253fcbcee3fed29fad636afd13e3cb85f4327833c9a5424f61a2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "83a3adf4-ec53-44b1-d996-7ba71cd200c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "b0af6955-8e2d-417a-f3cb-f09d82c6dba8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "47ecb4cc-bba4-4b0a-a832-655ea89c2567",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "021447a4-2acb-4989-c604-227fd0b82dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 800 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "6d10146c-61a9-4f23-e069-74927035bda7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -19.020000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -19.029800\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.049502\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -19.069007\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.088317\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -19.097434\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -19.106459\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.125395\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -19.124141\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.142899\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -19.161470\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.179856\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -19.198057\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.216077\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -19.223916\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -19.231677\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -19.249360\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -19.256866\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -19.264298\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -19.281655\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -19.288838\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.305950\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -19.312890\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.329761\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.346464\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -19.352999\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -19.349469\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -19.345974\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -19.352515\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.358990\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -19.375400\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.391646\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -19.407729\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -19.413652\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -19.419515\n",
            "resetting env. episode 37.000000, reward total was -18.000000. running mean: -19.405320\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -19.401267\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -19.407254\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.423182\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.438950\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -19.454560\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -19.460015\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -19.465415\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -19.470761\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.486053\n",
            "resetting env. episode 47.000000, reward total was -19.000000. running mean: -19.481192\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -19.486381\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.501517\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.516502\n",
            "resetting env. episode 51.000000, reward total was -19.000000. running mean: -19.511337\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -19.516223\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.531061\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.545750\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.560293\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.574690\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.588943\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.603054\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -19.597023\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.611053\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -19.604942\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -19.608893\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.622804\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.636576\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.650210\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.663708\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.677071\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.690300\n",
            "resetting env. episode 69.000000, reward total was -17.000000. running mean: -19.663397\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -19.666763\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.680096\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -19.683295\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.696462\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -19.689497\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.702602\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.715576\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.728420\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.741136\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.753725\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.766188\n",
            "resetting env. episode 81.000000, reward total was -18.000000. running mean: -19.748526\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.761040\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.773430\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.785696\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.797839\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -19.809860\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.821762\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -19.823544\n",
            "resetting env. episode 89.000000, reward total was -19.000000. running mean: -19.815309\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.827156\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.838884\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.850495\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.861990\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -19.863370\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.874737\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -19.875989\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -19.877229\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -19.888457\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.899573\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.910577\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -19.911471\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.922356\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -19.923133\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -19.923901\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -19.924662\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -19.925416\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -19.916162\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.927000\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.937730\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -19.938353\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.948969\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -19.949480\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -19.949985\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -19.940485\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.951080\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.961569\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -19.961954\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.972334\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.972611\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -19.972885\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.983156\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -19.993324\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -19.993391\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -19.993457\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -19.993522\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -19.993587\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.003651\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.013615\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.013479\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.023344\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.033110\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.042779\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.052352\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.051828\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.051310\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.060797\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.060189\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.069587\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.068891\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.078202\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.087420\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.086546\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.095680\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.104724\n",
            "resetting env. episode 145.000000, reward total was -18.000000. running mean: -20.083676\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.092840\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.101911\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.110892\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.119783\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.118585\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.127399\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.136125\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.144764\n",
            "resetting env. episode 154.000000, reward total was -19.000000. running mean: -20.133317\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.141983\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.140564\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.149158\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.157666\n",
            "resetting env. episode 159.000000, reward total was -17.000000. running mean: -20.126090\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.124829\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.123580\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.132345\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.141021\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.149611\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.158115\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.166534\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.164868\n",
            "resetting env. episode 168.000000, reward total was -18.000000. running mean: -20.143220\n",
            "resetting env. episode 169.000000, reward total was -18.000000. running mean: -20.121788\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.130570\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.129264\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.137971\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.146592\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.155126\n",
            "resetting env. episode 175.000000, reward total was -19.000000. running mean: -20.143574\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.152139\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.150617\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.149111\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.157620\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.156044\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.154483\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.162939\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.161309\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.169696\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.177999\n",
            "resetting env. episode 186.000000, reward total was -18.000000. running mean: -20.156219\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.154657\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.163110\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.161479\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.159864\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.168266\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.176583\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.184817\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.192969\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.201039\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.199029\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.197039\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.205068\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.213018\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.220888\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.228679\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.226392\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.214128\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.201987\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.209967\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.217867\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.225688\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.233432\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.221097\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.228886\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.226597\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.224331\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.232088\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.239767\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.247370\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.254896\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.252347\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.249823\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.257325\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.254752\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.252204\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.249682\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.257186\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.264614\n",
            "resetting env. episode 225.000000, reward total was -19.000000. running mean: -20.251968\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.259448\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.266853\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.274185\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.281443\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.288629\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.295742\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.302785\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.309757\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.316659\n",
            "resetting env. episode 235.000000, reward total was -18.000000. running mean: -20.293493\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.290558\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.297652\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.304676\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.301629\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.288613\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.295727\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.302769\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.309742\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.316644\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.313478\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.320343\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.327140\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.333868\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.340530\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.347124\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.353653\n",
            "resetting env. episode 252.000000, reward total was -19.000000. running mean: -20.340117\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.346715\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.353248\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.359716\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.366119\n",
            "resetting env. episode 257.000000, reward total was -17.000000. running mean: -20.332457\n",
            "resetting env. episode 258.000000, reward total was -19.000000. running mean: -20.319133\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.315941\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.312782\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.309654\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.306558\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.303492\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.310457\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.317353\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.324179\n",
            "resetting env. episode 267.000000, reward total was -18.000000. running mean: -20.300937\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.287928\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.285049\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.282198\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.289376\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.276482\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.283718\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.290880\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.297972\n",
            "resetting env. episode 276.000000, reward total was -18.000000. running mean: -20.274992\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.262242\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.259620\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.257023\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.254453\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.251909\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.259390\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.266796\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.274128\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.281386\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.288573\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.295687\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.302730\n",
            "resetting env. episode 289.000000, reward total was -19.000000. running mean: -20.289703\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.296806\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.303838\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.300799\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.307791\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.304713\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.311666\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.318549\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.325364\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.332110\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.328789\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.335501\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.342146\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.338725\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.345338\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.351884\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.358365\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.364782\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.371134\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.377423\n",
            "resetting env. episode 309.000000, reward total was -18.000000. running mean: -20.353648\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.350112\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.346611\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.343145\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.339713\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.346316\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.342853\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.349424\n",
            "resetting env. episode 317.000000, reward total was -19.000000. running mean: -20.335930\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.342571\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.349145\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.355654\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.362097\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.368476\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.374791\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.371043\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.367333\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.373660\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.369923\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.356224\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.342662\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.349235\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.345743\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.352285\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.358762\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.365175\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.371523\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.377808\n",
            "resetting env. episode 337.000000, reward total was -18.000000. running mean: -20.354030\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.360489\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.366885\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.363216\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.369584\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.365888\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.362229\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.358607\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.365020\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.361370\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.367757\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.374079\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.380338\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.376535\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.372769\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.379042\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.385251\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.381399\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.377585\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.373809\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.370071\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.366370\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.362707\n",
            "resetting env. episode 360.000000, reward total was -19.000000. running mean: -20.349079\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.335589\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.342233\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.348810\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.355322\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.351769\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.358251\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.354669\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -20.341122\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.347711\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.354234\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.340692\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.347285\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.343812\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.340374\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.346970\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.353500\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.359965\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.366366\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.372702\n",
            "resetting env. episode 380.000000, reward total was -19.000000. running mean: -20.358975\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.365385\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.361731\n",
            "resetting env. episode 383.000000, reward total was -18.000000. running mean: -20.338114\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.334733\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.341386\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.337972\n",
            "resetting env. episode 387.000000, reward total was -18.000000. running mean: -20.314592\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.321446\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.328232\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.324949\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.331700\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.328383\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.335099\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.341748\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.348330\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.354847\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.351299\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.357786\n",
            "resetting env. episode 399.000000, reward total was -19.000000. running mean: -20.344208\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.330766\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.337458\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.334084\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.330743\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.327435\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.324161\n",
            "resetting env. episode 406.000000, reward total was -19.000000. running mean: -20.310919\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.317810\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.324632\n",
            "resetting env. episode 409.000000, reward total was -18.000000. running mean: -20.301386\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.298372\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.305388\n",
            "resetting env. episode 412.000000, reward total was -18.000000. running mean: -20.282334\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.289511\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.296616\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.303650\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.300613\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.307607\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.314531\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.311386\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.318272\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.315089\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.321938\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.328719\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.325432\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.322177\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.318955\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.315766\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.322608\n",
            "resetting env. episode 429.000000, reward total was -19.000000. running mean: -20.309382\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.316288\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.323125\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.329894\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.316595\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.323429\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -20.310195\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.317093\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.323922\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.330683\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.327376\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.334102\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.340761\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.337354\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.343980\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.350540\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.357035\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.363465\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.359830\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.366232\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.372569\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.368844\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.365155\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.371504\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.377789\n",
            "resetting env. episode 454.000000, reward total was -18.000000. running mean: -20.354011\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.360471\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.346866\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.343397\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.339963\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.346564\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.343098\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.349667\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.346170\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.342709\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.339282\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.335889\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.342530\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.349105\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.355614\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.352057\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.358537\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.354951\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.351402\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.357888\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.354309\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.360766\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.367158\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.363487\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.359852\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.346253\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.352791\n",
            "resetting env. episode 481.000000, reward total was -19.000000. running mean: -20.339263\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.345870\n",
            "resetting env. episode 483.000000, reward total was -17.000000. running mean: -20.312412\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.319287\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.316095\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.302934\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.309904\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.306805\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.313737\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.320600\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.327394\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.314120\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.320979\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.317769\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.324591\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.331345\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.338032\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.334652\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.341305\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.347892\n",
            "CPU times: user 1h 28min 45s, sys: 14min 9s, total: 1h 42min 54s\n",
            "Wall time: 53min 23s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "49bbcc30-1fad-4f6f-c73c-bf550399c70a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990297\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.990394\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.990490\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.980585\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.980779\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.980972\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.981162\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.971350\n",
            "resetting env. episode 13.000000, reward total was -17.000000. running mean: -20.931637\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.922320\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.923097\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.923866\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.924628\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.925381\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.926127\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.916866\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.917697\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.918521\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.919335\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.920142\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.920941\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.911731\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.902614\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.893588\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -20.874652\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.875905\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.877146\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -20.858375\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.859791\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.851193\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -20.832681\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.824354\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.816111\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.807950\n",
            "resetting env. episode 39.000000, reward total was -18.000000. running mean: -20.779870\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.772072\n",
            "resetting env. episode 41.000000, reward total was -18.000000. running mean: -20.744351\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.746907\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -20.729438\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.722144\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.714922\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.717773\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.710595\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.713489\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.716355\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.719191\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.721999\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.724779\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.727531\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.720256\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.723053\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.725823\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.718565\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.721379\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.724165\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.716924\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.719754\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.722557\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.715331\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.718178\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.720996\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.723786\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.726548\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.729283\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.721990\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.724770\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.727522\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.730247\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.732945\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.735615\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.738259\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.740877\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.733468\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.736133\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.738772\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.721384\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.714170\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.707029\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.709958\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.702859\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.705830\n",
            "resetting env. episode 86.000000, reward total was -18.000000. running mean: -20.678772\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.681984\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.685164\n",
            "resetting env. episode 89.000000, reward total was -18.000000. running mean: -20.658313\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.661729\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.665112\n",
            "resetting env. episode 92.000000, reward total was -19.000000. running mean: -20.648461\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.651976\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.645457\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.649002\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.642512\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.636087\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.639726\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.643329\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.646896\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.650427\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.653922\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.647383\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.650909\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.654400\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.647856\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.651378\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.654864\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.658315\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.661732\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -20.645115\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.648664\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.642177\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -20.625755\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.619498\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.623303\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.627070\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.630799\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.634491\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.628146\n",
            "resetting env. episode 121.000000, reward total was -19.000000. running mean: -20.611865\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.605746\n",
            "resetting env. episode 123.000000, reward total was -18.000000. running mean: -20.579688\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.573892\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.578153\n",
            "resetting env. episode 126.000000, reward total was -18.000000. running mean: -20.552371\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.556847\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.541279\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.545866\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.540407\n",
            "resetting env. episode 131.000000, reward total was -19.000000. running mean: -20.525003\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -20.509753\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.504656\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.499609\n",
            "resetting env. episode 135.000000, reward total was -16.000000. running mean: -20.454613\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.450067\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.445566\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.451111\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.456600\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.452034\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -20.437513\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.433138\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.438807\n",
            "resetting env. episode 144.000000, reward total was -18.000000. running mean: -20.414419\n",
            "resetting env. episode 145.000000, reward total was -18.000000. running mean: -20.390275\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.396372\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.402408\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.408384\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.404300\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.400257\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.396255\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.402292\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.408269\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.414186\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.420045\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.425844\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.431586\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.437270\n",
            "resetting env. episode 159.000000, reward total was -18.000000. running mean: -20.412897\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.418768\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.414580\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.420435\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.426230\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.421968\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.427748\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.433471\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.439136\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.444745\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.450297\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.435794\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.431436\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.437122\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.442751\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.448323\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.443840\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.439402\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.425008\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.430758\n",
            "resetting env. episode 179.000000, reward total was -17.000000. running mean: -20.396450\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.392485\n",
            "resetting env. episode 181.000000, reward total was -19.000000. running mean: -20.378561\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.384775\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.390927\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.387018\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.373148\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.379416\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.375622\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.381866\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.388047\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.384167\n",
            "resetting env. episode 191.000000, reward total was -17.000000. running mean: -20.350325\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.346822\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.353354\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.359820\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.356222\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.362660\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.369033\n",
            "resetting env. episode 198.000000, reward total was -17.000000. running mean: -20.335343\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.341989\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.348569\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.355084\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.361533\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.357918\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.354338\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.360795\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.367187\n",
            "resetting env. episode 207.000000, reward total was -18.000000. running mean: -20.343515\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.350080\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.346579\n",
            "resetting env. episode 210.000000, reward total was -18.000000. running mean: -20.323113\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.309882\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.306784\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.313716\n",
            "resetting env. episode 214.000000, reward total was -17.000000. running mean: -20.280579\n",
            "resetting env. episode 215.000000, reward total was -19.000000. running mean: -20.267773\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.265095\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.272444\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.279720\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.286922\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.294053\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.301113\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.298102\n",
            "resetting env. episode 223.000000, reward total was -17.000000. running mean: -20.265121\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.272469\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.279745\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.276947\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.284178\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.291336\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.278423\n",
            "resetting env. episode 230.000000, reward total was -19.000000. running mean: -20.265638\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.272982\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.260252\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.257650\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.255073\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.262522\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.259897\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.257298\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.264725\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.272078\n",
            "resetting env. episode 240.000000, reward total was -18.000000. running mean: -20.249357\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.256864\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.244295\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.241852\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.239434\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.237039\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.224669\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.232422\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.240098\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.247697\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.245220\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.252768\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.260240\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.257638\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.265061\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.272411\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.279687\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.276890\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.284121\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.281280\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.288467\n",
            "resetting env. episode 261.000000, reward total was -19.000000. running mean: -20.275582\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.282826\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.289998\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.287098\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.294227\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.301285\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.298272\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.305289\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.312236\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.309114\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.306023\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.302963\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.299933\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.296934\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.303964\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.310925\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.307815\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.314737\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.321590\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.318374\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.315190\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.312038\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.318918\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.315729\n",
            "resetting env. episode 285.000000, reward total was -18.000000. running mean: -20.292572\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.289646\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.296749\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.303782\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.310744\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.297637\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.294660\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.291714\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.298796\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.305809\n",
            "resetting env. episode 295.000000, reward total was -18.000000. running mean: -20.282750\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.279923\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.267124\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.264452\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.261808\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.269190\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.276498\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.283733\n",
            "resetting env. episode 303.000000, reward total was -19.000000. running mean: -20.270896\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.278187\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.265405\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.262751\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.270123\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.267422\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.254748\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.262200\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.269578\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.256883\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.264314\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.271671\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.278954\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.276164\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.283403\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.270569\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.267863\n",
            "resetting env. episode 320.000000, reward total was -18.000000. running mean: -20.245184\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.252733\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.250205\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.237703\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.245326\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.252873\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.260344\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.267741\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.255063\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.262513\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.259888\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.257289\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.254716\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.262169\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.269547\n",
            "resetting env. episode 335.000000, reward total was -19.000000. running mean: -20.256851\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.264283\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.261640\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.259024\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.266433\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.263769\n",
            "resetting env. episode 341.000000, reward total was -18.000000. running mean: -20.241131\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.248720\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.256233\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.253671\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.261134\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.258523\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.265937\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.273278\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.270545\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.277840\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.275061\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.272311\n",
            "resetting env. episode 353.000000, reward total was -18.000000. running mean: -20.249588\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.257092\n",
            "resetting env. episode 355.000000, reward total was -18.000000. running mean: -20.234521\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.232176\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.229854\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.227555\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.235280\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.232927\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.230598\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.228292\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.236009\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.233649\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.231312\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.238999\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.246609\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.244143\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.241702\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.249285\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.256792\n",
            "resetting env. episode 372.000000, reward total was -19.000000. running mean: -20.244224\n",
            "resetting env. episode 373.000000, reward total was -18.000000. running mean: -20.221782\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.229564\n",
            "resetting env. episode 375.000000, reward total was -18.000000. running mean: -20.207268\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.205195\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.213143\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.221012\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.228802\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.236514\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.234149\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.231807\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.229489\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.237194\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.224822\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.212574\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.210448\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.218344\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.226160\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.223899\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.231660\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.239343\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.246950\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.254480\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.251936\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.249416\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.256922\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.254353\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.261809\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.249191\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.236699\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.244332\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.251889\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.259370\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.256776\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.254209\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.261667\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.249050\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.236559\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.244194\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.241752\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.249334\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.256841\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.244273\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.251830\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -20.239312\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.246918\n",
            "resetting env. episode 418.000000, reward total was -19.000000. running mean: -20.234449\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.242105\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.239684\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.247287\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.244814\n",
            "resetting env. episode 423.000000, reward total was -19.000000. running mean: -20.232366\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.240042\n",
            "resetting env. episode 425.000000, reward total was -17.000000. running mean: -20.207642\n",
            "resetting env. episode 426.000000, reward total was -19.000000. running mean: -20.195565\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.203610\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.211574\n",
            "resetting env. episode 429.000000, reward total was -19.000000. running mean: -20.199458\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.197463\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.185489\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.193634\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.201697\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.209680\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.207584\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.215508\n",
            "resetting env. episode 437.000000, reward total was -19.000000. running mean: -20.203353\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.201319\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.209306\n",
            "resetting env. episode 440.000000, reward total was -18.000000. running mean: -20.187213\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.185341\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.193487\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.191553\n",
            "resetting env. episode 444.000000, reward total was -17.000000. running mean: -20.159637\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.148041\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.156560\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.154995\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.153445\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.151910\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.150391\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.158887\n",
            "resetting env. episode 452.000000, reward total was -17.000000. running mean: -20.127298\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.136025\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.134665\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.123318\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.122085\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.120864\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.129656\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.128359\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.137076\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.135705\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -20.124348\n",
            "resetting env. episode 463.000000, reward total was -18.000000. running mean: -20.103104\n",
            "resetting env. episode 464.000000, reward total was -19.000000. running mean: -20.092073\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.101153\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.100141\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.099140\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.108148\n",
            "resetting env. episode 469.000000, reward total was -19.000000. running mean: -20.097067\n",
            "resetting env. episode 470.000000, reward total was -18.000000. running mean: -20.076096\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.085335\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.084482\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.093637\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.082701\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.081874\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.091055\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.100144\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.099143\n",
            "resetting env. episode 479.000000, reward total was -18.000000. running mean: -20.078151\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.067370\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.076696\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.065929\n",
            "resetting env. episode 483.000000, reward total was -18.000000. running mean: -20.045270\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.054817\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -20.044269\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.053826\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.053288\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.052755\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.062228\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.061605\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.060989\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.070379\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.079676\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.088879\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.097990\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.097010\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.096040\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.105080\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.094029\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.103089\n",
            "CPU times: user 1h 44min 13s, sys: 15min 49s, total: 2h 2s\n",
            "Wall time: 1h 2min 7s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "0e378506-aa2e-41ab-f2bf-fae1ebc45229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ4AAAFZCAYAAABpOsHqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAHHUlEQVR4nO3dO4+cVx3A4TPr9d3xbW0nsWJZ4mKJPg1FGmhIR8cXoKFAqfgCSFRISFBTU8AHiGjoKBBCQkKhIAIhZJE4eO21187aWXuHBiSSUcj+xpd3PX6eanWkM/sf7e5P857VvDObz+cDoFibegDgxSMcQCYcQCYcQCYcQCYcQCYcQCYcQCYcQCYcQCYcQCYcQCYcQLa+7MZvfeX4vt9WuzYb462rR8eJw8+uU69d2Bgnjh1fWL+xuTnu7+zs+3E2zp4ZZ0698sTz3L1/b9y8vfXEj8Pzs/3G+bH9xsbC+ql/3h6n/3FzgomevXfevTVbZt/S4Xj7q4t/pFN67eLFcfHcuYX1+zs7MRxnx9XLl594nusf3hCOF8zdKxvjg69fW1h/9fd/XdlwLMulCpAJB5AJB5AJB5AtfTj6stna3h53t+8trL9y6uQ4d/r0BBPBdIRjnzZvb42/Xb++sH718mXh4KXjUgXIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPI3Mhnn44fOzrOnzmzsH7i2LEJpoFpCcc+Xb50aVy+dGnqMeBAcKkCZMIBZMIBZMIBZCtzOPrxzs64s774dHYfPUqP8+DhJ+PO9vYTz7Pz8METPwbP15F7D8bJDxY/KPzItp/lZ83m8/lSG3/69vnlNsIB9f9+oWfPbYrn6513by311FbmFQc8qVWNw7PgjAPIhAPIlr5Ueev7P3uacwAvkKUPRzc3Nx2OwgtuY2NjqaMdlypAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAtvTb6v/4y588zTmACXzzez9aap97jsJLbNl7jrpUATLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhADLhALL1qQf4PBfOnR2H1w8vrG9ubY1PdncnmAj4rwMbji9fuTJOnzr1qbX5fD7+8N6fhYNktnboP1/Nx3xvb9JZVsWBDQc8DcfPvTq+8YOfj7VDh8fOnX+N3/z4u2Nv9+HUY73whIOVtnZofZx+/Uvj0OEjY/34iTGbzaYeaSU4HAUy4QAy4QAy4WD1zedjPt8bYz71IKvD4SgrbWfro/HrH35njNls7D3aHY/9R+WpEA5W2t6j3XHr7+9NPcbKcakCZMIBZMIBZMIBZMIBZMIBZMIBZMIBZMIBZMIBZMIBZMIBZMIBZMIBZAf2bfUfP3gw1tYWu/b48eMJpgH+14ENx5/+8v7UIwCfw6UKkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkK1PPQC87PbWZuPx0cML67O9+Tj0cHfMJpjpiwgHTOz+6+fG+99+c2H95Ed3xrVf/W6Cib6YcMDE5muzsXdkfYzZp19b7K0f3D9PZxxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAdnBvowwvm/n8swuTjLEfwgETO/nh1vjaL367sL62+2iCafZHOGBih3Yfj5M37kw9RuKMA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8jWl9148dqbT3MO4AUym8/nS228efPmchuBA+PChQuzZfYt/YpjNlvq+wErwBkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkC39uSrAy8srDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiD7N3Q4y5s3awpOAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "a4d917c5-e4b3-4653-f71a-36ecd32626b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 3.000000, reward total was -19.000000. running mean: -19.990000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.000100\n",
            "resetting env. episode 5.000000, reward total was -18.000000. running mean: -19.980099\n",
            "resetting env. episode 6.000000, reward total was -17.000000. running mean: -19.950298\n",
            "resetting env. episode 7.000000, reward total was -19.000000. running mean: -19.940795\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -19.951387\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -19.941873\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -19.952454\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -19.952930\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -19.953401\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.963867\n",
            "resetting env. episode 14.000000, reward total was -18.000000. running mean: -19.944228\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.954786\n",
            "resetting env. episode 16.000000, reward total was -19.000000. running mean: -19.945238\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -19.945785\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -19.946328\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -19.946864\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -19.947396\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -19.947922\n",
            "resetting env. episode 22.000000, reward total was -19.000000. running mean: -19.938442\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -19.939058\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -19.939667\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -19.930271\n",
            "resetting env. episode 26.000000, reward total was -18.000000. running mean: -19.910968\n",
            "resetting env. episode 27.000000, reward total was -19.000000. running mean: -19.901858\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -19.902840\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.913811\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -19.914673\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.915527\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -19.926371\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -19.927108\n",
            "resetting env. episode 34.000000, reward total was -19.000000. running mean: -19.917837\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -19.918658\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -19.919472\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -19.930277\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -19.920974\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -19.931764\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.942447\n",
            "resetting env. episode 41.000000, reward total was -19.000000. running mean: -19.933022\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -19.933692\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -19.934355\n",
            "resetting env. episode 44.000000, reward total was -19.000000. running mean: -19.925012\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.935761\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.946404\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -19.946940\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -19.947470\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -19.937996\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.948616\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -19.949130\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -19.949638\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.960142\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.970540\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -19.970835\n",
            "resetting env. episode 56.000000, reward total was -17.000000. running mean: -19.941127\n",
            "resetting env. episode 57.000000, reward total was -19.000000. running mean: -19.931715\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.942398\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -19.942974\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -19.943545\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.954109\n",
            "resetting env. episode 62.000000, reward total was -18.000000. running mean: -19.934568\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.945222\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.955770\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.966212\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.976550\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -19.966785\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.977117\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.987346\n",
            "resetting env. episode 70.000000, reward total was -19.000000. running mean: -19.977472\n",
            "resetting env. episode 71.000000, reward total was -19.000000. running mean: -19.967698\n",
            "resetting env. episode 72.000000, reward total was -19.000000. running mean: -19.958021\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -19.948440\n",
            "resetting env. episode 74.000000, reward total was -18.000000. running mean: -19.928956\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -19.929666\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.940370\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.950966\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -19.941456\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -19.942042\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.942621\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -19.933195\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -19.933863\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.944525\n",
            "resetting env. episode 84.000000, reward total was -18.000000. running mean: -19.925079\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -19.925829\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.926570\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -19.927305\n",
            "resetting env. episode 88.000000, reward total was -19.000000. running mean: -19.918032\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -19.918851\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.929663\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -19.930366\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -19.931062\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -19.931752\n",
            "resetting env. episode 94.000000, reward total was -19.000000. running mean: -19.922434\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -19.913210\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -19.914078\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -19.904937\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -19.905888\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -19.896829\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -19.897861\n",
            "resetting env. episode 101.000000, reward total was -18.000000. running mean: -19.878882\n",
            "resetting env. episode 102.000000, reward total was -19.000000. running mean: -19.870093\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -19.871392\n",
            "resetting env. episode 104.000000, reward total was -18.000000. running mean: -19.852678\n",
            "resetting env. episode 105.000000, reward total was -19.000000. running mean: -19.844151\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -19.845710\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.847253\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -19.838780\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -19.840393\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -19.841989\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -19.843569\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.855133\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -19.856582\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.868016\n",
            "resetting env. episode 115.000000, reward total was -18.000000. running mean: -19.849336\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.860842\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -19.862234\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -19.863612\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.864976\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -19.876326\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -19.877562\n",
            "resetting env. episode 122.000000, reward total was -18.000000. running mean: -19.858787\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -19.860199\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -19.861597\n",
            "resetting env. episode 125.000000, reward total was -19.000000. running mean: -19.852981\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.864451\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -19.865807\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -19.877149\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -19.868377\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.879693\n",
            "resetting env. episode 131.000000, reward total was -18.000000. running mean: -19.860896\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -19.852287\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -19.863765\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -19.875127\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -19.876376\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -19.887612\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -19.898736\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -19.909748\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -19.920651\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.931444\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -19.942130\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -19.952709\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -19.953182\n",
            "resetting env. episode 144.000000, reward total was -18.000000. running mean: -19.933650\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -19.934313\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -19.924970\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -19.915720\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -19.916563\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -19.917398\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -19.928224\n",
            "resetting env. episode 151.000000, reward total was -18.000000. running mean: -19.908941\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -19.919852\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -19.920654\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -19.931447\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -19.942133\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -19.952711\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -19.963184\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -19.963552\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -19.963917\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -19.974278\n",
            "resetting env. episode 161.000000, reward total was -17.000000. running mean: -19.944535\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -19.945089\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -19.955639\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -19.946082\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -19.956621\n",
            "resetting env. episode 166.000000, reward total was -19.000000. running mean: -19.947055\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -19.957585\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -19.948009\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -19.938529\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -19.939143\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -19.939752\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -19.950354\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -19.950851\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -19.961342\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -19.961729\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -19.952112\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -19.942590\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -19.953165\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -19.953633\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -19.964097\n",
            "resetting env. episode 181.000000, reward total was -19.000000. running mean: -19.954456\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -19.964911\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -19.965262\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -19.955609\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -19.956053\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -19.966493\n",
            "resetting env. episode 187.000000, reward total was -18.000000. running mean: -19.946828\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -19.947360\n",
            "resetting env. episode 189.000000, reward total was -18.000000. running mean: -19.927886\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -19.928607\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -19.939321\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -19.949928\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -19.940429\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -19.941024\n",
            "resetting env. episode 195.000000, reward total was -19.000000. running mean: -19.931614\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -19.922298\n",
            "resetting env. episode 197.000000, reward total was -19.000000. running mean: -19.913075\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -19.913944\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -19.924805\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -19.915557\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -19.926401\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -19.927137\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -19.927866\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -19.928587\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -19.939301\n",
            "resetting env. episode 206.000000, reward total was -17.000000. running mean: -19.909908\n",
            "resetting env. episode 207.000000, reward total was -17.000000. running mean: -19.880809\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -19.892001\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -19.893081\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -19.904150\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -19.915109\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -19.915958\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -19.916798\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -19.907630\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -19.918554\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -19.919368\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -19.930174\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -19.940873\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -19.951464\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -19.941949\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -19.952530\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -19.963005\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -19.963375\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -19.963741\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -19.974103\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -19.984362\n",
            "resetting env. episode 227.000000, reward total was -19.000000. running mean: -19.974519\n",
            "resetting env. episode 228.000000, reward total was -17.000000. running mean: -19.944774\n",
            "resetting env. episode 229.000000, reward total was -18.000000. running mean: -19.925326\n",
            "resetting env. episode 230.000000, reward total was -19.000000. running mean: -19.916073\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -19.916912\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -19.917743\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -19.928565\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -19.929280\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -19.929987\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -19.930687\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -19.931380\n",
            "resetting env. episode 238.000000, reward total was -19.000000. running mean: -19.922066\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -19.922846\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -19.923617\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -19.934381\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -19.945037\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -19.955587\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -19.956031\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -19.946471\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -19.957006\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -19.957436\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -19.967862\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -19.978183\n",
            "resetting env. episode 250.000000, reward total was -18.000000. running mean: -19.958401\n",
            "resetting env. episode 251.000000, reward total was -18.000000. running mean: -19.938817\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -19.939429\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -19.950035\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -19.950534\n",
            "resetting env. episode 255.000000, reward total was -19.000000. running mean: -19.941029\n",
            "resetting env. episode 256.000000, reward total was -16.000000. running mean: -19.901619\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -19.902602\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -19.903576\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -19.894541\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -19.885595\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -19.886739\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -19.897872\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -19.908893\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -19.919804\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -19.920606\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -19.921400\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -19.932186\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -19.942864\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -19.943436\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -19.944001\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -19.944561\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -19.955116\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -19.965564\n",
            "resetting env. episode 274.000000, reward total was -18.000000. running mean: -19.945909\n",
            "resetting env. episode 275.000000, reward total was -18.000000. running mean: -19.926450\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -19.927185\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -19.937913\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -19.948534\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -19.949049\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -19.949558\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -19.950063\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -19.950562\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -19.961057\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -19.971446\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -19.971732\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -19.972014\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -19.962294\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -19.962671\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -19.973044\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -19.973314\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -19.973581\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -19.983845\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -19.974007\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -19.974267\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -19.974524\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -19.974779\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -19.985031\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -19.995181\n",
            "resetting env. episode 299.000000, reward total was -19.000000. running mean: -19.985229\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -19.985376\n",
            "resetting env. episode 301.000000, reward total was -17.000000. running mean: -19.955523\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -19.955967\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -19.966408\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -19.976744\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -19.986976\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -19.987107\n",
            "resetting env. episode 307.000000, reward total was -19.000000. running mean: -19.977235\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -19.987463\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -19.987588\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -19.987713\n",
            "resetting env. episode 311.000000, reward total was -18.000000. running mean: -19.967835\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -19.978157\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -19.978376\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -19.968592\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -19.968906\n",
            "resetting env. episode 316.000000, reward total was -19.000000. running mean: -19.959217\n",
            "resetting env. episode 317.000000, reward total was -17.000000. running mean: -19.929625\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -19.940328\n",
            "resetting env. episode 319.000000, reward total was -19.000000. running mean: -19.930925\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -19.931616\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -19.932300\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -19.942977\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -19.943547\n",
            "resetting env. episode 324.000000, reward total was -19.000000. running mean: -19.934111\n",
            "resetting env. episode 325.000000, reward total was -18.000000. running mean: -19.914770\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -19.905623\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -19.916566\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -19.917401\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -19.908227\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -19.919144\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -19.919953\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -19.920754\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -19.931546\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -19.932231\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -19.942908\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -19.933479\n",
            "resetting env. episode 337.000000, reward total was -19.000000. running mean: -19.924144\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -19.934903\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -19.935554\n",
            "resetting env. episode 340.000000, reward total was -18.000000. running mean: -19.916198\n",
            "resetting env. episode 341.000000, reward total was -19.000000. running mean: -19.907036\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -19.907966\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -19.918886\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -19.929697\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -19.930400\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -19.941096\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -19.951686\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -19.952169\n",
            "resetting env. episode 349.000000, reward total was -19.000000. running mean: -19.942647\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -19.953220\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -19.943688\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -19.954251\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -19.954709\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -19.955162\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -19.955610\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -19.946054\n",
            "resetting env. episode 357.000000, reward total was -19.000000. running mean: -19.936594\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -19.947228\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -19.947755\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -19.958278\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -19.958695\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -19.959108\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -19.969517\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -19.979822\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -19.970024\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -19.960323\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -19.960720\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -19.951113\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -19.941602\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -19.942186\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -19.952764\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -19.963236\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -19.963604\n",
            "resetting env. episode 374.000000, reward total was -19.000000. running mean: -19.953968\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -19.954428\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -19.954884\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -19.955335\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -19.965782\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -19.966124\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -19.976463\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -19.986698\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -19.986831\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -19.986963\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -19.987093\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -19.987222\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -19.997350\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -19.987376\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -19.987503\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -19.997628\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.007651\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.007575\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -19.997499\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.007524\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.007449\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.017374\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.027201\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.036929\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.036559\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.046194\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.035732\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.035375\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.045021\n",
            "resetting env. episode 403.000000, reward total was -18.000000. running mean: -20.024571\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.034325\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.023982\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.033742\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.033404\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.023070\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.022840\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.032611\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.032285\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.041962\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.041543\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.031127\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.040816\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -20.030408\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.040104\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.049703\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.039206\n",
            "resetting env. episode 420.000000, reward total was -18.000000. running mean: -20.018814\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.028625\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.028339\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.028056\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -20.017775\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.007598\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.017522\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.027346\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.027073\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.036802\n",
            "resetting env. episode 430.000000, reward total was -19.000000. running mean: -20.026434\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.036170\n",
            "resetting env. episode 432.000000, reward total was -19.000000. running mean: -20.025808\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.035550\n",
            "resetting env. episode 434.000000, reward total was -18.000000. running mean: -20.015194\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.025043\n",
            "resetting env. episode 436.000000, reward total was -18.000000. running mean: -20.004792\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.004744\n",
            "resetting env. episode 438.000000, reward total was -18.000000. running mean: -19.984697\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -19.994850\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.004901\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.004852\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.004804\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.004756\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.014708\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.014561\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.014415\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.004271\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.014229\n",
            "resetting env. episode 449.000000, reward total was -16.000000. running mean: -19.974086\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -19.974345\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -19.974602\n",
            "resetting env. episode 452.000000, reward total was -17.000000. running mean: -19.944856\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -19.945407\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -19.945953\n",
            "resetting env. episode 455.000000, reward total was -18.000000. running mean: -19.926494\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -19.917229\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -19.928057\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -19.918776\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -19.919588\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -19.930392\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -19.921088\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -19.911878\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -19.912759\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -19.913631\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -19.914495\n",
            "resetting env. episode 466.000000, reward total was -19.000000. running mean: -19.905350\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -19.896296\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -19.907333\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -19.908260\n",
            "resetting env. episode 470.000000, reward total was -17.000000. running mean: -19.879178\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -19.890386\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -19.891482\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -19.882567\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -19.893741\n",
            "resetting env. episode 475.000000, reward total was -19.000000. running mean: -19.884804\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -19.885956\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -19.897096\n",
            "resetting env. episode 478.000000, reward total was -18.000000. running mean: -19.878125\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -19.879344\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -19.880551\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -19.881745\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -19.882928\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -19.874099\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -19.875358\n",
            "resetting env. episode 485.000000, reward total was -17.000000. running mean: -19.846604\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -19.858138\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -19.869557\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -19.860861\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -19.862252\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -19.873630\n",
            "resetting env. episode 491.000000, reward total was -18.000000. running mean: -19.854894\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -19.846345\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -19.857881\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -19.849302\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -19.850809\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -19.842301\n",
            "resetting env. episode 497.000000, reward total was -19.000000. running mean: -19.833878\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -19.835539\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -19.827184\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -19.818912\n",
            "resetting env. episode 501.000000, reward total was -15.000000. running mean: -19.770723\n",
            "resetting env. episode 502.000000, reward total was -19.000000. running mean: -19.763016\n",
            "resetting env. episode 503.000000, reward total was -19.000000. running mean: -19.755386\n",
            "resetting env. episode 504.000000, reward total was -19.000000. running mean: -19.747832\n",
            "resetting env. episode 505.000000, reward total was -20.000000. running mean: -19.750354\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -19.752850\n",
            "resetting env. episode 507.000000, reward total was -18.000000. running mean: -19.735321\n",
            "resetting env. episode 508.000000, reward total was -17.000000. running mean: -19.707968\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -19.720889\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -19.733680\n",
            "resetting env. episode 511.000000, reward total was -18.000000. running mean: -19.716343\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -19.729179\n",
            "resetting env. episode 513.000000, reward total was -20.000000. running mean: -19.731888\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -19.744569\n",
            "resetting env. episode 515.000000, reward total was -20.000000. running mean: -19.747123\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -19.749652\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -19.762155\n",
            "resetting env. episode 518.000000, reward total was -20.000000. running mean: -19.764534\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -19.776888\n",
            "resetting env. episode 520.000000, reward total was -20.000000. running mean: -19.779120\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -19.781328\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -19.793515\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -19.795580\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -19.807624\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -19.819548\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -19.831352\n",
            "resetting env. episode 527.000000, reward total was -19.000000. running mean: -19.823039\n",
            "resetting env. episode 528.000000, reward total was -20.000000. running mean: -19.824809\n",
            "resetting env. episode 529.000000, reward total was -17.000000. running mean: -19.796560\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -19.808595\n",
            "resetting env. episode 531.000000, reward total was -19.000000. running mean: -19.800509\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -19.812504\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -19.824379\n",
            "resetting env. episode 534.000000, reward total was -20.000000. running mean: -19.826135\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -19.827874\n",
            "resetting env. episode 536.000000, reward total was -19.000000. running mean: -19.819595\n",
            "resetting env. episode 537.000000, reward total was -18.000000. running mean: -19.801399\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -19.813385\n",
            "resetting env. episode 539.000000, reward total was -17.000000. running mean: -19.785251\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -19.797399\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -19.809425\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -19.811330\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -19.823217\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -19.834985\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -19.846635\n",
            "resetting env. episode 546.000000, reward total was -19.000000. running mean: -19.838169\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -19.849787\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -19.861289\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -19.872676\n",
            "resetting env. episode 550.000000, reward total was -18.000000. running mean: -19.853949\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -19.865410\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -19.876756\n",
            "resetting env. episode 553.000000, reward total was -19.000000. running mean: -19.867988\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -19.879308\n",
            "resetting env. episode 555.000000, reward total was -20.000000. running mean: -19.880515\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -19.881710\n",
            "resetting env. episode 557.000000, reward total was -20.000000. running mean: -19.882893\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -19.894064\n",
            "resetting env. episode 559.000000, reward total was -20.000000. running mean: -19.895124\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -19.896172\n",
            "resetting env. episode 561.000000, reward total was -19.000000. running mean: -19.887211\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -19.898338\n",
            "resetting env. episode 563.000000, reward total was -19.000000. running mean: -19.889355\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -19.900462\n",
            "resetting env. episode 565.000000, reward total was -19.000000. running mean: -19.891457\n",
            "resetting env. episode 566.000000, reward total was -18.000000. running mean: -19.872542\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -19.883817\n",
            "resetting env. episode 568.000000, reward total was -17.000000. running mean: -19.854979\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -19.866429\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -19.877765\n",
            "resetting env. episode 571.000000, reward total was -19.000000. running mean: -19.868987\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -19.870297\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -19.881594\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -19.892778\n",
            "resetting env. episode 575.000000, reward total was -20.000000. running mean: -19.893850\n",
            "resetting env. episode 576.000000, reward total was -19.000000. running mean: -19.884912\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -19.896063\n",
            "resetting env. episode 578.000000, reward total was -19.000000. running mean: -19.887102\n",
            "resetting env. episode 579.000000, reward total was -19.000000. running mean: -19.878231\n",
            "resetting env. episode 580.000000, reward total was -19.000000. running mean: -19.869449\n",
            "resetting env. episode 581.000000, reward total was -19.000000. running mean: -19.860754\n",
            "resetting env. episode 582.000000, reward total was -19.000000. running mean: -19.852147\n",
            "resetting env. episode 583.000000, reward total was -17.000000. running mean: -19.823625\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -19.835389\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -19.847035\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -19.858565\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -19.859979\n",
            "resetting env. episode 588.000000, reward total was -20.000000. running mean: -19.861379\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -19.872766\n",
            "resetting env. episode 590.000000, reward total was -17.000000. running mean: -19.844038\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -19.855598\n",
            "resetting env. episode 592.000000, reward total was -20.000000. running mean: -19.857042\n",
            "resetting env. episode 593.000000, reward total was -20.000000. running mean: -19.858471\n",
            "resetting env. episode 594.000000, reward total was -18.000000. running mean: -19.839886\n",
            "resetting env. episode 595.000000, reward total was -20.000000. running mean: -19.841488\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -19.853073\n",
            "resetting env. episode 597.000000, reward total was -19.000000. running mean: -19.844542\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -19.856097\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -19.867536\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -19.878860\n",
            "resetting env. episode 601.000000, reward total was -19.000000. running mean: -19.870072\n",
            "resetting env. episode 602.000000, reward total was -19.000000. running mean: -19.861371\n",
            "resetting env. episode 603.000000, reward total was -18.000000. running mean: -19.842757\n",
            "resetting env. episode 604.000000, reward total was -20.000000. running mean: -19.844330\n",
            "resetting env. episode 605.000000, reward total was -19.000000. running mean: -19.835886\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -19.837528\n",
            "resetting env. episode 607.000000, reward total was -20.000000. running mean: -19.839152\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -19.840761\n",
            "resetting env. episode 609.000000, reward total was -18.000000. running mean: -19.822353\n",
            "resetting env. episode 610.000000, reward total was -18.000000. running mean: -19.804130\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -19.816088\n",
            "resetting env. episode 612.000000, reward total was -19.000000. running mean: -19.807927\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -19.819848\n",
            "resetting env. episode 614.000000, reward total was -19.000000. running mean: -19.811650\n",
            "resetting env. episode 615.000000, reward total was -20.000000. running mean: -19.813533\n",
            "resetting env. episode 616.000000, reward total was -18.000000. running mean: -19.795398\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -19.807444\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -19.819369\n",
            "resetting env. episode 619.000000, reward total was -19.000000. running mean: -19.811176\n",
            "resetting env. episode 620.000000, reward total was -15.000000. running mean: -19.763064\n",
            "resetting env. episode 621.000000, reward total was -19.000000. running mean: -19.755433\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -19.767879\n",
            "resetting env. episode 623.000000, reward total was -19.000000. running mean: -19.760200\n",
            "resetting env. episode 624.000000, reward total was -19.000000. running mean: -19.752598\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -19.755072\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -19.767521\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -19.779846\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -19.792048\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -19.804127\n",
            "resetting env. episode 630.000000, reward total was -20.000000. running mean: -19.806086\n",
            "resetting env. episode 631.000000, reward total was -20.000000. running mean: -19.808025\n",
            "resetting env. episode 632.000000, reward total was -20.000000. running mean: -19.809945\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -19.821845\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -19.833627\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -19.845291\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -19.856838\n",
            "resetting env. episode 637.000000, reward total was -18.000000. running mean: -19.838269\n",
            "resetting env. episode 638.000000, reward total was -20.000000. running mean: -19.839887\n",
            "resetting env. episode 639.000000, reward total was -20.000000. running mean: -19.841488\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -19.843073\n",
            "resetting env. episode 641.000000, reward total was -17.000000. running mean: -19.814642\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -19.826496\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -19.828231\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -19.839949\n",
            "resetting env. episode 645.000000, reward total was -19.000000. running mean: -19.831549\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -19.843234\n",
            "resetting env. episode 647.000000, reward total was -19.000000. running mean: -19.834801\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -19.846453\n",
            "resetting env. episode 649.000000, reward total was -15.000000. running mean: -19.797989\n",
            "resetting env. episode 650.000000, reward total was -19.000000. running mean: -19.790009\n",
            "resetting env. episode 651.000000, reward total was -20.000000. running mean: -19.792109\n",
            "resetting env. episode 652.000000, reward total was -17.000000. running mean: -19.764188\n",
            "resetting env. episode 653.000000, reward total was -21.000000. running mean: -19.776546\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -19.788780\n",
            "resetting env. episode 655.000000, reward total was -19.000000. running mean: -19.780893\n",
            "resetting env. episode 656.000000, reward total was -17.000000. running mean: -19.753084\n",
            "resetting env. episode 657.000000, reward total was -19.000000. running mean: -19.745553\n",
            "resetting env. episode 658.000000, reward total was -20.000000. running mean: -19.748097\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -19.760616\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -19.763010\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -19.765380\n",
            "resetting env. episode 662.000000, reward total was -21.000000. running mean: -19.777726\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -19.789949\n",
            "resetting env. episode 664.000000, reward total was -20.000000. running mean: -19.792049\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -19.794129\n",
            "resetting env. episode 666.000000, reward total was -20.000000. running mean: -19.796188\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -19.808226\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -19.820144\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -19.831942\n",
            "resetting env. episode 670.000000, reward total was -20.000000. running mean: -19.833623\n",
            "resetting env. episode 671.000000, reward total was -20.000000. running mean: -19.835286\n",
            "resetting env. episode 672.000000, reward total was -19.000000. running mean: -19.826934\n",
            "resetting env. episode 673.000000, reward total was -18.000000. running mean: -19.808664\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -19.820578\n",
            "resetting env. episode 675.000000, reward total was -17.000000. running mean: -19.792372\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -19.804448\n",
            "resetting env. episode 677.000000, reward total was -19.000000. running mean: -19.796404\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -19.808440\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -19.810355\n",
            "resetting env. episode 680.000000, reward total was -19.000000. running mean: -19.802252\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -19.814229\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -19.816087\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -19.817926\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -19.819747\n",
            "resetting env. episode 685.000000, reward total was -20.000000. running mean: -19.821549\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -19.833334\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -19.845000\n",
            "resetting env. episode 688.000000, reward total was -20.000000. running mean: -19.846550\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -19.858085\n",
            "resetting env. episode 690.000000, reward total was -21.000000. running mean: -19.869504\n",
            "resetting env. episode 691.000000, reward total was -19.000000. running mean: -19.860809\n",
            "resetting env. episode 692.000000, reward total was -19.000000. running mean: -19.852201\n",
            "resetting env. episode 693.000000, reward total was -20.000000. running mean: -19.853679\n",
            "resetting env. episode 694.000000, reward total was -20.000000. running mean: -19.855142\n",
            "resetting env. episode 695.000000, reward total was -20.000000. running mean: -19.856591\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -19.868025\n",
            "resetting env. episode 697.000000, reward total was -20.000000. running mean: -19.869345\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -19.880651\n",
            "resetting env. episode 699.000000, reward total was -19.000000. running mean: -19.871845\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -19.873126\n",
            "resetting env. episode 701.000000, reward total was -20.000000. running mean: -19.874395\n",
            "resetting env. episode 702.000000, reward total was -19.000000. running mean: -19.865651\n",
            "resetting env. episode 703.000000, reward total was -20.000000. running mean: -19.866994\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -19.868325\n",
            "resetting env. episode 705.000000, reward total was -21.000000. running mean: -19.879641\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -19.890845\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -19.901936\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -19.912917\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -19.923788\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -19.934550\n",
            "resetting env. episode 711.000000, reward total was -21.000000. running mean: -19.945204\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -19.955752\n",
            "resetting env. episode 713.000000, reward total was -19.000000. running mean: -19.946195\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -19.956733\n",
            "resetting env. episode 715.000000, reward total was -20.000000. running mean: -19.957166\n",
            "resetting env. episode 716.000000, reward total was -18.000000. running mean: -19.937594\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -19.948218\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -19.958736\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -19.959148\n",
            "resetting env. episode 720.000000, reward total was -19.000000. running mean: -19.949557\n",
            "resetting env. episode 721.000000, reward total was -20.000000. running mean: -19.950061\n",
            "resetting env. episode 722.000000, reward total was -20.000000. running mean: -19.950561\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -19.961055\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -19.971445\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -19.981730\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -19.981913\n",
            "resetting env. episode 727.000000, reward total was -18.000000. running mean: -19.962094\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -19.962473\n",
            "resetting env. episode 729.000000, reward total was -20.000000. running mean: -19.962848\n",
            "resetting env. episode 730.000000, reward total was -20.000000. running mean: -19.963220\n",
            "resetting env. episode 731.000000, reward total was -20.000000. running mean: -19.963587\n",
            "resetting env. episode 732.000000, reward total was -20.000000. running mean: -19.963952\n",
            "resetting env. episode 733.000000, reward total was -18.000000. running mean: -19.944312\n",
            "resetting env. episode 734.000000, reward total was -19.000000. running mean: -19.934869\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -19.945520\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -19.956065\n",
            "resetting env. episode 737.000000, reward total was -19.000000. running mean: -19.946504\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -19.957039\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -19.957469\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -19.967894\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -19.978215\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -19.988433\n",
            "resetting env. episode 743.000000, reward total was -20.000000. running mean: -19.988549\n",
            "resetting env. episode 744.000000, reward total was -20.000000. running mean: -19.988663\n",
            "resetting env. episode 745.000000, reward total was -18.000000. running mean: -19.968777\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -19.969089\n",
            "resetting env. episode 747.000000, reward total was -19.000000. running mean: -19.959398\n",
            "resetting env. episode 748.000000, reward total was -19.000000. running mean: -19.949804\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -19.960306\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -19.970703\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -19.980996\n",
            "resetting env. episode 752.000000, reward total was -17.000000. running mean: -19.951186\n",
            "resetting env. episode 753.000000, reward total was -19.000000. running mean: -19.941674\n",
            "resetting env. episode 754.000000, reward total was -19.000000. running mean: -19.932257\n",
            "resetting env. episode 755.000000, reward total was -19.000000. running mean: -19.922935\n",
            "resetting env. episode 756.000000, reward total was -20.000000. running mean: -19.923705\n",
            "resetting env. episode 757.000000, reward total was -20.000000. running mean: -19.924468\n",
            "resetting env. episode 758.000000, reward total was -20.000000. running mean: -19.925224\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -19.935971\n",
            "resetting env. episode 760.000000, reward total was -19.000000. running mean: -19.926612\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -19.937346\n",
            "resetting env. episode 762.000000, reward total was -19.000000. running mean: -19.927972\n",
            "resetting env. episode 763.000000, reward total was -21.000000. running mean: -19.938692\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -19.949306\n",
            "resetting env. episode 765.000000, reward total was -20.000000. running mean: -19.949812\n",
            "resetting env. episode 766.000000, reward total was -19.000000. running mean: -19.940314\n",
            "resetting env. episode 767.000000, reward total was -17.000000. running mean: -19.910911\n",
            "resetting env. episode 768.000000, reward total was -20.000000. running mean: -19.911802\n",
            "resetting env. episode 769.000000, reward total was -18.000000. running mean: -19.892684\n",
            "resetting env. episode 770.000000, reward total was -18.000000. running mean: -19.873757\n",
            "resetting env. episode 771.000000, reward total was -17.000000. running mean: -19.845020\n",
            "resetting env. episode 772.000000, reward total was -19.000000. running mean: -19.836569\n",
            "resetting env. episode 773.000000, reward total was -19.000000. running mean: -19.828204\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -19.839922\n",
            "resetting env. episode 775.000000, reward total was -19.000000. running mean: -19.831523\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -19.833207\n",
            "resetting env. episode 777.000000, reward total was -18.000000. running mean: -19.814875\n",
            "resetting env. episode 778.000000, reward total was -18.000000. running mean: -19.796726\n",
            "resetting env. episode 779.000000, reward total was -19.000000. running mean: -19.788759\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -19.800872\n",
            "resetting env. episode 781.000000, reward total was -18.000000. running mean: -19.782863\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -19.785034\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -19.797184\n",
            "resetting env. episode 784.000000, reward total was -19.000000. running mean: -19.789212\n",
            "resetting env. episode 785.000000, reward total was -20.000000. running mean: -19.791320\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -19.803407\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -19.815373\n",
            "resetting env. episode 788.000000, reward total was -20.000000. running mean: -19.817219\n",
            "resetting env. episode 789.000000, reward total was -20.000000. running mean: -19.819047\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -19.830856\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -19.842548\n",
            "resetting env. episode 792.000000, reward total was -16.000000. running mean: -19.804122\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -19.816081\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -19.827920\n",
            "resetting env. episode 795.000000, reward total was -18.000000. running mean: -19.809641\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -19.821545\n",
            "resetting env. episode 797.000000, reward total was -19.000000. running mean: -19.813329\n",
            "resetting env. episode 798.000000, reward total was -19.000000. running mean: -19.805196\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -19.817144\n",
            "resetting env. episode 800.000000, reward total was -19.000000. running mean: -19.808973\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -19.820883\n",
            "resetting env. episode 802.000000, reward total was -16.000000. running mean: -19.782674\n",
            "resetting env. episode 803.000000, reward total was -19.000000. running mean: -19.774847\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -19.777099\n",
            "resetting env. episode 805.000000, reward total was -20.000000. running mean: -19.779328\n",
            "resetting env. episode 806.000000, reward total was -20.000000. running mean: -19.781534\n",
            "resetting env. episode 807.000000, reward total was -18.000000. running mean: -19.763719\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -19.776082\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -19.788321\n",
            "resetting env. episode 810.000000, reward total was -16.000000. running mean: -19.750438\n",
            "resetting env. episode 811.000000, reward total was -19.000000. running mean: -19.742934\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -19.745504\n",
            "resetting env. episode 813.000000, reward total was -19.000000. running mean: -19.738049\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -19.750669\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -19.753162\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -19.765630\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -19.777974\n",
            "resetting env. episode 818.000000, reward total was -14.000000. running mean: -19.720194\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -19.732992\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -19.745662\n",
            "resetting env. episode 821.000000, reward total was -18.000000. running mean: -19.728206\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -19.740924\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -19.753515\n",
            "resetting env. episode 824.000000, reward total was -20.000000. running mean: -19.755979\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -19.768420\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -19.780735\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -19.782928\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -19.795099\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -19.797148\n",
            "resetting env. episode 830.000000, reward total was -20.000000. running mean: -19.799176\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -19.811185\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -19.823073\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -19.834842\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -19.836494\n",
            "resetting env. episode 835.000000, reward total was -20.000000. running mean: -19.838129\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -19.839747\n",
            "resetting env. episode 837.000000, reward total was -21.000000. running mean: -19.851350\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -19.862836\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -19.874208\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -19.875466\n",
            "resetting env. episode 841.000000, reward total was -18.000000. running mean: -19.856711\n",
            "resetting env. episode 842.000000, reward total was -20.000000. running mean: -19.858144\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -19.869563\n",
            "resetting env. episode 844.000000, reward total was -20.000000. running mean: -19.870867\n",
            "resetting env. episode 845.000000, reward total was -17.000000. running mean: -19.842158\n",
            "resetting env. episode 846.000000, reward total was -19.000000. running mean: -19.833737\n",
            "resetting env. episode 847.000000, reward total was -20.000000. running mean: -19.835399\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -19.837045\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -19.848675\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -19.850188\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -19.851686\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -19.853169\n",
            "resetting env. episode 853.000000, reward total was -20.000000. running mean: -19.854638\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -19.866091\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -19.877431\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -19.888656\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -19.889770\n",
            "resetting env. episode 858.000000, reward total was -19.000000. running mean: -19.880872\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -19.882063\n",
            "resetting env. episode 860.000000, reward total was -20.000000. running mean: -19.883243\n",
            "resetting env. episode 861.000000, reward total was -19.000000. running mean: -19.874410\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -19.875666\n",
            "resetting env. episode 863.000000, reward total was -20.000000. running mean: -19.876909\n",
            "resetting env. episode 864.000000, reward total was -19.000000. running mean: -19.868140\n",
            "resetting env. episode 865.000000, reward total was -18.000000. running mean: -19.849459\n",
            "resetting env. episode 866.000000, reward total was -20.000000. running mean: -19.850964\n",
            "resetting env. episode 867.000000, reward total was -17.000000. running mean: -19.822455\n",
            "resetting env. episode 868.000000, reward total was -19.000000. running mean: -19.814230\n",
            "resetting env. episode 869.000000, reward total was -19.000000. running mean: -19.806088\n",
            "resetting env. episode 870.000000, reward total was -19.000000. running mean: -19.798027\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -19.810047\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -19.821946\n",
            "resetting env. episode 873.000000, reward total was -20.000000. running mean: -19.823727\n",
            "resetting env. episode 874.000000, reward total was -20.000000. running mean: -19.825489\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -19.827235\n",
            "resetting env. episode 876.000000, reward total was -19.000000. running mean: -19.818962\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -19.830773\n",
            "resetting env. episode 878.000000, reward total was -20.000000. running mean: -19.832465\n",
            "resetting env. episode 879.000000, reward total was -19.000000. running mean: -19.824140\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -19.835899\n",
            "resetting env. episode 881.000000, reward total was -20.000000. running mean: -19.837540\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -19.849164\n",
            "resetting env. episode 883.000000, reward total was -20.000000. running mean: -19.850673\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -19.862166\n",
            "resetting env. episode 885.000000, reward total was -20.000000. running mean: -19.863544\n",
            "resetting env. episode 886.000000, reward total was -19.000000. running mean: -19.854909\n",
            "resetting env. episode 887.000000, reward total was -19.000000. running mean: -19.846360\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -19.857896\n",
            "resetting env. episode 889.000000, reward total was -20.000000. running mean: -19.859317\n",
            "resetting env. episode 890.000000, reward total was -19.000000. running mean: -19.850724\n",
            "resetting env. episode 891.000000, reward total was -20.000000. running mean: -19.852217\n",
            "resetting env. episode 892.000000, reward total was -19.000000. running mean: -19.843695\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -19.855258\n",
            "resetting env. episode 894.000000, reward total was -20.000000. running mean: -19.856705\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -19.868138\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -19.879457\n",
            "resetting env. episode 897.000000, reward total was -19.000000. running mean: -19.870662\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -19.881956\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -19.893136\n",
            "resetting env. episode 900.000000, reward total was -19.000000. running mean: -19.884205\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -19.895363\n",
            "resetting env. episode 902.000000, reward total was -21.000000. running mean: -19.906409\n",
            "resetting env. episode 903.000000, reward total was -20.000000. running mean: -19.907345\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -19.918271\n",
            "resetting env. episode 905.000000, reward total was -20.000000. running mean: -19.919089\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -19.929898\n",
            "resetting env. episode 907.000000, reward total was -20.000000. running mean: -19.930599\n",
            "resetting env. episode 908.000000, reward total was -18.000000. running mean: -19.911293\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -19.922180\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -19.932958\n",
            "resetting env. episode 911.000000, reward total was -19.000000. running mean: -19.923629\n",
            "resetting env. episode 912.000000, reward total was -18.000000. running mean: -19.904392\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -19.915348\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -19.926195\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -19.926933\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -19.937664\n",
            "resetting env. episode 917.000000, reward total was -20.000000. running mean: -19.938287\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -19.948904\n",
            "resetting env. episode 919.000000, reward total was -19.000000. running mean: -19.939415\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -19.950021\n",
            "resetting env. episode 921.000000, reward total was -20.000000. running mean: -19.950521\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -19.951015\n",
            "resetting env. episode 923.000000, reward total was -18.000000. running mean: -19.931505\n",
            "resetting env. episode 924.000000, reward total was -19.000000. running mean: -19.922190\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -19.932968\n",
            "resetting env. episode 926.000000, reward total was -19.000000. running mean: -19.923639\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -19.934402\n",
            "resetting env. episode 928.000000, reward total was -20.000000. running mean: -19.935058\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -19.935708\n",
            "resetting env. episode 930.000000, reward total was -20.000000. running mean: -19.936351\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -19.946987\n",
            "resetting env. episode 932.000000, reward total was -19.000000. running mean: -19.937517\n",
            "resetting env. episode 933.000000, reward total was -18.000000. running mean: -19.918142\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -19.928961\n",
            "resetting env. episode 935.000000, reward total was -20.000000. running mean: -19.929671\n",
            "resetting env. episode 936.000000, reward total was -21.000000. running mean: -19.940374\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -19.940971\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -19.941561\n",
            "resetting env. episode 939.000000, reward total was -19.000000. running mean: -19.932145\n",
            "resetting env. episode 940.000000, reward total was -19.000000. running mean: -19.922824\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -19.933596\n",
            "resetting env. episode 942.000000, reward total was -17.000000. running mean: -19.904260\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -19.915217\n",
            "resetting env. episode 944.000000, reward total was -19.000000. running mean: -19.906065\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -19.917004\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -19.927834\n",
            "resetting env. episode 947.000000, reward total was -19.000000. running mean: -19.918556\n",
            "resetting env. episode 948.000000, reward total was -18.000000. running mean: -19.899370\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -19.910377\n",
            "resetting env. episode 950.000000, reward total was -20.000000. running mean: -19.911273\n",
            "resetting env. episode 951.000000, reward total was -19.000000. running mean: -19.902160\n",
            "resetting env. episode 952.000000, reward total was -20.000000. running mean: -19.903138\n",
            "resetting env. episode 953.000000, reward total was -20.000000. running mean: -19.904107\n",
            "resetting env. episode 954.000000, reward total was -20.000000. running mean: -19.905066\n",
            "resetting env. episode 955.000000, reward total was -20.000000. running mean: -19.906015\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -19.916955\n",
            "resetting env. episode 957.000000, reward total was -20.000000. running mean: -19.917786\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -19.928608\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -19.939322\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -19.949929\n",
            "resetting env. episode 961.000000, reward total was -18.000000. running mean: -19.930429\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -19.941125\n",
            "resetting env. episode 963.000000, reward total was -20.000000. running mean: -19.941714\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -19.952297\n",
            "resetting env. episode 965.000000, reward total was -19.000000. running mean: -19.942774\n",
            "resetting env. episode 966.000000, reward total was -19.000000. running mean: -19.933346\n",
            "resetting env. episode 967.000000, reward total was -19.000000. running mean: -19.924012\n",
            "resetting env. episode 968.000000, reward total was -19.000000. running mean: -19.914772\n",
            "resetting env. episode 969.000000, reward total was -20.000000. running mean: -19.915625\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -19.916468\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -19.927304\n",
            "resetting env. episode 972.000000, reward total was -19.000000. running mean: -19.918031\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -19.928850\n",
            "resetting env. episode 974.000000, reward total was -19.000000. running mean: -19.919562\n",
            "resetting env. episode 975.000000, reward total was -18.000000. running mean: -19.900366\n",
            "resetting env. episode 976.000000, reward total was -19.000000. running mean: -19.891362\n",
            "resetting env. episode 977.000000, reward total was -14.000000. running mean: -19.832449\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -19.844124\n",
            "resetting env. episode 979.000000, reward total was -19.000000. running mean: -19.835683\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -19.837326\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -19.848953\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -19.860464\n",
            "resetting env. episode 983.000000, reward total was -18.000000. running mean: -19.841859\n",
            "resetting env. episode 984.000000, reward total was -20.000000. running mean: -19.843440\n",
            "resetting env. episode 985.000000, reward total was -18.000000. running mean: -19.825006\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -19.836756\n",
            "resetting env. episode 987.000000, reward total was -19.000000. running mean: -19.828388\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -19.840104\n",
            "resetting env. episode 989.000000, reward total was -20.000000. running mean: -19.841703\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -19.853286\n",
            "resetting env. episode 991.000000, reward total was -19.000000. running mean: -19.844753\n",
            "resetting env. episode 992.000000, reward total was -20.000000. running mean: -19.846306\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -19.857843\n",
            "resetting env. episode 994.000000, reward total was -20.000000. running mean: -19.859264\n",
            "resetting env. episode 995.000000, reward total was -20.000000. running mean: -19.860672\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -19.862065\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -19.873444\n",
            "resetting env. episode 998.000000, reward total was -20.000000. running mean: -19.874710\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -19.885963\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -19.887103\n",
            "resetting env. episode 1001.000000, reward total was -19.000000. running mean: -19.878232\n",
            "resetting env. episode 1002.000000, reward total was -19.000000. running mean: -19.869450\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -19.880755\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -19.891948\n",
            "resetting env. episode 1005.000000, reward total was -20.000000. running mean: -19.893028\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -19.894098\n",
            "resetting env. episode 1007.000000, reward total was -19.000000. running mean: -19.885157\n",
            "resetting env. episode 1008.000000, reward total was -20.000000. running mean: -19.886306\n",
            "resetting env. episode 1009.000000, reward total was -20.000000. running mean: -19.887442\n",
            "resetting env. episode 1010.000000, reward total was -19.000000. running mean: -19.878568\n",
            "resetting env. episode 1011.000000, reward total was -19.000000. running mean: -19.869782\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -19.871085\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -19.882374\n",
            "resetting env. episode 1014.000000, reward total was -20.000000. running mean: -19.883550\n",
            "resetting env. episode 1015.000000, reward total was -19.000000. running mean: -19.874714\n",
            "resetting env. episode 1016.000000, reward total was -19.000000. running mean: -19.865967\n",
            "resetting env. episode 1017.000000, reward total was -19.000000. running mean: -19.857308\n",
            "resetting env. episode 1018.000000, reward total was -18.000000. running mean: -19.838735\n",
            "resetting env. episode 1019.000000, reward total was -20.000000. running mean: -19.840347\n",
            "resetting env. episode 1020.000000, reward total was -20.000000. running mean: -19.841944\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -19.853524\n",
            "resetting env. episode 1022.000000, reward total was -18.000000. running mean: -19.834989\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -19.846639\n",
            "resetting env. episode 1024.000000, reward total was -18.000000. running mean: -19.828173\n",
            "resetting env. episode 1025.000000, reward total was -18.000000. running mean: -19.809891\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -19.811792\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -19.823674\n",
            "resetting env. episode 1028.000000, reward total was -20.000000. running mean: -19.825437\n",
            "resetting env. episode 1029.000000, reward total was -20.000000. running mean: -19.827183\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -19.828911\n",
            "resetting env. episode 1031.000000, reward total was -18.000000. running mean: -19.810622\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -19.822516\n",
            "resetting env. episode 1033.000000, reward total was -19.000000. running mean: -19.814291\n",
            "resetting env. episode 1034.000000, reward total was -19.000000. running mean: -19.806148\n",
            "resetting env. episode 1035.000000, reward total was -19.000000. running mean: -19.798086\n",
            "resetting env. episode 1036.000000, reward total was -19.000000. running mean: -19.790106\n",
            "resetting env. episode 1037.000000, reward total was -19.000000. running mean: -19.782204\n",
            "resetting env. episode 1038.000000, reward total was -19.000000. running mean: -19.774382\n",
            "resetting env. episode 1039.000000, reward total was -18.000000. running mean: -19.756639\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -19.769072\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -19.781381\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -19.793568\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -19.805632\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -19.807576\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -19.809500\n",
            "resetting env. episode 1046.000000, reward total was -19.000000. running mean: -19.801405\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -19.813391\n",
            "resetting env. episode 1048.000000, reward total was -19.000000. running mean: -19.805257\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -19.807204\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -19.819132\n",
            "resetting env. episode 1051.000000, reward total was -17.000000. running mean: -19.790941\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -19.793032\n",
            "resetting env. episode 1053.000000, reward total was -20.000000. running mean: -19.795101\n",
            "resetting env. episode 1054.000000, reward total was -20.000000. running mean: -19.797150\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -19.809179\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -19.821087\n",
            "resetting env. episode 1057.000000, reward total was -18.000000. running mean: -19.802876\n",
            "resetting env. episode 1058.000000, reward total was -20.000000. running mean: -19.804847\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -19.806799\n",
            "resetting env. episode 1060.000000, reward total was -18.000000. running mean: -19.788731\n",
            "resetting env. episode 1061.000000, reward total was -20.000000. running mean: -19.790844\n",
            "resetting env. episode 1062.000000, reward total was -19.000000. running mean: -19.782935\n",
            "resetting env. episode 1063.000000, reward total was -19.000000. running mean: -19.775106\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -19.787355\n",
            "resetting env. episode 1065.000000, reward total was -19.000000. running mean: -19.779481\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -19.781686\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -19.793870\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -19.805931\n",
            "resetting env. episode 1069.000000, reward total was -19.000000. running mean: -19.797872\n",
            "resetting env. episode 1070.000000, reward total was -18.000000. running mean: -19.779893\n",
            "resetting env. episode 1071.000000, reward total was -20.000000. running mean: -19.782094\n",
            "resetting env. episode 1072.000000, reward total was -16.000000. running mean: -19.744273\n",
            "resetting env. episode 1073.000000, reward total was -20.000000. running mean: -19.746830\n",
            "resetting env. episode 1074.000000, reward total was -19.000000. running mean: -19.739362\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -19.751968\n",
            "resetting env. episode 1076.000000, reward total was -19.000000. running mean: -19.744449\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -19.757004\n",
            "resetting env. episode 1078.000000, reward total was -20.000000. running mean: -19.759434\n",
            "resetting env. episode 1079.000000, reward total was -20.000000. running mean: -19.761840\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -19.764221\n",
            "resetting env. episode 1081.000000, reward total was -19.000000. running mean: -19.756579\n",
            "resetting env. episode 1082.000000, reward total was -20.000000. running mean: -19.759013\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -19.761423\n",
            "resetting env. episode 1084.000000, reward total was -19.000000. running mean: -19.753809\n",
            "resetting env. episode 1085.000000, reward total was -20.000000. running mean: -19.756271\n",
            "resetting env. episode 1086.000000, reward total was -18.000000. running mean: -19.738708\n",
            "resetting env. episode 1087.000000, reward total was -18.000000. running mean: -19.721321\n",
            "resetting env. episode 1088.000000, reward total was -19.000000. running mean: -19.714108\n",
            "resetting env. episode 1089.000000, reward total was -19.000000. running mean: -19.706967\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -19.719897\n",
            "resetting env. episode 1091.000000, reward total was -20.000000. running mean: -19.722698\n",
            "resetting env. episode 1092.000000, reward total was -21.000000. running mean: -19.735471\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -19.738116\n",
            "resetting env. episode 1094.000000, reward total was -21.000000. running mean: -19.750735\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -19.763228\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -19.775596\n",
            "resetting env. episode 1097.000000, reward total was -20.000000. running mean: -19.777840\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -19.790061\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -19.802161\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -19.804139\n",
            "resetting env. episode 1101.000000, reward total was -19.000000. running mean: -19.796098\n",
            "resetting env. episode 1102.000000, reward total was -18.000000. running mean: -19.778137\n",
            "resetting env. episode 1103.000000, reward total was -19.000000. running mean: -19.770355\n",
            "resetting env. episode 1104.000000, reward total was -17.000000. running mean: -19.742652\n",
            "resetting env. episode 1105.000000, reward total was -19.000000. running mean: -19.735225\n",
            "resetting env. episode 1106.000000, reward total was -19.000000. running mean: -19.727873\n",
            "resetting env. episode 1107.000000, reward total was -20.000000. running mean: -19.730594\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -19.743288\n",
            "resetting env. episode 1109.000000, reward total was -18.000000. running mean: -19.725855\n",
            "resetting env. episode 1110.000000, reward total was -19.000000. running mean: -19.718597\n",
            "resetting env. episode 1111.000000, reward total was -20.000000. running mean: -19.721411\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -19.724197\n",
            "resetting env. episode 1113.000000, reward total was -20.000000. running mean: -19.726955\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -19.729685\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -19.732388\n",
            "resetting env. episode 1116.000000, reward total was -20.000000. running mean: -19.735065\n",
            "resetting env. episode 1117.000000, reward total was -20.000000. running mean: -19.737714\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -19.750337\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -19.762833\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -19.765205\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -19.777553\n",
            "resetting env. episode 1122.000000, reward total was -19.000000. running mean: -19.769778\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -19.782080\n",
            "resetting env. episode 1124.000000, reward total was -20.000000. running mean: -19.784259\n",
            "resetting env. episode 1125.000000, reward total was -18.000000. running mean: -19.766416\n",
            "resetting env. episode 1126.000000, reward total was -20.000000. running mean: -19.768752\n",
            "resetting env. episode 1127.000000, reward total was -19.000000. running mean: -19.761065\n",
            "resetting env. episode 1128.000000, reward total was -20.000000. running mean: -19.763454\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -19.775819\n",
            "resetting env. episode 1130.000000, reward total was -20.000000. running mean: -19.778061\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -19.790281\n",
            "resetting env. episode 1132.000000, reward total was -19.000000. running mean: -19.782378\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -19.794554\n",
            "resetting env. episode 1134.000000, reward total was -20.000000. running mean: -19.796609\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -19.798642\n",
            "resetting env. episode 1136.000000, reward total was -21.000000. running mean: -19.810656\n",
            "resetting env. episode 1137.000000, reward total was -19.000000. running mean: -19.802549\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -19.814524\n",
            "resetting env. episode 1139.000000, reward total was -20.000000. running mean: -19.816379\n",
            "resetting env. episode 1140.000000, reward total was -20.000000. running mean: -19.818215\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -19.830033\n",
            "resetting env. episode 1142.000000, reward total was -19.000000. running mean: -19.821732\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -19.833515\n",
            "resetting env. episode 1144.000000, reward total was -17.000000. running mean: -19.805180\n",
            "resetting env. episode 1145.000000, reward total was -18.000000. running mean: -19.787128\n",
            "resetting env. episode 1146.000000, reward total was -19.000000. running mean: -19.779257\n",
            "resetting env. episode 1147.000000, reward total was -19.000000. running mean: -19.771464\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -19.773750\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -19.776012\n",
            "resetting env. episode 1150.000000, reward total was -18.000000. running mean: -19.758252\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -19.770670\n",
            "resetting env. episode 1152.000000, reward total was -19.000000. running mean: -19.762963\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -19.775333\n",
            "resetting env. episode 1154.000000, reward total was -20.000000. running mean: -19.777580\n",
            "resetting env. episode 1155.000000, reward total was -20.000000. running mean: -19.779804\n",
            "resetting env. episode 1156.000000, reward total was -20.000000. running mean: -19.782006\n",
            "resetting env. episode 1157.000000, reward total was -18.000000. running mean: -19.764186\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -19.776544\n",
            "resetting env. episode 1159.000000, reward total was -18.000000. running mean: -19.758779\n",
            "resetting env. episode 1160.000000, reward total was -19.000000. running mean: -19.751191\n",
            "resetting env. episode 1161.000000, reward total was -19.000000. running mean: -19.743679\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -19.756242\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -19.758680\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -19.771093\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -19.773382\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -19.785648\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -19.797792\n",
            "resetting env. episode 1168.000000, reward total was -19.000000. running mean: -19.789814\n",
            "resetting env. episode 1169.000000, reward total was -19.000000. running mean: -19.781916\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -19.794097\n",
            "resetting env. episode 1171.000000, reward total was -19.000000. running mean: -19.786156\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -19.788294\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -19.800411\n",
            "resetting env. episode 1174.000000, reward total was -20.000000. running mean: -19.802407\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -19.814383\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -19.816239\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -19.828077\n",
            "resetting env. episode 1178.000000, reward total was -21.000000. running mean: -19.839796\n",
            "resetting env. episode 1179.000000, reward total was -20.000000. running mean: -19.841398\n",
            "resetting env. episode 1180.000000, reward total was -19.000000. running mean: -19.832984\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -19.844654\n",
            "resetting env. episode 1182.000000, reward total was -19.000000. running mean: -19.836208\n",
            "resetting env. episode 1183.000000, reward total was -14.000000. running mean: -19.777846\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -19.790067\n",
            "resetting env. episode 1185.000000, reward total was -18.000000. running mean: -19.772166\n",
            "resetting env. episode 1186.000000, reward total was -20.000000. running mean: -19.774445\n",
            "resetting env. episode 1187.000000, reward total was -20.000000. running mean: -19.776700\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -19.788933\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -19.801044\n",
            "resetting env. episode 1190.000000, reward total was -19.000000. running mean: -19.793034\n",
            "resetting env. episode 1191.000000, reward total was -19.000000. running mean: -19.785103\n",
            "resetting env. episode 1192.000000, reward total was -19.000000. running mean: -19.777252\n",
            "resetting env. episode 1193.000000, reward total was -20.000000. running mean: -19.779480\n",
            "resetting env. episode 1194.000000, reward total was -19.000000. running mean: -19.771685\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -19.783968\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -19.796128\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -19.808167\n",
            "resetting env. episode 1198.000000, reward total was -18.000000. running mean: -19.790085\n",
            "resetting env. episode 1199.000000, reward total was -19.000000. running mean: -19.782184\n",
            "resetting env. episode 1200.000000, reward total was -18.000000. running mean: -19.764363\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -19.766719\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -19.779052\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -19.791261\n",
            "resetting env. episode 1204.000000, reward total was -18.000000. running mean: -19.773349\n",
            "resetting env. episode 1205.000000, reward total was -19.000000. running mean: -19.765615\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -19.777959\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -19.770179\n",
            "resetting env. episode 1208.000000, reward total was -19.000000. running mean: -19.762478\n",
            "resetting env. episode 1209.000000, reward total was -20.000000. running mean: -19.764853\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -19.777204\n",
            "resetting env. episode 1211.000000, reward total was -20.000000. running mean: -19.779432\n",
            "resetting env. episode 1212.000000, reward total was -19.000000. running mean: -19.771638\n",
            "resetting env. episode 1213.000000, reward total was -20.000000. running mean: -19.773922\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -19.786182\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -19.798321\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -19.800337\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -19.802334\n",
            "resetting env. episode 1218.000000, reward total was -19.000000. running mean: -19.794311\n",
            "resetting env. episode 1219.000000, reward total was -20.000000. running mean: -19.796368\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -19.808404\n",
            "resetting env. episode 1221.000000, reward total was -20.000000. running mean: -19.810320\n",
            "resetting env. episode 1222.000000, reward total was -20.000000. running mean: -19.812217\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -19.824094\n",
            "resetting env. episode 1224.000000, reward total was -20.000000. running mean: -19.825854\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -19.837595\n",
            "resetting env. episode 1226.000000, reward total was -20.000000. running mean: -19.839219\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -19.850827\n",
            "resetting env. episode 1228.000000, reward total was -19.000000. running mean: -19.842319\n",
            "resetting env. episode 1229.000000, reward total was -19.000000. running mean: -19.833895\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -19.845556\n",
            "resetting env. episode 1231.000000, reward total was -20.000000. running mean: -19.847101\n",
            "resetting env. episode 1232.000000, reward total was -18.000000. running mean: -19.828630\n",
            "resetting env. episode 1233.000000, reward total was -17.000000. running mean: -19.800344\n",
            "resetting env. episode 1234.000000, reward total was -20.000000. running mean: -19.802340\n",
            "resetting env. episode 1235.000000, reward total was -19.000000. running mean: -19.794317\n",
            "resetting env. episode 1236.000000, reward total was -19.000000. running mean: -19.786374\n",
            "resetting env. episode 1237.000000, reward total was -19.000000. running mean: -19.778510\n",
            "resetting env. episode 1238.000000, reward total was -19.000000. running mean: -19.770725\n",
            "resetting env. episode 1239.000000, reward total was -20.000000. running mean: -19.773017\n",
            "resetting env. episode 1240.000000, reward total was -17.000000. running mean: -19.745287\n",
            "resetting env. episode 1241.000000, reward total was -20.000000. running mean: -19.747834\n",
            "resetting env. episode 1242.000000, reward total was -20.000000. running mean: -19.750356\n",
            "resetting env. episode 1243.000000, reward total was -19.000000. running mean: -19.742853\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -19.745424\n",
            "resetting env. episode 1245.000000, reward total was -20.000000. running mean: -19.747970\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -19.760490\n",
            "resetting env. episode 1247.000000, reward total was -20.000000. running mean: -19.762885\n",
            "resetting env. episode 1248.000000, reward total was -20.000000. running mean: -19.765256\n",
            "resetting env. episode 1249.000000, reward total was -20.000000. running mean: -19.767604\n",
            "resetting env. episode 1250.000000, reward total was -19.000000. running mean: -19.759928\n",
            "resetting env. episode 1251.000000, reward total was -18.000000. running mean: -19.742328\n",
            "resetting env. episode 1252.000000, reward total was -18.000000. running mean: -19.724905\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -19.737656\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -19.750280\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -19.762777\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -19.775149\n",
            "resetting env. episode 1257.000000, reward total was -19.000000. running mean: -19.767397\n",
            "resetting env. episode 1258.000000, reward total was -20.000000. running mean: -19.769724\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -19.782026\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -19.784206\n",
            "resetting env. episode 1261.000000, reward total was -19.000000. running mean: -19.776364\n",
            "resetting env. episode 1262.000000, reward total was -17.000000. running mean: -19.748600\n",
            "resetting env. episode 1263.000000, reward total was -19.000000. running mean: -19.741114\n",
            "resetting env. episode 1264.000000, reward total was -19.000000. running mean: -19.733703\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -19.746366\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -19.758902\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -19.761313\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -19.773700\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -19.785963\n",
            "resetting env. episode 1270.000000, reward total was -19.000000. running mean: -19.778104\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -19.780323\n",
            "resetting env. episode 1272.000000, reward total was -20.000000. running mean: -19.782519\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -19.794694\n",
            "resetting env. episode 1274.000000, reward total was -19.000000. running mean: -19.786747\n",
            "resetting env. episode 1275.000000, reward total was -20.000000. running mean: -19.788880\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -19.800991\n",
            "resetting env. episode 1277.000000, reward total was -19.000000. running mean: -19.792981\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -19.805051\n",
            "resetting env. episode 1279.000000, reward total was -19.000000. running mean: -19.797001\n",
            "resetting env. episode 1280.000000, reward total was -19.000000. running mean: -19.789031\n",
            "resetting env. episode 1281.000000, reward total was -18.000000. running mean: -19.771140\n",
            "resetting env. episode 1282.000000, reward total was -20.000000. running mean: -19.773429\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -19.785695\n",
            "resetting env. episode 1284.000000, reward total was -19.000000. running mean: -19.777838\n",
            "resetting env. episode 1285.000000, reward total was -19.000000. running mean: -19.770059\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -19.782359\n",
            "resetting env. episode 1287.000000, reward total was -19.000000. running mean: -19.774535\n",
            "resetting env. episode 1288.000000, reward total was -21.000000. running mean: -19.786790\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -19.788922\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -19.801033\n",
            "resetting env. episode 1291.000000, reward total was -20.000000. running mean: -19.803022\n",
            "resetting env. episode 1292.000000, reward total was -20.000000. running mean: -19.804992\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -19.816942\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -19.828773\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -19.840485\n",
            "resetting env. episode 1296.000000, reward total was -20.000000. running mean: -19.842080\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -19.853660\n",
            "resetting env. episode 1298.000000, reward total was -20.000000. running mean: -19.855123\n",
            "resetting env. episode 1299.000000, reward total was -19.000000. running mean: -19.846572\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -19.858106\n",
            "resetting env. episode 1301.000000, reward total was -20.000000. running mean: -19.859525\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -19.870930\n",
            "resetting env. episode 1303.000000, reward total was -17.000000. running mean: -19.842220\n",
            "resetting env. episode 1304.000000, reward total was -18.000000. running mean: -19.823798\n",
            "resetting env. episode 1305.000000, reward total was -18.000000. running mean: -19.805560\n",
            "resetting env. episode 1306.000000, reward total was -18.000000. running mean: -19.787505\n",
            "resetting env. episode 1307.000000, reward total was -20.000000. running mean: -19.789630\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -19.801733\n",
            "resetting env. episode 1309.000000, reward total was -18.000000. running mean: -19.783716\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -19.795879\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -19.807920\n",
            "resetting env. episode 1312.000000, reward total was -20.000000. running mean: -19.809841\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -19.821742\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -19.833525\n",
            "resetting env. episode 1315.000000, reward total was -19.000000. running mean: -19.825190\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -19.836938\n",
            "resetting env. episode 1317.000000, reward total was -20.000000. running mean: -19.838568\n",
            "resetting env. episode 1318.000000, reward total was -18.000000. running mean: -19.820183\n",
            "resetting env. episode 1319.000000, reward total was -18.000000. running mean: -19.801981\n",
            "resetting env. episode 1320.000000, reward total was -19.000000. running mean: -19.793961\n",
            "resetting env. episode 1321.000000, reward total was -21.000000. running mean: -19.806021\n",
            "resetting env. episode 1322.000000, reward total was -19.000000. running mean: -19.797961\n",
            "resetting env. episode 1323.000000, reward total was -20.000000. running mean: -19.799982\n",
            "resetting env. episode 1324.000000, reward total was -19.000000. running mean: -19.791982\n",
            "resetting env. episode 1325.000000, reward total was -20.000000. running mean: -19.794062\n",
            "resetting env. episode 1326.000000, reward total was -19.000000. running mean: -19.786121\n",
            "resetting env. episode 1327.000000, reward total was -19.000000. running mean: -19.778260\n",
            "resetting env. episode 1328.000000, reward total was -18.000000. running mean: -19.760478\n",
            "resetting env. episode 1329.000000, reward total was -19.000000. running mean: -19.752873\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -19.765344\n",
            "resetting env. episode 1331.000000, reward total was -19.000000. running mean: -19.757691\n",
            "resetting env. episode 1332.000000, reward total was -20.000000. running mean: -19.760114\n",
            "resetting env. episode 1333.000000, reward total was -18.000000. running mean: -19.742513\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -19.755087\n",
            "resetting env. episode 1335.000000, reward total was -20.000000. running mean: -19.757537\n",
            "resetting env. episode 1336.000000, reward total was -19.000000. running mean: -19.749961\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -19.752462\n",
            "resetting env. episode 1338.000000, reward total was -19.000000. running mean: -19.744937\n",
            "resetting env. episode 1339.000000, reward total was -19.000000. running mean: -19.737488\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -19.750113\n",
            "resetting env. episode 1341.000000, reward total was -20.000000. running mean: -19.752612\n",
            "resetting env. episode 1342.000000, reward total was -18.000000. running mean: -19.735086\n",
            "resetting env. episode 1343.000000, reward total was -20.000000. running mean: -19.737735\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -19.740357\n",
            "resetting env. episode 1345.000000, reward total was -20.000000. running mean: -19.742954\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -19.755524\n",
            "resetting env. episode 1347.000000, reward total was -19.000000. running mean: -19.747969\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -19.750489\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -19.752984\n",
            "resetting env. episode 1350.000000, reward total was -20.000000. running mean: -19.755455\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -19.757900\n",
            "resetting env. episode 1352.000000, reward total was -19.000000. running mean: -19.750321\n",
            "resetting env. episode 1353.000000, reward total was -20.000000. running mean: -19.752818\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -19.755290\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -19.767737\n",
            "resetting env. episode 1356.000000, reward total was -17.000000. running mean: -19.740059\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -19.742659\n",
            "resetting env. episode 1358.000000, reward total was -18.000000. running mean: -19.725232\n",
            "resetting env. episode 1359.000000, reward total was -20.000000. running mean: -19.727980\n",
            "resetting env. episode 1360.000000, reward total was -16.000000. running mean: -19.690700\n",
            "resetting env. episode 1361.000000, reward total was -17.000000. running mean: -19.663793\n",
            "resetting env. episode 1362.000000, reward total was -18.000000. running mean: -19.647155\n",
            "resetting env. episode 1363.000000, reward total was -19.000000. running mean: -19.640684\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -19.654277\n",
            "resetting env. episode 1365.000000, reward total was -18.000000. running mean: -19.637734\n",
            "resetting env. episode 1366.000000, reward total was -19.000000. running mean: -19.631357\n",
            "resetting env. episode 1367.000000, reward total was -20.000000. running mean: -19.635043\n",
            "resetting env. episode 1368.000000, reward total was -18.000000. running mean: -19.618693\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -19.632506\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -19.646181\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -19.659719\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -19.663122\n",
            "resetting env. episode 1373.000000, reward total was -18.000000. running mean: -19.646490\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -19.660026\n",
            "resetting env. episode 1375.000000, reward total was -17.000000. running mean: -19.633425\n",
            "resetting env. episode 1376.000000, reward total was -18.000000. running mean: -19.617091\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -19.620920\n",
            "resetting env. episode 1378.000000, reward total was -20.000000. running mean: -19.624711\n",
            "resetting env. episode 1379.000000, reward total was -20.000000. running mean: -19.628464\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -19.632179\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -19.635857\n",
            "resetting env. episode 1382.000000, reward total was -20.000000. running mean: -19.639499\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -19.653104\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -19.656573\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -19.670007\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -19.683307\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -19.696474\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -19.709509\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -19.722414\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -19.725190\n",
            "resetting env. episode 1391.000000, reward total was -20.000000. running mean: -19.727938\n",
            "resetting env. episode 1392.000000, reward total was -19.000000. running mean: -19.720659\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -19.733452\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -19.736118\n",
            "resetting env. episode 1395.000000, reward total was -19.000000. running mean: -19.728756\n",
            "resetting env. episode 1396.000000, reward total was -19.000000. running mean: -19.721469\n",
            "resetting env. episode 1397.000000, reward total was -20.000000. running mean: -19.724254\n",
            "resetting env. episode 1398.000000, reward total was -20.000000. running mean: -19.727012\n",
            "resetting env. episode 1399.000000, reward total was -19.000000. running mean: -19.719741\n",
            "resetting env. episode 1400.000000, reward total was -20.000000. running mean: -19.722544\n",
            "resetting env. episode 1401.000000, reward total was -19.000000. running mean: -19.715319\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -19.728165\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -19.740884\n",
            "resetting env. episode 1404.000000, reward total was -19.000000. running mean: -19.733475\n",
            "resetting env. episode 1405.000000, reward total was -18.000000. running mean: -19.716140\n",
            "resetting env. episode 1406.000000, reward total was -19.000000. running mean: -19.708979\n",
            "resetting env. episode 1407.000000, reward total was -18.000000. running mean: -19.691889\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -19.694970\n",
            "resetting env. episode 1409.000000, reward total was -18.000000. running mean: -19.678020\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -19.691240\n",
            "resetting env. episode 1411.000000, reward total was -16.000000. running mean: -19.654328\n",
            "resetting env. episode 1412.000000, reward total was -20.000000. running mean: -19.657785\n",
            "resetting env. episode 1413.000000, reward total was -20.000000. running mean: -19.661207\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -19.674595\n",
            "resetting env. episode 1415.000000, reward total was -19.000000. running mean: -19.667849\n",
            "resetting env. episode 1416.000000, reward total was -19.000000. running mean: -19.661170\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -19.664558\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -19.677913\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -19.691134\n",
            "resetting env. episode 1420.000000, reward total was -17.000000. running mean: -19.664222\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -19.667580\n",
            "resetting env. episode 1422.000000, reward total was -19.000000. running mean: -19.660904\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -19.674295\n",
            "resetting env. episode 1424.000000, reward total was -20.000000. running mean: -19.677552\n",
            "resetting env. episode 1425.000000, reward total was -19.000000. running mean: -19.670777\n",
            "resetting env. episode 1426.000000, reward total was -19.000000. running mean: -19.664069\n",
            "resetting env. episode 1427.000000, reward total was -20.000000. running mean: -19.667428\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -19.680754\n",
            "resetting env. episode 1429.000000, reward total was -19.000000. running mean: -19.673947\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -19.677207\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -19.680435\n",
            "resetting env. episode 1432.000000, reward total was -17.000000. running mean: -19.653631\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -19.667094\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -19.680423\n",
            "resetting env. episode 1435.000000, reward total was -20.000000. running mean: -19.683619\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -19.686783\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -19.699915\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -19.712916\n",
            "resetting env. episode 1439.000000, reward total was -20.000000. running mean: -19.715787\n",
            "resetting env. episode 1440.000000, reward total was -20.000000. running mean: -19.718629\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -19.731443\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -19.744128\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -19.756687\n",
            "resetting env. episode 1444.000000, reward total was -20.000000. running mean: -19.759120\n",
            "resetting env. episode 1445.000000, reward total was -17.000000. running mean: -19.731529\n",
            "resetting env. episode 1446.000000, reward total was -18.000000. running mean: -19.714214\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -19.727072\n",
            "resetting env. episode 1448.000000, reward total was -21.000000. running mean: -19.739801\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -19.742403\n",
            "resetting env. episode 1450.000000, reward total was -16.000000. running mean: -19.704979\n",
            "resetting env. episode 1451.000000, reward total was -19.000000. running mean: -19.697929\n",
            "resetting env. episode 1452.000000, reward total was -16.000000. running mean: -19.660950\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -19.674340\n",
            "resetting env. episode 1454.000000, reward total was -18.000000. running mean: -19.657597\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -19.671021\n",
            "resetting env. episode 1456.000000, reward total was -20.000000. running mean: -19.674311\n",
            "resetting env. episode 1457.000000, reward total was -20.000000. running mean: -19.677567\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -19.680792\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -19.683984\n",
            "resetting env. episode 1460.000000, reward total was -20.000000. running mean: -19.687144\n",
            "resetting env. episode 1461.000000, reward total was -20.000000. running mean: -19.690273\n",
            "resetting env. episode 1462.000000, reward total was -18.000000. running mean: -19.673370\n",
            "resetting env. episode 1463.000000, reward total was -20.000000. running mean: -19.676636\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -19.689870\n",
            "resetting env. episode 1465.000000, reward total was -21.000000. running mean: -19.702971\n",
            "resetting env. episode 1466.000000, reward total was -18.000000. running mean: -19.685941\n",
            "resetting env. episode 1467.000000, reward total was -18.000000. running mean: -19.669082\n",
            "resetting env. episode 1468.000000, reward total was -18.000000. running mean: -19.652391\n",
            "resetting env. episode 1469.000000, reward total was -21.000000. running mean: -19.665867\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -19.679209\n",
            "resetting env. episode 1471.000000, reward total was -19.000000. running mean: -19.672417\n",
            "resetting env. episode 1472.000000, reward total was -18.000000. running mean: -19.655692\n",
            "resetting env. episode 1473.000000, reward total was -19.000000. running mean: -19.649135\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -19.662644\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -19.676018\n",
            "resetting env. episode 1476.000000, reward total was -19.000000. running mean: -19.669257\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -19.682565\n",
            "resetting env. episode 1478.000000, reward total was -18.000000. running mean: -19.665739\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -19.679082\n",
            "resetting env. episode 1480.000000, reward total was -19.000000. running mean: -19.672291\n",
            "resetting env. episode 1481.000000, reward total was -20.000000. running mean: -19.675568\n",
            "resetting env. episode 1482.000000, reward total was -20.000000. running mean: -19.678812\n",
            "resetting env. episode 1483.000000, reward total was -18.000000. running mean: -19.662024\n",
            "resetting env. episode 1484.000000, reward total was -18.000000. running mean: -19.645404\n",
            "resetting env. episode 1485.000000, reward total was -18.000000. running mean: -19.628950\n",
            "resetting env. episode 1486.000000, reward total was -20.000000. running mean: -19.632661\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -19.646334\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -19.649871\n",
            "resetting env. episode 1489.000000, reward total was -21.000000. running mean: -19.663372\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -19.666738\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -19.670071\n",
            "resetting env. episode 1492.000000, reward total was -19.000000. running mean: -19.663370\n",
            "resetting env. episode 1493.000000, reward total was -20.000000. running mean: -19.666736\n",
            "resetting env. episode 1494.000000, reward total was -18.000000. running mean: -19.650069\n",
            "resetting env. episode 1495.000000, reward total was -18.000000. running mean: -19.633568\n",
            "resetting env. episode 1496.000000, reward total was -18.000000. running mean: -19.617233\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -19.631060\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -19.644750\n",
            "resetting env. episode 1499.000000, reward total was -19.000000. running mean: -19.638302\n",
            "resetting env. episode 1500.000000, reward total was -18.000000. running mean: -19.621919\n",
            "CPU times: user 6h 13min 44s, sys: 55min 58s, total: 7h 9min 42s\n",
            "Wall time: 3h 42min 28s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "e3a742e8-2909-497e-b3b6-29fe79bfd821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}