{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VIJAYA_C0851848_200_Neurons.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64a428e-6f41-416c-ebc6-8963191b6abf"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null\n",
        "!pip install JSAnimation >/dev/null\n",
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 14.7 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=bcdd050079a80302bd3af5f7083e3d566dccbc73e0b7afc48b0b118025eb1fae\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "72988169-d09e-4c3f-f286-79d3d0bcbec5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "bb9a393c-532e-45fb-bbb7-8711a463c69d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "27d27004-eea8-48cd-f695-932ad5e860cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "f5da7aec-4571-4e3a-da1c-4916a23877e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -19.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "e31b5721-d9fa-4cb4-9575-ee75ac38f8ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.990297\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.990394\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.990490\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.980585\n",
            "resetting env. episode 10.000000, reward total was -18.000000. running mean: -20.950779\n",
            "resetting env. episode 11.000000, reward total was -18.000000. running mean: -20.921272\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.922059\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.922838\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.923610\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.924374\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.925130\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.925879\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.926620\n",
            "resetting env. episode 19.000000, reward total was -19.000000. running mean: -20.907354\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -20.888280\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.879397\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.880603\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.881797\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -20.862979\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.844350\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.845906\n",
            "resetting env. episode 27.000000, reward total was -19.000000. running mean: -20.827447\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.829173\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.830881\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.822572\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.824346\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.816103\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.817942\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.809762\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.811665\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.813548\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.815413\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.817259\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.819086\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.810895\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.802786\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.804758\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.806711\n",
            "resetting env. episode 44.000000, reward total was -18.000000. running mean: -20.778644\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.780857\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.783049\n",
            "resetting env. episode 47.000000, reward total was -18.000000. running mean: -20.755218\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.747666\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.750189\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.752687\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.745161\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.747709\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.740232\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.732829\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.735501\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.738146\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.740765\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.733357\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.736024\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.728663\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.721377\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.724163\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.726921\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.729652\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.732356\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.725032\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.717782\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.710604\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.713498\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.716363\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.709199\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.712107\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.704986\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.707936\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.700857\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.703848\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.706810\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -20.689742\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -20.672844\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.656116\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -20.639555\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.643159\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.646728\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.650260\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.653758\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.657220\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.660648\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.664041\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.667401\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.670727\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -20.654020\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.657480\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.660905\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.664296\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.657653\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.661076\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.654465\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.647921\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.651442\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.654927\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.658378\n",
            "resetting env. episode 102.000000, reward total was -19.000000. running mean: -20.641794\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.645376\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.648922\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.652433\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.645909\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.649450\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.652955\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.646426\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.639961\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.643562\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.637126\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.630755\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.624447\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.628203\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.621921\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.625702\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.629445\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.633150\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.636819\n",
            "resetting env. episode 121.000000, reward total was -19.000000. running mean: -20.620451\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.624246\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.628004\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.631724\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.635406\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.639052\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.642662\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.636235\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.639873\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.633474\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.637139\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.640768\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.634360\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.628017\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.631736\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.635419\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.639065\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.642674\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.636248\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.639885\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.643486\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.637051\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.640681\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.644274\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.647831\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -20.631353\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.635039\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.638689\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.642302\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.625879\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.629620\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.633324\n",
            "resetting env. episode 153.000000, reward total was -19.000000. running mean: -20.616991\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.610821\n",
            "resetting env. episode 155.000000, reward total was -18.000000. running mean: -20.584713\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.588866\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.582977\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.587147\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.591276\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.595363\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.589409\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.583515\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.577680\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.571903\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.576184\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.580422\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.584618\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.588772\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.582884\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.577055\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.571285\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.575572\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.559816\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.554218\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.548676\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.553189\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.557657\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.552081\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.556560\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.560994\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.565384\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.559731\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.564133\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.568492\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.572807\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.577079\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.581308\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.575495\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.569740\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.574043\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.578302\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.582519\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.566694\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.571027\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.565317\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -20.549664\n",
            "resetting env. episode 197.000000, reward total was -18.000000. running mean: -20.524167\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.528925\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.533636\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.538300\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.542917\n",
            "resetting env. episode 202.000000, reward total was -19.000000. running mean: -20.527488\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.532213\n",
            "resetting env. episode 204.000000, reward total was -17.000000. running mean: -20.496891\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.491922\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.497002\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.492032\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.497112\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.502141\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.507120\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.502048\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.507028\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.511958\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.506838\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.501770\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.506752\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.501684\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.506668\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.511601\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.516485\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.521320\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.526107\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.520846\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.515637\n",
            "resetting env. episode 225.000000, reward total was -19.000000. running mean: -20.500481\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.495476\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.500521\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.495516\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.500561\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.505555\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.500500\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.505495\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.510440\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.515336\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.510182\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.495080\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.500130\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.495128\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.490177\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.495275\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.490322\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.475419\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.480665\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.475858\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.471100\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.466389\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.461725\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.467108\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.472437\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.477712\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.482935\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.488106\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.493225\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.488292\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.483410\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.488575\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.483690\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.488853\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.473964\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.469225\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.474532\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.469787\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.465089\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.460438\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.465834\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.471176\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.466464\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.471799\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.477081\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.472310\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.467587\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.472911\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.468182\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.463500\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.468865\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.474177\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.469435\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.464741\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.460093\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.455492\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.450937\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.446428\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.451964\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.457444\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.462870\n",
            "resetting env. episode 286.000000, reward total was -18.000000. running mean: -20.438241\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.423859\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.429620\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.435324\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.440971\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.446561\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.452095\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.457574\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.442999\n",
            "resetting env. episode 295.000000, reward total was -19.000000. running mean: -20.428569\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.434283\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.429940\n",
            "resetting env. episode 298.000000, reward total was -19.000000. running mean: -20.415641\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.421484\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.407269\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.413197\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.419065\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.414874\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.400725\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.396718\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.402751\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.408723\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.404636\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.410590\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.406484\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.402419\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.388395\n",
            "resetting env. episode 313.000000, reward total was -18.000000. running mean: -20.364511\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.370866\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.377157\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.383386\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.379552\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.385756\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.381899\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.388080\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.384199\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.390357\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.386453\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.392589\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.388663\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.374776\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.381028\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.387218\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.383346\n",
            "resetting env. episode 330.000000, reward total was -19.000000. running mean: -20.369513\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.375817\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.382059\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.388239\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.394356\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.400413\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.406409\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.412345\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.418221\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.424039\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.429798\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.435500\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.431145\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.436834\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.442466\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.448041\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.443561\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.449125\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.454634\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.450087\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.445587\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.431131\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.436819\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.422451\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.418227\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.424044\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.419804\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.425606\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.431350\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.437036\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.442666\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.448239\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.453757\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.439219\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.444827\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -20.430379\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.436075\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.441714\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.447297\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.452824\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.458296\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.453713\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.449176\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.444684\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.450237\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.445735\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.451278\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.446765\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.452297\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.447774\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.443296\n",
            "resetting env. episode 381.000000, reward total was -18.000000. running mean: -20.418863\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.424675\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.410428\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.406324\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.402261\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.408238\n",
            "resetting env. episode 387.000000, reward total was -18.000000. running mean: -20.384156\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -20.370314\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.366611\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.372945\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.369215\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.365523\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.371868\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.378149\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.384368\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -20.370524\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.366819\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.373151\n",
            "resetting env. episode 399.000000, reward total was -19.000000. running mean: -20.359419\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.365825\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.362167\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.368545\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.374860\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.381111\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.367300\n",
            "resetting env. episode 406.000000, reward total was -19.000000. running mean: -20.353627\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.350091\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.346590\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.353124\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -20.339593\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.336197\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.342835\n",
            "resetting env. episode 413.000000, reward total was -18.000000. running mean: -20.319406\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.316212\n",
            "resetting env. episode 415.000000, reward total was -19.000000. running mean: -20.303050\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.310020\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.306919\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.303850\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.300812\n",
            "resetting env. episode 420.000000, reward total was -19.000000. running mean: -20.287804\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.284926\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.292076\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.289156\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.296264\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.303301\n",
            "resetting env. episode 426.000000, reward total was -19.000000. running mean: -20.290268\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.287366\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.294492\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.301547\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.298532\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.295546\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.302591\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.309565\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.316469\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -20.303305\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.310272\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.317169\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.323997\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.330757\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.327450\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.334175\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.340833\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.347425\n",
            "resetting env. episode 444.000000, reward total was -15.000000. running mean: -20.293951\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.281011\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.288201\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.275319\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.262566\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.259940\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.267341\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.274668\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.271921\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.279202\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.286410\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.293546\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.300610\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.307604\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.304528\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.311483\n",
            "resetting env. episode 460.000000, reward total was -18.000000. running mean: -20.288368\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.295484\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.302529\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.309504\n",
            "resetting env. episode 464.000000, reward total was -19.000000. running mean: -20.296409\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.303445\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.310410\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.307306\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.314233\n",
            "resetting env. episode 469.000000, reward total was -19.000000. running mean: -20.301091\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.298080\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.295099\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.282148\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.289327\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.296433\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.303469\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.310434\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.307330\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.314257\n",
            "resetting env. episode 479.000000, reward total was -18.000000. running mean: -20.291114\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.278203\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.275421\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.282667\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.289840\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.296942\n",
            "resetting env. episode 485.000000, reward total was -18.000000. running mean: -20.273972\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.281233\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.288420\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.295536\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.292581\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.289655\n",
            "resetting env. episode 491.000000, reward total was -19.000000. running mean: -20.276758\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.283991\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.291151\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.288239\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.295357\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.282403\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.289579\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.286684\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.283817\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.290979\n",
            "CPU times: user 22min 29s, sys: 10min 27s, total: 32min 57s\n",
            "Wall time: 17min 6s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "13728bbc-de14-4cdf-8460-2b18f8b57dbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.990297\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.980394\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.970590\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -20.950884\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -20.931375\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.932062\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.932741\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.923414\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.914179\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.895038\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.896087\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.897126\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.898155\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.899174\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.890182\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.891280\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.892367\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.893444\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -20.874509\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.855764\n",
            "resetting env. episode 26.000000, reward total was -19.000000. running mean: -20.837206\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.828834\n",
            "resetting env. episode 28.000000, reward total was -18.000000. running mean: -20.800546\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.802541\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.794515\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.786570\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.788704\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.780817\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.783009\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.785179\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -20.767327\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.769654\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.771957\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.774238\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.776495\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.768730\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.771043\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.773333\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.775599\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.777843\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.780065\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.782264\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.774442\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.766697\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.759030\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.751440\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.753926\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.746386\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.738922\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.731533\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.724218\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.726976\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.729706\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.722409\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.725185\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.717933\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.710754\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.703646\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.706610\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.709544\n",
            "resetting env. episode 66.000000, reward total was -17.000000. running mean: -20.672448\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.665724\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.659066\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.652476\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.645951\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.639491\n",
            "resetting env. episode 72.000000, reward total was -18.000000. running mean: -20.613097\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.616966\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.620796\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.614588\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.618442\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.622258\n",
            "resetting env. episode 78.000000, reward total was -18.000000. running mean: -20.596035\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -20.580075\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.584274\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.578431\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.582647\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.586820\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.580952\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -20.565143\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.559491\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.563896\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.568257\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.562575\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.566949\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.571280\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.575567\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.579811\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.574013\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.578273\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.582490\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.576665\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.580899\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.565090\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.569439\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.573744\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.578007\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.582227\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.586405\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.590541\n",
            "resetting env. episode 106.000000, reward total was -19.000000. running mean: -20.574635\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.578889\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.573100\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.577369\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.581595\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.575779\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.580021\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.584221\n",
            "resetting env. episode 114.000000, reward total was -18.000000. running mean: -20.558379\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -20.542795\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.547367\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.551894\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.556375\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.560811\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.565203\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.559551\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.553955\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.558416\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.562832\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.567203\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.571531\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.565816\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.550158\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.554656\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.559110\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.563519\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.567883\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.572205\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.576482\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.570718\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.575010\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.579260\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.583468\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.587633\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.581757\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.585939\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.590080\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.594179\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.598237\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -20.582255\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.576432\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.570668\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.564961\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.569312\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.553619\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.558082\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.562502\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.566877\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.561208\n",
            "resetting env. episode 155.000000, reward total was -19.000000. running mean: -20.545596\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.540140\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.544738\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.549291\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.553798\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.558260\n",
            "resetting env. episode 161.000000, reward total was -19.000000. running mean: -20.542677\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.537251\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.541878\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.536459\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.541095\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.535684\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.540327\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.534924\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.539574\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.544179\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.528737\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.523450\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.508215\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.513133\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.508002\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.502922\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.497892\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.502913\n",
            "resetting env. episode 179.000000, reward total was -18.000000. running mean: -20.477884\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.483105\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.478274\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.483492\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.488657\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -20.473770\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.479032\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -20.464242\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.459600\n",
            "resetting env. episode 188.000000, reward total was -19.000000. running mean: -20.445004\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.440554\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.436148\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.441787\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.437369\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.442995\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.448565\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.454080\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.459539\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.464943\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.470294\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.465591\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.470935\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.476226\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.481463\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.486649\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.491782\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.496865\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.501896\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.496877\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.501908\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.496889\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.501920\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.496901\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.501932\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.506913\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.511844\n",
            "resetting env. episode 215.000000, reward total was -18.000000. running mean: -20.486725\n",
            "resetting env. episode 216.000000, reward total was -16.000000. running mean: -20.441858\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.447439\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.452965\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.458435\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.463851\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.459212\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.454620\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.460074\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.445473\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.451019\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.446508\n",
            "resetting env. episode 227.000000, reward total was -18.000000. running mean: -20.422043\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.427823\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.433545\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.439209\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.444817\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.450369\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.445865\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.441407\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.446993\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.452523\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.447997\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.443517\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.439082\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.444691\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.430244\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.435942\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.431583\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.437267\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.432894\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.428565\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.424280\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.430037\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.425736\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.431479\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.437164\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.442793\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.438365\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.433981\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.439641\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.435245\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.440892\n",
            "resetting env. episode 258.000000, reward total was -18.000000. running mean: -20.416483\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.412319\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.418195\n",
            "resetting env. episode 261.000000, reward total was -19.000000. running mean: -20.404013\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.399973\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.395974\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.392014\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.398094\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.404113\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.400072\n",
            "resetting env. episode 268.000000, reward total was -18.000000. running mean: -20.376071\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.382310\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.388487\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.384602\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.390756\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.396849\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.402880\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.408851\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.414763\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.410615\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.406509\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.412444\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.408320\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.414236\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.420094\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.425893\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.431634\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.437318\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.432945\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.438615\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.444229\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.449787\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.455289\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.450736\n",
            "resetting env. episode 292.000000, reward total was -19.000000. running mean: -20.436229\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.441866\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.427448\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.433173\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.438841\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.434453\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.430108\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.435807\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.441449\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.437035\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.432664\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.428338\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.434054\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.429714\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.435417\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.441063\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.446652\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.452185\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.447664\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.453187\n",
            "resetting env. episode 312.000000, reward total was -17.000000. running mean: -20.418655\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.424469\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.430224\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.435922\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.441562\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.447147\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.452675\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.458149\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.453567\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.459031\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.464441\n",
            "resetting env. episode 323.000000, reward total was -18.000000. running mean: -20.439797\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.445399\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.450945\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.456435\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.451871\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.447352\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.432879\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.438550\n",
            "resetting env. episode 331.000000, reward total was -19.000000. running mean: -20.424164\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.419923\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.425724\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.431466\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.437152\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.432780\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.438452\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.444068\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.449627\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.445131\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.450680\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.456173\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.451611\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.447095\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.442624\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.448198\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.443716\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.449279\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.454786\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.450238\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.445736\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.441278\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.446865\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.452397\n",
            "resetting env. episode 355.000000, reward total was -18.000000. running mean: -20.427873\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.433594\n",
            "resetting env. episode 357.000000, reward total was -17.000000. running mean: -20.399258\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.405266\n",
            "resetting env. episode 359.000000, reward total was -19.000000. running mean: -20.391213\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.387301\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.393428\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.399493\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.405499\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.401444\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.407429\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.413355\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.419221\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.415029\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.420879\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.426670\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.432403\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.438079\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.443698\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.439261\n",
            "resetting env. episode 375.000000, reward total was -18.000000. running mean: -20.414869\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.420720\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.416513\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.402348\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.398324\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.404341\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.390298\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.396395\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.402431\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.408406\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.414322\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.420179\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.425977\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.431718\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.437400\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.443026\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.448596\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.454110\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.459569\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.464973\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.470324\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.465620\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.470964\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.476255\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.471492\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.466777\n",
            "resetting env. episode 401.000000, reward total was -18.000000. running mean: -20.442109\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.447688\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.453211\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.458679\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.464092\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.469452\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.464757\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.470109\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.455408\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.450854\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.446346\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.441882\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.447463\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.432989\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.438659\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -20.424272\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.410030\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.415929\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.401770\n",
            "resetting env. episode 420.000000, reward total was -18.000000. running mean: -20.377752\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.383975\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.380135\n",
            "resetting env. episode 423.000000, reward total was -19.000000. running mean: -20.366334\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.362670\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.349044\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.355553\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.361998\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.368378\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.374694\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.380947\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.387138\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.393266\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.399334\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.385340\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.391487\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.397572\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.403596\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.409560\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.405465\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.411410\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.417296\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.423123\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.428892\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.424603\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.430357\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.436053\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.431693\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.417376\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.423202\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.428970\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.434680\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.430333\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.426030\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.431770\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.427452\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.433178\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.428846\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.434557\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.440212\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.445810\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.451352\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.456838\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.452270\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.457747\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.463170\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.468538\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.473852\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.469114\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.464423\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.469779\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.465081\n",
            "resetting env. episode 472.000000, reward total was -17.000000. running mean: -20.430430\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.426126\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.431864\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.437546\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.433170\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.438839\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.434450\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.440106\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.425705\n",
            "resetting env. episode 481.000000, reward total was -19.000000. running mean: -20.411448\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.417333\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.413160\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.409028\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.414938\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.420789\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.416581\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.422415\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.428191\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.423909\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.419670\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.415473\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.411318\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -20.397205\n",
            "resetting env. episode 495.000000, reward total was -16.000000. running mean: -20.353233\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.359701\n",
            "resetting env. episode 497.000000, reward total was -17.000000. running mean: -20.326104\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.312843\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.319714\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.326517\n",
            "CPU times: user 22min 40s, sys: 10min 22s, total: 33min 2s\n",
            "Wall time: 17min 2s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "4ceaf3c4-1da3-40ee-b2e1-162657caa63f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "3dc047a0-c207-40be-df21-3cde4aff4be1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -19.020000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.039800\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.059402\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -19.078808\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.098020\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -19.107040\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -19.125969\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.144710\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -19.143263\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.161830\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -19.170212\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.188509\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -19.196624\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -19.204658\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -19.212612\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -19.230485\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -19.248181\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -19.255699\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -19.263142\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -19.270510\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -19.287805\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.304927\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -19.321878\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -19.328659\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.345373\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.361919\n",
            "resetting env. episode 28.000000, reward total was -18.000000. running mean: -19.348300\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -19.344817\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.361368\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.367755\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -19.384077\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.400236\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -19.406234\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -19.422172\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.437950\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -19.433571\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.449235\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -19.464743\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.480095\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -19.485294\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -19.500441\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -19.515437\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.530282\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.544980\n",
            "resetting env. episode 46.000000, reward total was -18.000000. running mean: -19.529530\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.544234\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.558792\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.573204\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -19.577472\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.591697\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -19.605780\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -19.609723\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.623625\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.637389\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.651015\n",
            "resetting env. episode 57.000000, reward total was -18.000000. running mean: -19.634505\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.648160\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -19.641679\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -19.645262\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -19.648809\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.662321\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.675698\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.688941\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -19.692051\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.705131\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -19.708080\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.720999\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.733789\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.746451\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -19.748986\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -19.751497\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.763982\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -19.776342\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -19.778578\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -19.780793\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.792985\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.805055\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.817004\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.828834\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -19.830546\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -19.832240\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.843918\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.855479\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.866924\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -19.878255\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.889472\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -19.890578\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.901672\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.912655\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.923528\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -19.924293\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -19.925050\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -19.925800\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -19.926542\n",
            "resetting env. episode 96.000000, reward total was -19.000000. running mean: -19.917276\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -19.908104\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -19.899023\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.910032\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -19.900932\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -19.891923\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -19.893003\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -19.904073\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.915033\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.925882\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.936624\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.947257\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.957785\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.968207\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.978525\n",
            "resetting env. episode 111.000000, reward total was -17.000000. running mean: -19.948740\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -19.949252\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -19.959760\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.970162\n",
            "resetting env. episode 115.000000, reward total was -18.000000. running mean: -19.950460\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.960956\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.971346\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -19.971633\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.971916\n",
            "resetting env. episode 120.000000, reward total was -18.000000. running mean: -19.952197\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.962675\n",
            "resetting env. episode 122.000000, reward total was -19.000000. running mean: -19.953049\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -19.963518\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -19.973883\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -19.984144\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -19.984303\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -19.994460\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -19.994515\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.004570\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.004524\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.014479\n",
            "resetting env. episode 132.000000, reward total was -18.000000. running mean: -19.994334\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.004391\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.014347\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.014203\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.024061\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.033821\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.033483\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.033148\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.032816\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.042488\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -20.032063\n",
            "resetting env. episode 143.000000, reward total was -17.000000. running mean: -20.001743\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.011725\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.021608\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.021392\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.031178\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.040866\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.040457\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.040053\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.039652\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.049256\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.058763\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.068176\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.077494\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.086719\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.085852\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.094993\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.104043\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.113003\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.111873\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.120754\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.129547\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.138251\n",
            "resetting env. episode 165.000000, reward total was -19.000000. running mean: -20.126869\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.125600\n",
            "resetting env. episode 167.000000, reward total was -18.000000. running mean: -20.104344\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.103300\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.102267\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.111245\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.120132\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.128931\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.117642\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.116465\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.125301\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.134048\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.122707\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.131480\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.140165\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.148764\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.147276\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.145803\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.144345\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.152902\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.151373\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.159859\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.158260\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.166678\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.165011\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.173361\n",
            "resetting env. episode 191.000000, reward total was -19.000000. running mean: -20.161627\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.160011\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.168411\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.176727\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.184960\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -20.173110\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.181379\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.189565\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.187669\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.195793\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.203835\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.211796\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.219678\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.217482\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.215307\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.203154\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.211122\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.219011\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.226821\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.234553\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.242207\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.249785\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.257287\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.264714\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.272067\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.269347\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.266653\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.273987\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.271247\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.268534\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.275849\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.283090\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.280260\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.277457\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.284682\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.281836\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.289017\n",
            "resetting env. episode 228.000000, reward total was -19.000000. running mean: -20.276127\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.263366\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.270732\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.278025\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.285245\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.292392\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.279468\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.286673\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.283807\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.290969\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.288059\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.295178\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.302227\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.289204\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.296312\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.293349\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.300416\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.297412\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.284437\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.291593\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.298677\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.305690\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.312633\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.309507\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.316412\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.323248\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.320015\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.326815\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.333547\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.340212\n",
            "resetting env. episode 258.000000, reward total was -18.000000. running mean: -20.316810\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.313641\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.320505\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.317300\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.324127\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.330886\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.337577\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.334201\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.340859\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.347450\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.353976\n",
            "resetting env. episode 269.000000, reward total was -19.000000. running mean: -20.340436\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.327032\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.333762\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.340424\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.347020\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.353549\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.360014\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.356414\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.362850\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.359221\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.355629\n",
            "resetting env. episode 280.000000, reward total was -18.000000. running mean: -20.332073\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.338752\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.345364\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.341911\n",
            "resetting env. episode 284.000000, reward total was -19.000000. running mean: -20.328492\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.325207\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.331955\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.338635\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.345249\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.351796\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.338278\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.344896\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.341447\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.338032\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.334652\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.341305\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.347892\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.344413\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.350969\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.357460\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.353885\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.350346\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.346843\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.343374\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.349940\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.356441\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.352877\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.349348\n",
            "resetting env. episode 308.000000, reward total was -19.000000. running mean: -20.335854\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.332496\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.339171\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.345779\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.342321\n",
            "resetting env. episode 313.000000, reward total was -19.000000. running mean: -20.328898\n",
            "resetting env. episode 314.000000, reward total was -18.000000. running mean: -20.305609\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.302553\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.309528\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.316432\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.323268\n",
            "resetting env. episode 319.000000, reward total was -19.000000. running mean: -20.310035\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -20.296935\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.303966\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.310926\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.307817\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.304739\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.311691\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.318574\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.325388\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.312135\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.319013\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.315823\n",
            "resetting env. episode 331.000000, reward total was -19.000000. running mean: -20.302665\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.309638\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.316542\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.313376\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.320243\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.327040\n",
            "resetting env. episode 337.000000, reward total was -19.000000. running mean: -20.313770\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.320632\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.307426\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.314352\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.311208\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.308096\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.315015\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.321865\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.328646\n",
            "resetting env. episode 346.000000, reward total was -19.000000. running mean: -20.315360\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.302206\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.309184\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.316092\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.312931\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.309802\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.306704\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.303637\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.300601\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.307595\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.304519\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.311473\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.308359\n",
            "resetting env. episode 359.000000, reward total was -19.000000. running mean: -20.295275\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.302322\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.309299\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.316206\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.303044\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.300014\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.307014\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.303943\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.310904\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.317795\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.304617\n",
            "resetting env. episode 370.000000, reward total was -19.000000. running mean: -20.291571\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.288655\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.295769\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.292811\n",
            "resetting env. episode 374.000000, reward total was -19.000000. running mean: -20.279883\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.277084\n",
            "resetting env. episode 376.000000, reward total was -19.000000. running mean: -20.264313\n",
            "resetting env. episode 377.000000, reward total was -19.000000. running mean: -20.251670\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.239153\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.246762\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.254294\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.241751\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.249334\n",
            "resetting env. episode 383.000000, reward total was -18.000000. running mean: -20.226840\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.224572\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.222326\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.210103\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.208002\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.205922\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.213863\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.221724\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.219507\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.217312\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.225139\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.232887\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.240558\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.248153\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.245671\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.243215\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.240782\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.248375\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.235891\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.233532\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.231197\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.228885\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.236596\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.244230\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.251788\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.249270\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.246777\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.254309\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.251766\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.259248\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.266656\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.263989\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.271349\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.268636\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.265950\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.263290\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.260657\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.268051\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.265370\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.262716\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.270089\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -20.257388\n",
            "resetting env. episode 425.000000, reward total was -16.000000. running mean: -20.214815\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.222666\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.220440\n",
            "resetting env. episode 428.000000, reward total was -18.000000. running mean: -20.198235\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.196253\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.194290\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.202348\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.200324\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.198321\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.206338\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.204274\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.202231\n",
            "resetting env. episode 437.000000, reward total was -19.000000. running mean: -20.190209\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.198307\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.206324\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.204261\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.202218\n",
            "resetting env. episode 442.000000, reward total was -18.000000. running mean: -20.180196\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.178394\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.186610\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.184744\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.192897\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.190968\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.199058\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.187067\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.185197\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.193345\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.201411\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.209397\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.207303\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.215230\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.223078\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.230847\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.218539\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.226353\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.234090\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.231749\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.239431\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.237037\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.234667\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -20.222320\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.230097\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.237796\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.245418\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.252964\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.250434\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.247930\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.245450\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.252996\n",
            "resetting env. episode 474.000000, reward total was -18.000000. running mean: -20.230466\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.228161\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.235880\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -20.223521\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.221286\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.209073\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.216982\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.214812\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.222664\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.220437\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.218233\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.226051\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.223790\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.231552\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.219237\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.227044\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.234774\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.242426\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.240002\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.247602\n",
            "resetting env. episode 494.000000, reward total was -17.000000. running mean: -20.215126\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.222975\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.230745\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.238437\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.246053\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.253593\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.261057\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.258446\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.265862\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -20.263203\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.270571\n",
            "resetting env. episode 505.000000, reward total was -20.000000. running mean: -20.267865\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -20.265187\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -20.262535\n",
            "resetting env. episode 508.000000, reward total was -19.000000. running mean: -20.249909\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.257410\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.254836\n",
            "resetting env. episode 511.000000, reward total was -19.000000. running mean: -20.242288\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.249865\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.257366\n",
            "resetting env. episode 514.000000, reward total was -20.000000. running mean: -20.254793\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.262245\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.269622\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.276926\n",
            "resetting env. episode 518.000000, reward total was -20.000000. running mean: -20.274157\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.271415\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.278701\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.285914\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.293055\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -20.290124\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.287223\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.284351\n",
            "resetting env. episode 526.000000, reward total was -20.000000. running mean: -20.281507\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.288692\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.295805\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.302847\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.309819\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.316721\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.323553\n",
            "resetting env. episode 533.000000, reward total was -20.000000. running mean: -20.320318\n",
            "resetting env. episode 534.000000, reward total was -20.000000. running mean: -20.317115\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.323944\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.330704\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.337397\n",
            "resetting env. episode 538.000000, reward total was -19.000000. running mean: -20.324023\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.320783\n",
            "resetting env. episode 540.000000, reward total was -20.000000. running mean: -20.317575\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.324399\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.331155\n",
            "resetting env. episode 543.000000, reward total was -19.000000. running mean: -20.317844\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -20.324665\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.331419\n",
            "resetting env. episode 546.000000, reward total was -19.000000. running mean: -20.318105\n",
            "resetting env. episode 547.000000, reward total was -19.000000. running mean: -20.304923\n",
            "resetting env. episode 548.000000, reward total was -19.000000. running mean: -20.291874\n",
            "resetting env. episode 549.000000, reward total was -18.000000. running mean: -20.268955\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.276266\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.283503\n",
            "resetting env. episode 552.000000, reward total was -20.000000. running mean: -20.280668\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.287862\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.294983\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.302033\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.299013\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.306023\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.312962\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.319833\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -20.316634\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -20.323468\n",
            "resetting env. episode 562.000000, reward total was -20.000000. running mean: -20.320233\n",
            "resetting env. episode 563.000000, reward total was -20.000000. running mean: -20.317031\n",
            "resetting env. episode 564.000000, reward total was -20.000000. running mean: -20.313861\n",
            "resetting env. episode 565.000000, reward total was -18.000000. running mean: -20.290722\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.297815\n",
            "resetting env. episode 567.000000, reward total was -20.000000. running mean: -20.294837\n",
            "resetting env. episode 568.000000, reward total was -20.000000. running mean: -20.291888\n",
            "resetting env. episode 569.000000, reward total was -20.000000. running mean: -20.288970\n",
            "resetting env. episode 570.000000, reward total was -19.000000. running mean: -20.276080\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.283319\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.290486\n",
            "resetting env. episode 573.000000, reward total was -19.000000. running mean: -20.277581\n",
            "resetting env. episode 574.000000, reward total was -20.000000. running mean: -20.274805\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.282057\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.289237\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -20.286344\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.293481\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.300546\n",
            "resetting env. episode 580.000000, reward total was -21.000000. running mean: -20.307541\n",
            "resetting env. episode 581.000000, reward total was -20.000000. running mean: -20.304465\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.311420\n",
            "resetting env. episode 583.000000, reward total was -20.000000. running mean: -20.308306\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.315223\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.322071\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.328850\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -20.325562\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.332306\n",
            "resetting env. episode 589.000000, reward total was -19.000000. running mean: -20.318983\n",
            "resetting env. episode 590.000000, reward total was -20.000000. running mean: -20.315793\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.322635\n",
            "resetting env. episode 592.000000, reward total was -20.000000. running mean: -20.319409\n",
            "resetting env. episode 593.000000, reward total was -19.000000. running mean: -20.306215\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.303153\n",
            "resetting env. episode 595.000000, reward total was -20.000000. running mean: -20.300121\n",
            "resetting env. episode 596.000000, reward total was -19.000000. running mean: -20.287120\n",
            "resetting env. episode 597.000000, reward total was -20.000000. running mean: -20.284249\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.281406\n",
            "resetting env. episode 599.000000, reward total was -20.000000. running mean: -20.278592\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.285806\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.292948\n",
            "resetting env. episode 602.000000, reward total was -18.000000. running mean: -20.270019\n",
            "resetting env. episode 603.000000, reward total was -19.000000. running mean: -20.257319\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.264745\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.272098\n",
            "resetting env. episode 606.000000, reward total was -19.000000. running mean: -20.259377\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.266783\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.274115\n",
            "resetting env. episode 609.000000, reward total was -21.000000. running mean: -20.281374\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.288560\n",
            "resetting env. episode 611.000000, reward total was -20.000000. running mean: -20.285675\n",
            "resetting env. episode 612.000000, reward total was -20.000000. running mean: -20.282818\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.289990\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -20.287090\n",
            "resetting env. episode 615.000000, reward total was -20.000000. running mean: -20.284219\n",
            "resetting env. episode 616.000000, reward total was -21.000000. running mean: -20.291377\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.298463\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -20.295479\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.292524\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -20.289599\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.296703\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.303736\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.310698\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.317591\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.324415\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.331171\n",
            "resetting env. episode 627.000000, reward total was -19.000000. running mean: -20.317859\n",
            "resetting env. episode 628.000000, reward total was -20.000000. running mean: -20.314681\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.321534\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.328319\n",
            "resetting env. episode 631.000000, reward total was -20.000000. running mean: -20.325035\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.331785\n",
            "resetting env. episode 633.000000, reward total was -20.000000. running mean: -20.328467\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.335183\n",
            "resetting env. episode 635.000000, reward total was -19.000000. running mean: -20.321831\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.328612\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.335326\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.341973\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.348553\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.345068\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.351617\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.348101\n",
            "resetting env. episode 643.000000, reward total was -18.000000. running mean: -20.324620\n",
            "resetting env. episode 644.000000, reward total was -19.000000. running mean: -20.311374\n",
            "resetting env. episode 645.000000, reward total was -19.000000. running mean: -20.298260\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.305277\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.312225\n",
            "resetting env. episode 648.000000, reward total was -20.000000. running mean: -20.309102\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.316011\n",
            "resetting env. episode 650.000000, reward total was -19.000000. running mean: -20.302851\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.309823\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.316725\n",
            "resetting env. episode 653.000000, reward total was -21.000000. running mean: -20.323557\n",
            "resetting env. episode 654.000000, reward total was -20.000000. running mean: -20.320322\n",
            "resetting env. episode 655.000000, reward total was -19.000000. running mean: -20.307118\n",
            "resetting env. episode 656.000000, reward total was -19.000000. running mean: -20.294047\n",
            "resetting env. episode 657.000000, reward total was -20.000000. running mean: -20.291107\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.298196\n",
            "resetting env. episode 659.000000, reward total was -20.000000. running mean: -20.295214\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.302262\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.309239\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.306147\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.313085\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.319954\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -20.316755\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.323587\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.330351\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.337048\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.343677\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.350241\n",
            "resetting env. episode 671.000000, reward total was -19.000000. running mean: -20.336738\n",
            "resetting env. episode 672.000000, reward total was -21.000000. running mean: -20.343371\n",
            "resetting env. episode 673.000000, reward total was -18.000000. running mean: -20.319937\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.326738\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.333470\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -20.340136\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.336734\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.343367\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.349933\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -20.346434\n",
            "resetting env. episode 681.000000, reward total was -19.000000. running mean: -20.332970\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.329640\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.326344\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.323080\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.329849\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.336551\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -20.343185\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.349753\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.356256\n",
            "resetting env. episode 690.000000, reward total was -17.000000. running mean: -20.322693\n",
            "resetting env. episode 691.000000, reward total was -21.000000. running mean: -20.329466\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.336172\n",
            "resetting env. episode 693.000000, reward total was -19.000000. running mean: -20.322810\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.329582\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.336286\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.342923\n",
            "resetting env. episode 697.000000, reward total was -20.000000. running mean: -20.339494\n",
            "resetting env. episode 698.000000, reward total was -20.000000. running mean: -20.336099\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.342738\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.339311\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.345918\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -20.342458\n",
            "resetting env. episode 703.000000, reward total was -19.000000. running mean: -20.329034\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.325743\n",
            "resetting env. episode 705.000000, reward total was -21.000000. running mean: -20.332486\n",
            "resetting env. episode 706.000000, reward total was -19.000000. running mean: -20.319161\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -20.325970\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -20.322710\n",
            "resetting env. episode 709.000000, reward total was -19.000000. running mean: -20.309483\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -20.306388\n",
            "resetting env. episode 711.000000, reward total was -20.000000. running mean: -20.303324\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -20.300291\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.307288\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -20.314215\n",
            "resetting env. episode 715.000000, reward total was -19.000000. running mean: -20.301073\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.308062\n",
            "resetting env. episode 717.000000, reward total was -20.000000. running mean: -20.304982\n",
            "resetting env. episode 718.000000, reward total was -19.000000. running mean: -20.291932\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.299012\n",
            "resetting env. episode 720.000000, reward total was -18.000000. running mean: -20.276022\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.283262\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.290429\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.297525\n",
            "resetting env. episode 724.000000, reward total was -19.000000. running mean: -20.284550\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.291704\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.298787\n",
            "resetting env. episode 727.000000, reward total was -19.000000. running mean: -20.285799\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.292941\n",
            "resetting env. episode 729.000000, reward total was -19.000000. running mean: -20.280012\n",
            "resetting env. episode 730.000000, reward total was -20.000000. running mean: -20.277212\n",
            "resetting env. episode 731.000000, reward total was -20.000000. running mean: -20.274440\n",
            "resetting env. episode 732.000000, reward total was -18.000000. running mean: -20.251695\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.259178\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.256587\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.264021\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -20.271381\n",
            "resetting env. episode 737.000000, reward total was -18.000000. running mean: -20.248667\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.256180\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -20.263618\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.270982\n",
            "resetting env. episode 741.000000, reward total was -20.000000. running mean: -20.268272\n",
            "resetting env. episode 742.000000, reward total was -19.000000. running mean: -20.255590\n",
            "resetting env. episode 743.000000, reward total was -18.000000. running mean: -20.233034\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.240703\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.248296\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -20.255813\n",
            "resetting env. episode 747.000000, reward total was -20.000000. running mean: -20.253255\n",
            "resetting env. episode 748.000000, reward total was -20.000000. running mean: -20.250723\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.258215\n",
            "resetting env. episode 750.000000, reward total was -20.000000. running mean: -20.255633\n",
            "resetting env. episode 751.000000, reward total was -19.000000. running mean: -20.243077\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -20.240646\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.248240\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -20.245757\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -20.243300\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -20.250867\n",
            "resetting env. episode 757.000000, reward total was -20.000000. running mean: -20.248358\n",
            "resetting env. episode 758.000000, reward total was -19.000000. running mean: -20.235875\n",
            "resetting env. episode 759.000000, reward total was -20.000000. running mean: -20.233516\n",
            "resetting env. episode 760.000000, reward total was -20.000000. running mean: -20.231181\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.238869\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.246480\n",
            "resetting env. episode 763.000000, reward total was -21.000000. running mean: -20.254015\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -20.251475\n",
            "resetting env. episode 765.000000, reward total was -18.000000. running mean: -20.228960\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.236671\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.244304\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.251861\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.259342\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.266749\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -20.274082\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -20.271341\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -20.278627\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.285841\n",
            "resetting env. episode 775.000000, reward total was -19.000000. running mean: -20.272983\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -20.270253\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.277550\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -20.284775\n",
            "resetting env. episode 779.000000, reward total was -21.000000. running mean: -20.291927\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.299008\n",
            "resetting env. episode 781.000000, reward total was -20.000000. running mean: -20.296018\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -20.303058\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.310027\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.316927\n",
            "resetting env. episode 785.000000, reward total was -18.000000. running mean: -20.293757\n",
            "resetting env. episode 786.000000, reward total was -20.000000. running mean: -20.290820\n",
            "resetting env. episode 787.000000, reward total was -19.000000. running mean: -20.277912\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.285133\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.292281\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -20.299358\n",
            "resetting env. episode 791.000000, reward total was -19.000000. running mean: -20.286365\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -20.283501\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.290666\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.297759\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.304782\n",
            "resetting env. episode 796.000000, reward total was -20.000000. running mean: -20.301734\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.308717\n",
            "resetting env. episode 798.000000, reward total was -20.000000. running mean: -20.305630\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.312573\n",
            "resetting env. episode 800.000000, reward total was -20.000000. running mean: -20.309448\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.316353\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.323190\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.329958\n",
            "resetting env. episode 804.000000, reward total was -19.000000. running mean: -20.316658\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.323491\n",
            "resetting env. episode 806.000000, reward total was -20.000000. running mean: -20.320257\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.327054\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.333783\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -20.330446\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.337141\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.333770\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -20.340432\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.347028\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.353557\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.360022\n",
            "resetting env. episode 816.000000, reward total was -20.000000. running mean: -20.356422\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.362857\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.369229\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -20.365537\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -20.371881\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.378162\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.384381\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.390537\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.396632\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -20.402665\n",
            "resetting env. episode 826.000000, reward total was -19.000000. running mean: -20.388639\n",
            "resetting env. episode 827.000000, reward total was -21.000000. running mean: -20.394752\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.400805\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.406797\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -20.412729\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -20.418601\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -20.414415\n",
            "resetting env. episode 833.000000, reward total was -19.000000. running mean: -20.400271\n",
            "resetting env. episode 834.000000, reward total was -21.000000. running mean: -20.406269\n",
            "resetting env. episode 835.000000, reward total was -20.000000. running mean: -20.402206\n",
            "resetting env. episode 836.000000, reward total was -19.000000. running mean: -20.388184\n",
            "resetting env. episode 837.000000, reward total was -19.000000. running mean: -20.374302\n",
            "resetting env. episode 838.000000, reward total was -20.000000. running mean: -20.370559\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.376853\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -20.373085\n",
            "resetting env. episode 841.000000, reward total was -19.000000. running mean: -20.359354\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.365760\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -20.372103\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.378382\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.384598\n",
            "resetting env. episode 846.000000, reward total was -20.000000. running mean: -20.380752\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.386944\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -20.383075\n",
            "resetting env. episode 849.000000, reward total was -19.000000. running mean: -20.369244\n",
            "resetting env. episode 850.000000, reward total was -19.000000. running mean: -20.355552\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.361996\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.368376\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.374693\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -20.380946\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.387136\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.393265\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -20.399332\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.405339\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -20.411285\n",
            "resetting env. episode 860.000000, reward total was -20.000000. running mean: -20.407173\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.413101\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -20.418970\n",
            "resetting env. episode 863.000000, reward total was -20.000000. running mean: -20.414780\n",
            "resetting env. episode 864.000000, reward total was -20.000000. running mean: -20.410632\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -20.416526\n",
            "resetting env. episode 866.000000, reward total was -20.000000. running mean: -20.412361\n",
            "resetting env. episode 867.000000, reward total was -20.000000. running mean: -20.408237\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -20.404155\n",
            "resetting env. episode 869.000000, reward total was -20.000000. running mean: -20.400113\n",
            "resetting env. episode 870.000000, reward total was -18.000000. running mean: -20.376112\n",
            "resetting env. episode 871.000000, reward total was -20.000000. running mean: -20.372351\n",
            "resetting env. episode 872.000000, reward total was -20.000000. running mean: -20.368628\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.374941\n",
            "resetting env. episode 874.000000, reward total was -17.000000. running mean: -20.341192\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -20.337780\n",
            "resetting env. episode 876.000000, reward total was -20.000000. running mean: -20.334402\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.341058\n",
            "resetting env. episode 878.000000, reward total was -19.000000. running mean: -20.327648\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.334371\n",
            "resetting env. episode 880.000000, reward total was -20.000000. running mean: -20.331027\n",
            "resetting env. episode 881.000000, reward total was -20.000000. running mean: -20.327717\n",
            "resetting env. episode 882.000000, reward total was -18.000000. running mean: -20.304440\n",
            "resetting env. episode 883.000000, reward total was -18.000000. running mean: -20.281395\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.288582\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -20.295696\n",
            "resetting env. episode 886.000000, reward total was -19.000000. running mean: -20.282739\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.289911\n",
            "resetting env. episode 888.000000, reward total was -20.000000. running mean: -20.287012\n",
            "resetting env. episode 889.000000, reward total was -19.000000. running mean: -20.274142\n",
            "resetting env. episode 890.000000, reward total was -20.000000. running mean: -20.271401\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.278687\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.285900\n",
            "resetting env. episode 893.000000, reward total was -19.000000. running mean: -20.273041\n",
            "resetting env. episode 894.000000, reward total was -20.000000. running mean: -20.270310\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -20.277607\n",
            "resetting env. episode 896.000000, reward total was -20.000000. running mean: -20.274831\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.282083\n",
            "resetting env. episode 898.000000, reward total was -20.000000. running mean: -20.279262\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.286469\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.293605\n",
            "resetting env. episode 901.000000, reward total was -20.000000. running mean: -20.290669\n",
            "resetting env. episode 902.000000, reward total was -19.000000. running mean: -20.277762\n",
            "resetting env. episode 903.000000, reward total was -20.000000. running mean: -20.274984\n",
            "resetting env. episode 904.000000, reward total was -18.000000. running mean: -20.252235\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.259712\n",
            "resetting env. episode 906.000000, reward total was -19.000000. running mean: -20.247115\n",
            "resetting env. episode 907.000000, reward total was -19.000000. running mean: -20.234644\n",
            "resetting env. episode 908.000000, reward total was -18.000000. running mean: -20.212298\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -20.220175\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -20.227973\n",
            "resetting env. episode 911.000000, reward total was -18.000000. running mean: -20.205693\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.213636\n",
            "resetting env. episode 913.000000, reward total was -19.000000. running mean: -20.201500\n",
            "resetting env. episode 914.000000, reward total was -20.000000. running mean: -20.199485\n",
            "resetting env. episode 915.000000, reward total was -19.000000. running mean: -20.187490\n",
            "resetting env. episode 916.000000, reward total was -20.000000. running mean: -20.185615\n",
            "resetting env. episode 917.000000, reward total was -20.000000. running mean: -20.183759\n",
            "resetting env. episode 918.000000, reward total was -20.000000. running mean: -20.181921\n",
            "resetting env. episode 919.000000, reward total was -19.000000. running mean: -20.170102\n",
            "resetting env. episode 920.000000, reward total was -20.000000. running mean: -20.168401\n",
            "resetting env. episode 921.000000, reward total was -20.000000. running mean: -20.166717\n",
            "resetting env. episode 922.000000, reward total was -19.000000. running mean: -20.155050\n",
            "resetting env. episode 923.000000, reward total was -20.000000. running mean: -20.153499\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.161964\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -20.160345\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.168741\n",
            "resetting env. episode 927.000000, reward total was -19.000000. running mean: -20.157054\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.165483\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -20.163829\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.172190\n",
            "resetting env. episode 931.000000, reward total was -20.000000. running mean: -20.170468\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.178764\n",
            "resetting env. episode 933.000000, reward total was -20.000000. running mean: -20.176976\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -20.165206\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.173554\n",
            "resetting env. episode 936.000000, reward total was -21.000000. running mean: -20.181819\n",
            "resetting env. episode 937.000000, reward total was -19.000000. running mean: -20.170000\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -20.178300\n",
            "resetting env. episode 939.000000, reward total was -21.000000. running mean: -20.186517\n",
            "resetting env. episode 940.000000, reward total was -20.000000. running mean: -20.184652\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -20.192806\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -20.200878\n",
            "resetting env. episode 943.000000, reward total was -20.000000. running mean: -20.198869\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -20.196880\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -20.204911\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -20.212862\n",
            "resetting env. episode 947.000000, reward total was -20.000000. running mean: -20.210734\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.218626\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.216440\n",
            "resetting env. episode 950.000000, reward total was -20.000000. running mean: -20.214276\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.222133\n",
            "resetting env. episode 952.000000, reward total was -20.000000. running mean: -20.219912\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.227712\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.235435\n",
            "resetting env. episode 955.000000, reward total was -20.000000. running mean: -20.233081\n",
            "resetting env. episode 956.000000, reward total was -19.000000. running mean: -20.220750\n",
            "resetting env. episode 957.000000, reward total was -18.000000. running mean: -20.198543\n",
            "resetting env. episode 958.000000, reward total was -19.000000. running mean: -20.186557\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.194692\n",
            "resetting env. episode 960.000000, reward total was -20.000000. running mean: -20.192745\n",
            "resetting env. episode 961.000000, reward total was -20.000000. running mean: -20.190817\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.198909\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.206920\n",
            "resetting env. episode 964.000000, reward total was -20.000000. running mean: -20.204851\n",
            "resetting env. episode 965.000000, reward total was -19.000000. running mean: -20.192802\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.200874\n",
            "resetting env. episode 967.000000, reward total was -20.000000. running mean: -20.198866\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.206877\n",
            "resetting env. episode 969.000000, reward total was -20.000000. running mean: -20.204808\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -20.202760\n",
            "resetting env. episode 971.000000, reward total was -17.000000. running mean: -20.170732\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.179025\n",
            "resetting env. episode 973.000000, reward total was -19.000000. running mean: -20.167235\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -20.165563\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.173907\n",
            "resetting env. episode 976.000000, reward total was -19.000000. running mean: -20.162168\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -20.170546\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.178841\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.187052\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.195182\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.203230\n",
            "resetting env. episode 982.000000, reward total was -20.000000. running mean: -20.201198\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.209186\n",
            "resetting env. episode 984.000000, reward total was -19.000000. running mean: -20.197094\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.205123\n",
            "resetting env. episode 986.000000, reward total was -20.000000. running mean: -20.203072\n",
            "resetting env. episode 987.000000, reward total was -20.000000. running mean: -20.201041\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -20.209031\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.216940\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.224771\n",
            "resetting env. episode 991.000000, reward total was -20.000000. running mean: -20.222523\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.230298\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -20.227995\n",
            "resetting env. episode 994.000000, reward total was -20.000000. running mean: -20.225715\n",
            "resetting env. episode 995.000000, reward total was -20.000000. running mean: -20.223458\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -20.221223\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.229011\n",
            "resetting env. episode 998.000000, reward total was -19.000000. running mean: -20.216721\n",
            "resetting env. episode 999.000000, reward total was -18.000000. running mean: -20.194554\n",
            "resetting env. episode 1000.000000, reward total was -19.000000. running mean: -20.182608\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -20.190782\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -20.198874\n",
            "resetting env. episode 1003.000000, reward total was -18.000000. running mean: -20.176886\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -20.185117\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.193265\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.201333\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.209320\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.217226\n",
            "resetting env. episode 1009.000000, reward total was -20.000000. running mean: -20.215054\n",
            "resetting env. episode 1010.000000, reward total was -21.000000. running mean: -20.222904\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.230674\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -20.228368\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.236084\n",
            "resetting env. episode 1014.000000, reward total was -19.000000. running mean: -20.223723\n",
            "resetting env. episode 1015.000000, reward total was -20.000000. running mean: -20.221486\n",
            "resetting env. episode 1016.000000, reward total was -21.000000. running mean: -20.229271\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -20.226978\n",
            "resetting env. episode 1018.000000, reward total was -20.000000. running mean: -20.224709\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -20.232462\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.240137\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -20.247736\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.255258\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.262706\n",
            "resetting env. episode 1024.000000, reward total was -19.000000. running mean: -20.250079\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.257578\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -20.255002\n",
            "resetting env. episode 1027.000000, reward total was -20.000000. running mean: -20.252452\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.259927\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.267328\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -20.264655\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.272008\n",
            "resetting env. episode 1032.000000, reward total was -20.000000. running mean: -20.269288\n",
            "resetting env. episode 1033.000000, reward total was -20.000000. running mean: -20.266595\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.273929\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -20.281190\n",
            "resetting env. episode 1036.000000, reward total was -20.000000. running mean: -20.278378\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.285594\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -20.282739\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.289911\n",
            "resetting env. episode 1040.000000, reward total was -20.000000. running mean: -20.287012\n",
            "resetting env. episode 1041.000000, reward total was -19.000000. running mean: -20.274142\n",
            "resetting env. episode 1042.000000, reward total was -20.000000. running mean: -20.271400\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.278686\n",
            "resetting env. episode 1044.000000, reward total was -21.000000. running mean: -20.285900\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -20.293041\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.300110\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -20.307109\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -20.314038\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.320898\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.327689\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -20.334412\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -20.331068\n",
            "resetting env. episode 1053.000000, reward total was -20.000000. running mean: -20.327757\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -20.334479\n",
            "resetting env. episode 1055.000000, reward total was -19.000000. running mean: -20.321135\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -20.317923\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.324744\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -20.331497\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -20.328182\n",
            "resetting env. episode 1060.000000, reward total was -19.000000. running mean: -20.314900\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.321751\n",
            "resetting env. episode 1062.000000, reward total was -20.000000. running mean: -20.318533\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -20.325348\n",
            "resetting env. episode 1064.000000, reward total was -19.000000. running mean: -20.312094\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.318974\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -20.315784\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -20.322626\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -20.329400\n",
            "resetting env. episode 1069.000000, reward total was -20.000000. running mean: -20.326106\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -20.322845\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.329616\n",
            "resetting env. episode 1072.000000, reward total was -20.000000. running mean: -20.326320\n",
            "resetting env. episode 1073.000000, reward total was -20.000000. running mean: -20.323057\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.329826\n",
            "resetting env. episode 1075.000000, reward total was -20.000000. running mean: -20.326528\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.333263\n",
            "resetting env. episode 1077.000000, reward total was -20.000000. running mean: -20.329930\n",
            "resetting env. episode 1078.000000, reward total was -20.000000. running mean: -20.326631\n",
            "resetting env. episode 1079.000000, reward total was -20.000000. running mean: -20.323364\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -20.320131\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.326930\n",
            "resetting env. episode 1082.000000, reward total was -20.000000. running mean: -20.323660\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.330424\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.337119\n",
            "resetting env. episode 1085.000000, reward total was -19.000000. running mean: -20.323748\n",
            "resetting env. episode 1086.000000, reward total was -18.000000. running mean: -20.300511\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.307506\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.314431\n",
            "resetting env. episode 1089.000000, reward total was -18.000000. running mean: -20.291286\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.298373\n",
            "resetting env. episode 1091.000000, reward total was -19.000000. running mean: -20.285390\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -20.272536\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.279810\n",
            "resetting env. episode 1094.000000, reward total was -21.000000. running mean: -20.287012\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.294142\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -20.301201\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.308189\n",
            "resetting env. episode 1098.000000, reward total was -20.000000. running mean: -20.305107\n",
            "resetting env. episode 1099.000000, reward total was -20.000000. running mean: -20.302056\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -20.299035\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -20.306045\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.312984\n",
            "resetting env. episode 1103.000000, reward total was -20.000000. running mean: -20.309855\n",
            "resetting env. episode 1104.000000, reward total was -20.000000. running mean: -20.306756\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.313688\n",
            "resetting env. episode 1106.000000, reward total was -20.000000. running mean: -20.310552\n",
            "resetting env. episode 1107.000000, reward total was -20.000000. running mean: -20.307446\n",
            "resetting env. episode 1108.000000, reward total was -20.000000. running mean: -20.304372\n",
            "resetting env. episode 1109.000000, reward total was -19.000000. running mean: -20.291328\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.298415\n",
            "resetting env. episode 1111.000000, reward total was -19.000000. running mean: -20.285430\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.292576\n",
            "resetting env. episode 1113.000000, reward total was -17.000000. running mean: -20.259650\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -20.257054\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -20.254483\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.261939\n",
            "resetting env. episode 1117.000000, reward total was -20.000000. running mean: -20.259319\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -20.266726\n",
            "resetting env. episode 1119.000000, reward total was -20.000000. running mean: -20.264059\n",
            "resetting env. episode 1120.000000, reward total was -17.000000. running mean: -20.231418\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -20.239104\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -20.246713\n",
            "resetting env. episode 1123.000000, reward total was -20.000000. running mean: -20.244246\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.251803\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -20.249285\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.256792\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.264225\n",
            "resetting env. episode 1128.000000, reward total was -19.000000. running mean: -20.251582\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.259066\n",
            "resetting env. episode 1130.000000, reward total was -21.000000. running mean: -20.266476\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -20.273811\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.281073\n",
            "resetting env. episode 1133.000000, reward total was -19.000000. running mean: -20.268262\n",
            "resetting env. episode 1134.000000, reward total was -20.000000. running mean: -20.265580\n",
            "resetting env. episode 1135.000000, reward total was -21.000000. running mean: -20.272924\n",
            "resetting env. episode 1136.000000, reward total was -18.000000. running mean: -20.250195\n",
            "resetting env. episode 1137.000000, reward total was -19.000000. running mean: -20.237693\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -20.235316\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.242962\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.250533\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -20.258028\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -20.265447\n",
            "resetting env. episode 1143.000000, reward total was -19.000000. running mean: -20.252793\n",
            "resetting env. episode 1144.000000, reward total was -20.000000. running mean: -20.250265\n",
            "resetting env. episode 1145.000000, reward total was -19.000000. running mean: -20.237762\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -20.235385\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -20.243031\n",
            "resetting env. episode 1148.000000, reward total was -21.000000. running mean: -20.250600\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -20.248094\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -20.245613\n",
            "resetting env. episode 1151.000000, reward total was -19.000000. running mean: -20.233157\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -20.240826\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -20.248418\n",
            "resetting env. episode 1154.000000, reward total was -18.000000. running mean: -20.225933\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -20.233674\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.241337\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.248924\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.256435\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -20.253870\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -20.251332\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -20.258818\n",
            "resetting env. episode 1162.000000, reward total was -20.000000. running mean: -20.256230\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -20.263668\n",
            "resetting env. episode 1164.000000, reward total was -19.000000. running mean: -20.251031\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -20.258521\n",
            "resetting env. episode 1166.000000, reward total was -18.000000. running mean: -20.235936\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.243576\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -20.251140\n",
            "resetting env. episode 1169.000000, reward total was -19.000000. running mean: -20.238629\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.246243\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -20.253780\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -20.251243\n",
            "resetting env. episode 1173.000000, reward total was -18.000000. running mean: -20.228730\n",
            "resetting env. episode 1174.000000, reward total was -20.000000. running mean: -20.226443\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.234178\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -20.231837\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -20.239518\n",
            "resetting env. episode 1178.000000, reward total was -21.000000. running mean: -20.247123\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.254652\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.262105\n",
            "resetting env. episode 1181.000000, reward total was -19.000000. running mean: -20.249484\n",
            "resetting env. episode 1182.000000, reward total was -19.000000. running mean: -20.236989\n",
            "resetting env. episode 1183.000000, reward total was -19.000000. running mean: -20.224620\n",
            "resetting env. episode 1184.000000, reward total was -17.000000. running mean: -20.192373\n",
            "resetting env. episode 1185.000000, reward total was -21.000000. running mean: -20.200450\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.208445\n",
            "resetting env. episode 1187.000000, reward total was -20.000000. running mean: -20.206361\n",
            "resetting env. episode 1188.000000, reward total was -18.000000. running mean: -20.184297\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.192454\n",
            "resetting env. episode 1190.000000, reward total was -19.000000. running mean: -20.180530\n",
            "resetting env. episode 1191.000000, reward total was -20.000000. running mean: -20.178724\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -20.186937\n",
            "resetting env. episode 1193.000000, reward total was -20.000000. running mean: -20.185068\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.193217\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.201285\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -20.199272\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -20.207279\n",
            "resetting env. episode 1198.000000, reward total was -18.000000. running mean: -20.185206\n",
            "resetting env. episode 1199.000000, reward total was -18.000000. running mean: -20.163354\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.171721\n",
            "resetting env. episode 1201.000000, reward total was -21.000000. running mean: -20.180004\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.188204\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -20.196322\n",
            "resetting env. episode 1204.000000, reward total was -19.000000. running mean: -20.184358\n",
            "resetting env. episode 1205.000000, reward total was -20.000000. running mean: -20.182515\n",
            "resetting env. episode 1206.000000, reward total was -19.000000. running mean: -20.170690\n",
            "resetting env. episode 1207.000000, reward total was -21.000000. running mean: -20.178983\n",
            "resetting env. episode 1208.000000, reward total was -20.000000. running mean: -20.177193\n",
            "resetting env. episode 1209.000000, reward total was -20.000000. running mean: -20.175421\n",
            "resetting env. episode 1210.000000, reward total was -17.000000. running mean: -20.143667\n",
            "resetting env. episode 1211.000000, reward total was -20.000000. running mean: -20.142230\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -20.150808\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.159300\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -20.157707\n",
            "resetting env. episode 1215.000000, reward total was -19.000000. running mean: -20.146130\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -20.144668\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -20.143222\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.151789\n",
            "resetting env. episode 1219.000000, reward total was -18.000000. running mean: -20.130272\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -20.128969\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -20.137679\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -20.146302\n",
            "resetting env. episode 1223.000000, reward total was -20.000000. running mean: -20.144839\n",
            "resetting env. episode 1224.000000, reward total was -20.000000. running mean: -20.143391\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -20.151957\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.160437\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.168833\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -20.177145\n",
            "resetting env. episode 1229.000000, reward total was -20.000000. running mean: -20.175373\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -20.173620\n",
            "resetting env. episode 1231.000000, reward total was -20.000000. running mean: -20.171883\n",
            "resetting env. episode 1232.000000, reward total was -21.000000. running mean: -20.180165\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -20.178363\n",
            "resetting env. episode 1234.000000, reward total was -19.000000. running mean: -20.166579\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.174913\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -20.183164\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.191333\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.199419\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.207425\n",
            "resetting env. episode 1240.000000, reward total was -20.000000. running mean: -20.205351\n",
            "resetting env. episode 1241.000000, reward total was -20.000000. running mean: -20.203297\n",
            "resetting env. episode 1242.000000, reward total was -20.000000. running mean: -20.201264\n",
            "resetting env. episode 1243.000000, reward total was -20.000000. running mean: -20.199252\n",
            "resetting env. episode 1244.000000, reward total was -18.000000. running mean: -20.177259\n",
            "resetting env. episode 1245.000000, reward total was -17.000000. running mean: -20.145487\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.154032\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -20.162491\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.170867\n",
            "resetting env. episode 1249.000000, reward total was -20.000000. running mean: -20.169158\n",
            "resetting env. episode 1250.000000, reward total was -20.000000. running mean: -20.167466\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -20.165792\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.174134\n",
            "resetting env. episode 1253.000000, reward total was -19.000000. running mean: -20.162392\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -20.170768\n",
            "resetting env. episode 1255.000000, reward total was -20.000000. running mean: -20.169061\n",
            "resetting env. episode 1256.000000, reward total was -19.000000. running mean: -20.157370\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -20.165796\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.174139\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.182397\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.190573\n",
            "resetting env. episode 1261.000000, reward total was -20.000000. running mean: -20.188667\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -20.196781\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.204813\n",
            "resetting env. episode 1264.000000, reward total was -19.000000. running mean: -20.192765\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -20.200837\n",
            "resetting env. episode 1266.000000, reward total was -19.000000. running mean: -20.188829\n",
            "resetting env. episode 1267.000000, reward total was -18.000000. running mean: -20.166941\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -20.175271\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.183518\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.191683\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -20.189766\n",
            "resetting env. episode 1272.000000, reward total was -19.000000. running mean: -20.177869\n",
            "resetting env. episode 1273.000000, reward total was -19.000000. running mean: -20.166090\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -20.174429\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -20.182685\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -20.180858\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.189049\n",
            "resetting env. episode 1278.000000, reward total was -16.000000. running mean: -20.147159\n",
            "resetting env. episode 1279.000000, reward total was -20.000000. running mean: -20.145687\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -20.154230\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.162688\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.171061\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.179351\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -20.177557\n",
            "resetting env. episode 1285.000000, reward total was -19.000000. running mean: -20.165782\n",
            "resetting env. episode 1286.000000, reward total was -20.000000. running mean: -20.164124\n",
            "resetting env. episode 1287.000000, reward total was -19.000000. running mean: -20.152483\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -20.150958\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -20.159448\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -20.167854\n",
            "resetting env. episode 1291.000000, reward total was -20.000000. running mean: -20.166175\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -20.174513\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.182768\n",
            "resetting env. episode 1294.000000, reward total was -20.000000. running mean: -20.180941\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.189131\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.197240\n",
            "resetting env. episode 1297.000000, reward total was -19.000000. running mean: -20.185267\n",
            "resetting env. episode 1298.000000, reward total was -20.000000. running mean: -20.183415\n",
            "resetting env. episode 1299.000000, reward total was -19.000000. running mean: -20.171581\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -20.179865\n",
            "resetting env. episode 1301.000000, reward total was -20.000000. running mean: -20.178066\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -20.186285\n",
            "resetting env. episode 1303.000000, reward total was -20.000000. running mean: -20.184423\n",
            "resetting env. episode 1304.000000, reward total was -19.000000. running mean: -20.172578\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.180853\n",
            "resetting env. episode 1306.000000, reward total was -20.000000. running mean: -20.179044\n",
            "resetting env. episode 1307.000000, reward total was -19.000000. running mean: -20.167254\n",
            "resetting env. episode 1308.000000, reward total was -19.000000. running mean: -20.155581\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -20.164025\n",
            "resetting env. episode 1310.000000, reward total was -20.000000. running mean: -20.162385\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.170761\n",
            "resetting env. episode 1312.000000, reward total was -19.000000. running mean: -20.159054\n",
            "resetting env. episode 1313.000000, reward total was -19.000000. running mean: -20.147463\n",
            "resetting env. episode 1314.000000, reward total was -20.000000. running mean: -20.145988\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -20.144529\n",
            "resetting env. episode 1316.000000, reward total was -19.000000. running mean: -20.133083\n",
            "resetting env. episode 1317.000000, reward total was -21.000000. running mean: -20.141752\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.150335\n",
            "resetting env. episode 1319.000000, reward total was -19.000000. running mean: -20.138832\n",
            "resetting env. episode 1320.000000, reward total was -20.000000. running mean: -20.137443\n",
            "resetting env. episode 1321.000000, reward total was -20.000000. running mean: -20.136069\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.144708\n",
            "resetting env. episode 1323.000000, reward total was -20.000000. running mean: -20.143261\n",
            "resetting env. episode 1324.000000, reward total was -19.000000. running mean: -20.131828\n",
            "resetting env. episode 1325.000000, reward total was -20.000000. running mean: -20.130510\n",
            "resetting env. episode 1326.000000, reward total was -20.000000. running mean: -20.129205\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.137913\n",
            "resetting env. episode 1328.000000, reward total was -18.000000. running mean: -20.116534\n",
            "resetting env. episode 1329.000000, reward total was -19.000000. running mean: -20.105369\n",
            "resetting env. episode 1330.000000, reward total was -19.000000. running mean: -20.094315\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -20.093372\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.102438\n",
            "resetting env. episode 1333.000000, reward total was -20.000000. running mean: -20.101414\n",
            "resetting env. episode 1334.000000, reward total was -20.000000. running mean: -20.100399\n",
            "resetting env. episode 1335.000000, reward total was -20.000000. running mean: -20.099395\n",
            "resetting env. episode 1336.000000, reward total was -21.000000. running mean: -20.108402\n",
            "resetting env. episode 1337.000000, reward total was -21.000000. running mean: -20.117317\n",
            "resetting env. episode 1338.000000, reward total was -19.000000. running mean: -20.106144\n",
            "resetting env. episode 1339.000000, reward total was -20.000000. running mean: -20.105083\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.114032\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -20.122892\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -20.121663\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.130446\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -20.129142\n",
            "resetting env. episode 1345.000000, reward total was -19.000000. running mean: -20.117850\n",
            "resetting env. episode 1346.000000, reward total was -20.000000. running mean: -20.116672\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -20.125505\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -20.134250\n",
            "resetting env. episode 1349.000000, reward total was -21.000000. running mean: -20.142908\n",
            "resetting env. episode 1350.000000, reward total was -19.000000. running mean: -20.131478\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -20.130164\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -20.128862\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.137573\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -20.136198\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -20.144836\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.153387\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -20.151853\n",
            "resetting env. episode 1358.000000, reward total was -20.000000. running mean: -20.150335\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.158832\n",
            "resetting env. episode 1360.000000, reward total was -20.000000. running mean: -20.157243\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -20.155671\n",
            "resetting env. episode 1362.000000, reward total was -20.000000. running mean: -20.154114\n",
            "resetting env. episode 1363.000000, reward total was -20.000000. running mean: -20.152573\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -20.161047\n",
            "resetting env. episode 1365.000000, reward total was -19.000000. running mean: -20.149437\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -20.157942\n",
            "resetting env. episode 1367.000000, reward total was -20.000000. running mean: -20.156363\n",
            "resetting env. episode 1368.000000, reward total was -19.000000. running mean: -20.144799\n",
            "resetting env. episode 1369.000000, reward total was -19.000000. running mean: -20.133351\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -20.132018\n",
            "resetting env. episode 1371.000000, reward total was -19.000000. running mean: -20.120698\n",
            "resetting env. episode 1372.000000, reward total was -19.000000. running mean: -20.109491\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -20.118396\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.127212\n",
            "resetting env. episode 1375.000000, reward total was -20.000000. running mean: -20.125940\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -20.134680\n",
            "resetting env. episode 1377.000000, reward total was -19.000000. running mean: -20.123334\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.132100\n",
            "resetting env. episode 1379.000000, reward total was -20.000000. running mean: -20.130779\n",
            "resetting env. episode 1380.000000, reward total was -21.000000. running mean: -20.139471\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -20.138077\n",
            "resetting env. episode 1382.000000, reward total was -20.000000. running mean: -20.136696\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -20.145329\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -20.143876\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.152437\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.160913\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -20.159303\n",
            "resetting env. episode 1388.000000, reward total was -20.000000. running mean: -20.157710\n",
            "resetting env. episode 1389.000000, reward total was -20.000000. running mean: -20.156133\n",
            "resetting env. episode 1390.000000, reward total was -21.000000. running mean: -20.164572\n",
            "resetting env. episode 1391.000000, reward total was -20.000000. running mean: -20.162926\n",
            "resetting env. episode 1392.000000, reward total was -20.000000. running mean: -20.161297\n",
            "resetting env. episode 1393.000000, reward total was -20.000000. running mean: -20.159684\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -20.158087\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -20.166506\n",
            "resetting env. episode 1396.000000, reward total was -20.000000. running mean: -20.164841\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.173193\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -20.181461\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -20.189646\n",
            "resetting env. episode 1400.000000, reward total was -20.000000. running mean: -20.187750\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -20.185872\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.194014\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.202073\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.210053\n",
            "resetting env. episode 1405.000000, reward total was -19.000000. running mean: -20.197952\n",
            "resetting env. episode 1406.000000, reward total was -20.000000. running mean: -20.195973\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -20.204013\n",
            "resetting env. episode 1408.000000, reward total was -21.000000. running mean: -20.211973\n",
            "resetting env. episode 1409.000000, reward total was -20.000000. running mean: -20.209853\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.217755\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.225577\n",
            "resetting env. episode 1412.000000, reward total was -18.000000. running mean: -20.203321\n",
            "resetting env. episode 1413.000000, reward total was -20.000000. running mean: -20.201288\n",
            "resetting env. episode 1414.000000, reward total was -19.000000. running mean: -20.189275\n",
            "resetting env. episode 1415.000000, reward total was -20.000000. running mean: -20.187382\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.195509\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -20.193553\n",
            "resetting env. episode 1418.000000, reward total was -20.000000. running mean: -20.191618\n",
            "resetting env. episode 1419.000000, reward total was -20.000000. running mean: -20.189702\n",
            "resetting env. episode 1420.000000, reward total was -21.000000. running mean: -20.197805\n",
            "resetting env. episode 1421.000000, reward total was -19.000000. running mean: -20.185827\n",
            "resetting env. episode 1422.000000, reward total was -19.000000. running mean: -20.173968\n",
            "resetting env. episode 1423.000000, reward total was -20.000000. running mean: -20.172229\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.180506\n",
            "resetting env. episode 1425.000000, reward total was -19.000000. running mean: -20.168701\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -20.177014\n",
            "resetting env. episode 1427.000000, reward total was -19.000000. running mean: -20.165244\n",
            "resetting env. episode 1428.000000, reward total was -20.000000. running mean: -20.163592\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -20.171956\n",
            "resetting env. episode 1430.000000, reward total was -21.000000. running mean: -20.180236\n",
            "resetting env. episode 1431.000000, reward total was -19.000000. running mean: -20.168434\n",
            "resetting env. episode 1432.000000, reward total was -16.000000. running mean: -20.126750\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -20.135482\n",
            "resetting env. episode 1434.000000, reward total was -20.000000. running mean: -20.134127\n",
            "resetting env. episode 1435.000000, reward total was -18.000000. running mean: -20.112786\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -20.111658\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.120542\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.129336\n",
            "resetting env. episode 1439.000000, reward total was -20.000000. running mean: -20.128043\n",
            "resetting env. episode 1440.000000, reward total was -21.000000. running mean: -20.136762\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -20.145395\n",
            "resetting env. episode 1442.000000, reward total was -19.000000. running mean: -20.133941\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.142601\n",
            "resetting env. episode 1444.000000, reward total was -20.000000. running mean: -20.141175\n",
            "resetting env. episode 1445.000000, reward total was -19.000000. running mean: -20.129764\n",
            "resetting env. episode 1446.000000, reward total was -18.000000. running mean: -20.108466\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -20.117381\n",
            "resetting env. episode 1448.000000, reward total was -20.000000. running mean: -20.116208\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -20.125045\n",
            "resetting env. episode 1450.000000, reward total was -20.000000. running mean: -20.123795\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -20.132557\n",
            "resetting env. episode 1452.000000, reward total was -20.000000. running mean: -20.131231\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -20.139919\n",
            "resetting env. episode 1454.000000, reward total was -20.000000. running mean: -20.138520\n",
            "resetting env. episode 1455.000000, reward total was -19.000000. running mean: -20.127135\n",
            "resetting env. episode 1456.000000, reward total was -20.000000. running mean: -20.125863\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.134605\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -20.133259\n",
            "resetting env. episode 1459.000000, reward total was -18.000000. running mean: -20.111926\n",
            "resetting env. episode 1460.000000, reward total was -20.000000. running mean: -20.110807\n",
            "resetting env. episode 1461.000000, reward total was -20.000000. running mean: -20.109699\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -20.118602\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -20.127416\n",
            "resetting env. episode 1464.000000, reward total was -20.000000. running mean: -20.126142\n",
            "resetting env. episode 1465.000000, reward total was -18.000000. running mean: -20.104880\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.113831\n",
            "resetting env. episode 1467.000000, reward total was -20.000000. running mean: -20.112693\n",
            "resetting env. episode 1468.000000, reward total was -20.000000. running mean: -20.111566\n",
            "resetting env. episode 1469.000000, reward total was -21.000000. running mean: -20.120451\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -20.129246\n",
            "resetting env. episode 1471.000000, reward total was -19.000000. running mean: -20.117954\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -20.116774\n",
            "resetting env. episode 1473.000000, reward total was -17.000000. running mean: -20.085606\n",
            "resetting env. episode 1474.000000, reward total was -20.000000. running mean: -20.084750\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -20.093903\n",
            "resetting env. episode 1476.000000, reward total was -20.000000. running mean: -20.092964\n",
            "resetting env. episode 1477.000000, reward total was -19.000000. running mean: -20.082034\n",
            "resetting env. episode 1478.000000, reward total was -20.000000. running mean: -20.081214\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.090402\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -20.099498\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.108503\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -20.117418\n",
            "resetting env. episode 1483.000000, reward total was -20.000000. running mean: -20.116243\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.125081\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -20.133830\n",
            "resetting env. episode 1486.000000, reward total was -20.000000. running mean: -20.132492\n",
            "resetting env. episode 1487.000000, reward total was -20.000000. running mean: -20.131167\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -20.139855\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -20.138457\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -20.137072\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -20.135701\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -20.144344\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.152901\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.161372\n",
            "resetting env. episode 1495.000000, reward total was -17.000000. running mean: -20.129758\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -20.138461\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -20.147076\n",
            "resetting env. episode 1498.000000, reward total was -20.000000. running mean: -20.145605\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -20.154149\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -20.162608\n",
            "CPU times: user 1h 10min 11s, sys: 32min 20s, total: 1h 42min 32s\n",
            "Wall time: 52min 58s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "52070a34-8a26-40b6-e58d-70d390ad2388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}