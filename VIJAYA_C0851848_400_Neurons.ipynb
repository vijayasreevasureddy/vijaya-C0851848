{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VIJAYA_C0851848_400_Neurons.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2182abf0-4a34-4315-8aed-679d8ca46a43"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null\n",
        "!pip install JSAnimation >/dev/null\n",
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "301b94af-2bb4-4ed2-a7f5-dc030d5552b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "9d609fba-8356-4e68-ab51-a544699ff3c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "87d5ecbf-1ae9-440d-8943-aed1890962f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "89809355-9ddb-418c-908f-e8fc09a36f6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -17.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 400 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "d8743f09-8c13-4e8c-e4cf-9ae9d45fb9ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.970200\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.970498\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.960793\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.961185\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.961573\n",
            "resetting env. episode 8.000000, reward total was -19.000000. running mean: -20.941958\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.942538\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.933113\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.923781\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.924544\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.915298\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.916145\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.916984\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.917814\n",
            "resetting env. episode 17.000000, reward total was -19.000000. running mean: -20.898636\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.899649\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.900653\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.901646\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.882630\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.883804\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.884966\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.886116\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.877255\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.868482\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.869797\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.871099\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.872388\n",
            "resetting env. episode 30.000000, reward total was -19.000000. running mean: -20.853665\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.855128\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.856577\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -20.838011\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.829631\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.831334\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.833021\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.834691\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.836344\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.837981\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.839601\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.831205\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.832893\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.834564\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.826218\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.827956\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -20.809676\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.811580\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.813464\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.805329\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.807276\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.809203\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.801111\n",
            "resetting env. episode 53.000000, reward total was -18.000000. running mean: -20.773100\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.765369\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.767715\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.770038\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.772338\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.774614\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.776868\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.779100\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.771309\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.763595\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.765960\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.768300\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.770617\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.762911\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.765282\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.757629\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.760053\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.752452\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.754927\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.747378\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.739904\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -20.722505\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.725280\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.728028\n",
            "resetting env. episode 77.000000, reward total was -18.000000. running mean: -20.700747\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.703740\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -20.686702\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.669835\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.673137\n",
            "resetting env. episode 82.000000, reward total was -19.000000. running mean: -20.656406\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.659842\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.663243\n",
            "resetting env. episode 85.000000, reward total was -18.000000. running mean: -20.636611\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.630245\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.633942\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.627603\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.631327\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.635013\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.628663\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.632377\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.636053\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.639692\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.643295\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.646863\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.650394\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.653890\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.657351\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -20.640778\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.644370\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.647926\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.651447\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.644932\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.638483\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.642098\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.645677\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.649220\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -20.632728\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.626401\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -20.610137\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.614036\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.607895\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.611816\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.605698\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.599641\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.603645\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.607608\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -20.601532\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.605517\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.609462\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.613367\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.617233\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.621061\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.624850\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.618602\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.622416\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.606192\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.600130\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.604129\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.608087\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.602006\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.605986\n",
            "resetting env. episode 134.000000, reward total was -19.000000. running mean: -20.589926\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.594027\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.598087\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.602106\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.596085\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.600124\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.604123\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.608082\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -20.592001\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.586081\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -20.570220\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -20.554518\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.558973\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.563383\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.557749\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.562172\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.556550\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.550984\n",
            "resetting env. episode 152.000000, reward total was -19.000000. running mean: -20.535475\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.530120\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.534819\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.539470\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.544076\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.538635\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.543249\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.547816\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.552338\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.546815\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.541346\n",
            "resetting env. episode 163.000000, reward total was -18.000000. running mean: -20.515933\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.520774\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.525566\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.530310\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.535007\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.539657\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.544261\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.548818\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.553330\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.557796\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.562218\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.566596\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.570930\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.565221\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.569569\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.563873\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.568234\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.572552\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.576827\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.571058\n",
            "resetting env. episode 183.000000, reward total was -18.000000. running mean: -20.545348\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.549894\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.534395\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.539051\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.543661\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.548224\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.542742\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.547315\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.551841\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.556323\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.560760\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.555152\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.559601\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.554005\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.558465\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.562880\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.567251\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.561579\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.555963\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.550403\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.544899\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.539450\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.524056\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.508815\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.513727\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.508590\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.503504\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.498469\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.493484\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.488549\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.493664\n",
            "resetting env. episode 214.000000, reward total was -17.000000. running mean: -20.458727\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.464140\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.469498\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.474803\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.480055\n",
            "resetting env. episode 219.000000, reward total was -19.000000. running mean: -20.465255\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.470602\n",
            "resetting env. episode 221.000000, reward total was -18.000000. running mean: -20.445896\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.451437\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.456923\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.462354\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.467730\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.473053\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.478322\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.473539\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.478804\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.484016\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.469176\n",
            "resetting env. episode 232.000000, reward total was -18.000000. running mean: -20.444484\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.450039\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.455539\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.460983\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.446373\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.451910\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.457391\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.462817\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.458188\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.453607\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.459070\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.464480\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.459835\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.465237\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.460584\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.465978\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.451319\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.446805\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.442337\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.447914\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.453435\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.458901\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.464312\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.469668\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.474972\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.470222\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.465520\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.470865\n",
            "resetting env. episode 260.000000, reward total was -17.000000. running mean: -20.436156\n",
            "resetting env. episode 261.000000, reward total was -19.000000. running mean: -20.421794\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.417576\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.423401\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.429167\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.434875\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.440526\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.446121\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.451660\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.457143\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.442572\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.448146\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.443665\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.449228\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.454736\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.450188\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.445686\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.441230\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -20.426817\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.432549\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.438224\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.443841\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.449403\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.454909\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.450360\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -20.435856\n",
            "resetting env. episode 286.000000, reward total was -19.000000. running mean: -20.421498\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.427283\n",
            "resetting env. episode 288.000000, reward total was -19.000000. running mean: -20.413010\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.408880\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.394791\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.400843\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.406835\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.412766\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.418639\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.414452\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.420308\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.426105\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.431844\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.437525\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.423150\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.418918\n",
            "resetting env. episode 302.000000, reward total was -18.000000. running mean: -20.394729\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.390782\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.396874\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.402905\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.408876\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.414788\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.410640\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.406533\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.402468\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.408443\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.414359\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.420215\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.426013\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.421753\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.417535\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.413360\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.419226\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.415034\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.410884\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.416775\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.412607\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.418481\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.414296\n",
            "resetting env. episode 325.000000, reward total was -19.000000. running mean: -20.400153\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.386152\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.392290\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.388367\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.374484\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.380739\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.386932\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.393062\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.389132\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.395240\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.391288\n",
            "resetting env. episode 336.000000, reward total was -18.000000. running mean: -20.367375\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.373701\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.369964\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.356265\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.362702\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.359075\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.355484\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.351929\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.348410\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.354926\n",
            "resetting env. episode 346.000000, reward total was -18.000000. running mean: -20.331377\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.338063\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.344682\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.351235\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -20.337723\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.344346\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.350902\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.347393\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.353919\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.360380\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.366776\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.363109\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.369478\n",
            "resetting env. episode 359.000000, reward total was -19.000000. running mean: -20.355783\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.362225\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.368603\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.364917\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.371268\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.377555\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.383779\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.389942\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.396042\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.402082\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.408061\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.413980\n",
            "resetting env. episode 371.000000, reward total was -18.000000. running mean: -20.389841\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.385942\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.392083\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.388162\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.394280\n",
            "resetting env. episode 376.000000, reward total was -18.000000. running mean: -20.370337\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.366634\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.362968\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.349338\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.355845\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.362286\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.358663\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.355077\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.351526\n",
            "resetting env. episode 385.000000, reward total was -17.000000. running mean: -20.318011\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.324831\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.331582\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.338266\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.344884\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.351435\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.357921\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.364341\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.370698\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.376991\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.383221\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.379389\n",
            "resetting env. episode 397.000000, reward total was -18.000000. running mean: -20.355595\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.362039\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.368419\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.374734\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.370987\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.377277\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.383504\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.389669\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.395773\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.401815\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.407797\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.413719\n",
            "resetting env. episode 409.000000, reward total was -18.000000. running mean: -20.389582\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -20.375686\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.371929\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.378210\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.384428\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.380583\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.386778\n",
            "resetting env. episode 416.000000, reward total was -18.000000. running mean: -20.362910\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.369281\n",
            "resetting env. episode 418.000000, reward total was -19.000000. running mean: -20.355588\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.362032\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.358412\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.364828\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.371179\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.367467\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.363793\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.370155\n",
            "resetting env. episode 426.000000, reward total was -19.000000. running mean: -20.356453\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.352889\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.359360\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.365766\n",
            "resetting env. episode 430.000000, reward total was -19.000000. running mean: -20.352109\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.358588\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.355002\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.351452\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.347937\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.354458\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.360913\n",
            "resetting env. episode 437.000000, reward total was -18.000000. running mean: -20.337304\n",
            "resetting env. episode 438.000000, reward total was -17.000000. running mean: -20.303931\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.310892\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.317783\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.324605\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.331359\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.318045\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.314865\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.321716\n",
            "resetting env. episode 446.000000, reward total was -18.000000. running mean: -20.298499\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.305514\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.312459\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.319334\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.326141\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.322880\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.329651\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.316354\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.313191\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.320059\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.326858\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -20.313590\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.320454\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.327249\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.333977\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.320637\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.327431\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.334156\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.340815\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.347407\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.353933\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.350393\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.346889\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.353420\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.359886\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.366287\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.372624\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.378898\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.385109\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.391258\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.397346\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.403372\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.409338\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.405245\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.411193\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.417081\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.412910\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.418781\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.424593\n",
            "resetting env. episode 485.000000, reward total was -18.000000. running mean: -20.400347\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.406343\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.412280\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.418157\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.403976\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -20.389936\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.396037\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.402076\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.408055\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.403975\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.399935\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.385936\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.392076\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.398156\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.394174\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.400232\n",
            "CPU times: user 33min 34s, sys: 10min 48s, total: 44min 23s\n",
            "Wall time: 23min 7s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "c90d11d7-c471-4cd9-9201-302bde343d6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -19.000000. running mean: -20.970100\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.960399\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.950795\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.941287\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.931874\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.932555\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.933230\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.933898\n",
            "resetting env. episode 11.000000, reward total was -19.000000. running mean: -20.914559\n",
            "resetting env. episode 12.000000, reward total was -19.000000. running mean: -20.895413\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.896459\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.897494\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.898519\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.889534\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.880639\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.881832\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.883014\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.874184\n",
            "resetting env. episode 21.000000, reward total was -18.000000. running mean: -20.845442\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.836988\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.838618\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.840232\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.831829\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.823511\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.825276\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.817023\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.808853\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.800764\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.802757\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -20.784729\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.786882\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.789013\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -20.771123\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.763412\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.765778\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.768120\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.770439\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.772734\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.775007\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.767257\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.769584\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.771888\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.764170\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.766528\n",
            "resetting env. episode 47.000000, reward total was -18.000000. running mean: -20.738863\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.741474\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.744059\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.746619\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.739152\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.731761\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.724443\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.717199\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.710027\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.712927\n",
            "resetting env. episode 57.000000, reward total was -18.000000. running mean: -20.685797\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.688939\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.692050\n",
            "resetting env. episode 60.000000, reward total was -19.000000. running mean: -20.675129\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -20.658378\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.661794\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.665176\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -20.648525\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.652039\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.645519\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.639064\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.642673\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.646246\n",
            "resetting env. episode 70.000000, reward total was -18.000000. running mean: -20.619784\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.613586\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.617450\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.611276\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.615163\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.609011\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.612921\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.606792\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.610724\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.614617\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.618471\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.612286\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.606163\n",
            "resetting env. episode 83.000000, reward total was -18.000000. running mean: -20.580102\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.584301\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.578458\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.582673\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.586846\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.590978\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.595068\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.599117\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.603126\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.607095\n",
            "resetting env. episode 93.000000, reward total was -18.000000. running mean: -20.581024\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.575214\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.569462\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.563767\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.568129\n",
            "resetting env. episode 98.000000, reward total was -18.000000. running mean: -20.542448\n",
            "resetting env. episode 99.000000, reward total was -17.000000. running mean: -20.507024\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.511953\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -20.496834\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.501865\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.506847\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.511778\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.516660\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.511494\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.516379\n",
            "resetting env. episode 108.000000, reward total was -18.000000. running mean: -20.491215\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.496303\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.501340\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.496327\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.491363\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.486450\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -20.471585\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -20.456869\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.462301\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.467678\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.463001\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.468371\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.473687\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.478950\n",
            "resetting env. episode 122.000000, reward total was -18.000000. running mean: -20.454161\n",
            "resetting env. episode 123.000000, reward total was -19.000000. running mean: -20.439619\n",
            "resetting env. episode 124.000000, reward total was -19.000000. running mean: -20.425223\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.430971\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.426661\n",
            "resetting env. episode 127.000000, reward total was -18.000000. running mean: -20.402394\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.398370\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.404387\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.410343\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.416239\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.412077\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.417956\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.423777\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.429539\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.425244\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.430991\n",
            "resetting env. episode 138.000000, reward total was -19.000000. running mean: -20.416681\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -20.402514\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.408489\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -20.394404\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.400460\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.396456\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.402491\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.408466\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.404382\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.400338\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.396334\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.402371\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.398347\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.394364\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.400420\n",
            "resetting env. episode 153.000000, reward total was -19.000000. running mean: -20.386416\n",
            "resetting env. episode 154.000000, reward total was -17.000000. running mean: -20.352552\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.349026\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.345536\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.352081\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.358560\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.364974\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -20.351325\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.357811\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.364233\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.360591\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -20.346985\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.353515\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.359980\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.366380\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.372716\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.378989\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.365199\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.361547\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.367932\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.354253\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.360710\n",
            "resetting env. episode 175.000000, reward total was -18.000000. running mean: -20.337103\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.333732\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.330395\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.337091\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.343720\n",
            "resetting env. episode 180.000000, reward total was -19.000000. running mean: -20.330283\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.336980\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.333610\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.340274\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.346871\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.333402\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.330068\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.326768\n",
            "resetting env. episode 188.000000, reward total was -19.000000. running mean: -20.313500\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.310365\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.317261\n",
            "resetting env. episode 191.000000, reward total was -16.000000. running mean: -20.274089\n",
            "resetting env. episode 192.000000, reward total was -19.000000. running mean: -20.261348\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.258734\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.266147\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.273486\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.270751\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.278043\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.265263\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.272610\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.259884\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.257285\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.264712\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.272065\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.269345\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.266651\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.253985\n",
            "resetting env. episode 207.000000, reward total was -18.000000. running mean: -20.231445\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.229130\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.236839\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.234471\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.222126\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.219905\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.217706\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.225529\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.223273\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.231041\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.228730\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.236443\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.244078\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -20.231638\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.239321\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.246928\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.254459\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.241914\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.249495\n",
            "resetting env. episode 226.000000, reward total was -18.000000. running mean: -20.227000\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.234730\n",
            "resetting env. episode 228.000000, reward total was -17.000000. running mean: -20.202383\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.210359\n",
            "resetting env. episode 230.000000, reward total was -19.000000. running mean: -20.198255\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.206273\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.214210\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.222068\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.229847\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.217549\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.215373\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.213220\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.211087\n",
            "resetting env. episode 239.000000, reward total was -19.000000. running mean: -20.198977\n",
            "resetting env. episode 240.000000, reward total was -18.000000. running mean: -20.176987\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.175217\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.173465\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.181730\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.179913\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.178114\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.186332\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.184469\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.182624\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.190798\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.188890\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.187001\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.195131\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.193180\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.201248\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.199236\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.197243\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -20.185271\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.193418\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.201484\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.209469\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.217375\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.225201\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.222949\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.220719\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.228512\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.226227\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.233965\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.241625\n",
            "resetting env. episode 269.000000, reward total was -19.000000. running mean: -20.229209\n",
            "resetting env. episode 270.000000, reward total was -18.000000. running mean: -20.206917\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.214848\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.222699\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.230472\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.238167\n",
            "resetting env. episode 275.000000, reward total was -18.000000. running mean: -20.215786\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.223628\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.231392\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.229078\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.236787\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.224419\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.212175\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.210053\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.207953\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.215873\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.223714\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.231477\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.229162\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.236871\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.234502\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.222157\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.229935\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.227636\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -20.215360\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.223206\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.230974\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.228664\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.226378\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.224114\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.221873\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.229654\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.237357\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.244984\n",
            "resetting env. episode 303.000000, reward total was -18.000000. running mean: -20.222534\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.210309\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.218206\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.216024\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.223863\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.231625\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.229308\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.227015\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.224745\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.232498\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.230173\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.237871\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.245492\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.253037\n",
            "resetting env. episode 317.000000, reward total was -18.000000. running mean: -20.230507\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.238202\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.245820\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.253362\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.250828\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.248320\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.255837\n",
            "resetting env. episode 324.000000, reward total was -19.000000. running mean: -20.243278\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.250846\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.258337\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.255754\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.263196\n",
            "resetting env. episode 329.000000, reward total was -18.000000. running mean: -20.240564\n",
            "resetting env. episode 330.000000, reward total was -19.000000. running mean: -20.228159\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.235877\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.233518\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.241183\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.238771\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.246383\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.253920\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.251380\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.258867\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.266278\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.263615\n",
            "resetting env. episode 341.000000, reward total was -19.000000. running mean: -20.250979\n",
            "resetting env. episode 342.000000, reward total was -18.000000. running mean: -20.228469\n",
            "resetting env. episode 343.000000, reward total was -18.000000. running mean: -20.206185\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.204123\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -20.192081\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.200161\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.208159\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.216077\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.223917\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.231678\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.239361\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.246967\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.254497\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.241953\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.249533\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.257038\n",
            "resetting env. episode 357.000000, reward total was -18.000000. running mean: -20.234467\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.242123\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.249701\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.257204\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.264632\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.251986\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.259466\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.266871\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.274203\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.261461\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.268846\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.276158\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.283396\n",
            "resetting env. episode 370.000000, reward total was -18.000000. running mean: -20.260562\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.267957\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.275277\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.282524\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.289699\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.286802\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.293934\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.290995\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.298085\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.295104\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.302153\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.309131\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.306040\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.312980\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -20.299850\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.306851\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.303783\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.310745\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.307637\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.314561\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.321415\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.328201\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.324919\n",
            "resetting env. episode 393.000000, reward total was -19.000000. running mean: -20.311670\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.308553\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.305468\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -20.292413\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.299489\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.306494\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.313429\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.310295\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.317192\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.324020\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.320780\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.327572\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.334296\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.340953\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.337544\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.344168\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.350727\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.357219\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.363647\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.360011\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.356411\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.362847\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.369218\n",
            "resetting env. episode 416.000000, reward total was -18.000000. running mean: -20.345526\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.342071\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.348650\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.345163\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.351712\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.338195\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.344813\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.351365\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.357851\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.364272\n",
            "resetting env. episode 426.000000, reward total was -19.000000. running mean: -20.350630\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.357123\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.363552\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.369917\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.366218\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.362555\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.368930\n",
            "resetting env. episode 433.000000, reward total was -18.000000. running mean: -20.345240\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.331788\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.328470\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.325186\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.331934\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.328614\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.325328\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.322075\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.328854\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.335566\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.332210\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.338888\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.325499\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.332244\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.338922\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.345532\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.342077\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.328656\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.325370\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.332116\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.328795\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.335507\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.332152\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.338830\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.345442\n",
            "resetting env. episode 458.000000, reward total was -18.000000. running mean: -20.321988\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.318768\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.305580\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.312524\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.319399\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.326205\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.332943\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.339613\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.346217\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.352755\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.359228\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.365635\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.361979\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.368359\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.354676\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.361129\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.367518\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.373842\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.380104\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -20.366303\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.372640\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.378914\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.375124\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.381373\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.387559\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -20.373684\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.369947\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.376247\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.382485\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.388660\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.374774\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.381026\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.377216\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.373443\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.359709\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.346112\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.342651\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.349224\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.355732\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.352175\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.358653\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.365066\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.371416\n",
            "CPU times: user 34min 48s, sys: 11min 10s, total: 45min 59s\n",
            "Wall time: 23min 51s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "a619097c-25c6-4cd9-9063-a82c2469cca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "78e75058-0277-4c96-ef1c-00097fb27a3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -19.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.029900\n",
            "resetting env. episode 4.000000, reward total was -18.000000. running mean: -19.019601\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -19.019405\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -19.029211\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.048919\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -19.068430\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.087745\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -19.106868\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.125799\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -19.144541\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -19.153096\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -19.151565\n",
            "resetting env. episode 15.000000, reward total was -18.000000. running mean: -19.140049\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -19.158649\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -19.177062\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -19.195292\n",
            "resetting env. episode 19.000000, reward total was -18.000000. running mean: -19.183339\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -19.191505\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -19.209590\n",
            "resetting env. episode 22.000000, reward total was -19.000000. running mean: -19.207494\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.225419\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -19.243165\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -19.250734\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -19.258226\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.275644\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -19.282888\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.300059\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -19.307058\n",
            "resetting env. episode 31.000000, reward total was -19.000000. running mean: -19.303987\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -19.320948\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -19.327738\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -19.344461\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -19.341016\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.357606\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -19.364030\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -19.370390\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -19.386686\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.402819\n",
            "resetting env. episode 41.000000, reward total was -19.000000. running mean: -19.398791\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -19.404803\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -19.410755\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.426647\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.442381\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.457957\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -19.463377\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -19.468744\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.484056\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.499216\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.514223\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -19.519081\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -19.523890\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -19.528651\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.543365\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -19.547931\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.562452\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.576827\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.591059\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.605149\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -19.599097\n",
            "resetting env. episode 62.000000, reward total was -19.000000. running mean: -19.593106\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.607175\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.621103\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -19.614892\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -19.618743\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.632556\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.646230\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -19.649768\n",
            "resetting env. episode 70.000000, reward total was -19.000000. running mean: -19.643270\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -19.646838\n",
            "resetting env. episode 72.000000, reward total was -18.000000. running mean: -19.630369\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -19.634066\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -19.647725\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -19.651248\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -19.644735\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.658288\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -19.651705\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.665188\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.678536\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -19.671751\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.685033\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.698183\n",
            "resetting env. episode 84.000000, reward total was -18.000000. running mean: -19.681201\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -19.684389\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.687545\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -19.680670\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.693863\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -19.696924\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -19.699955\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -19.702956\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.715926\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -19.718767\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.731579\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.744263\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.756821\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.769252\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -19.781560\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.793744\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.805807\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.817749\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -19.819571\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -19.821376\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.833162\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.844830\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.856382\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.857818\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.869240\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.880548\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -19.881742\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -19.882925\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -19.874095\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -19.885354\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -19.886501\n",
            "resetting env. episode 115.000000, reward total was -18.000000. running mean: -19.867636\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.878960\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -19.880170\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -19.881368\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -19.872555\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -19.873829\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -19.875091\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.876340\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -19.887576\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -19.898701\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -19.909714\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -19.910617\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -19.911510\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -19.922395\n",
            "resetting env. episode 129.000000, reward total was -17.000000. running mean: -19.893171\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.904240\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -19.915197\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -19.926045\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -19.916785\n",
            "resetting env. episode 134.000000, reward total was -19.000000. running mean: -19.907617\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -19.898541\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -19.909555\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -19.920460\n",
            "resetting env. episode 138.000000, reward total was -19.000000. running mean: -19.911255\n",
            "resetting env. episode 139.000000, reward total was -18.000000. running mean: -19.892143\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.903221\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -19.894189\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -19.895247\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -19.896295\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -19.897332\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -19.908358\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -19.899275\n",
            "resetting env. episode 147.000000, reward total was -18.000000. running mean: -19.880282\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -19.891479\n",
            "resetting env. episode 149.000000, reward total was -19.000000. running mean: -19.882564\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -19.893739\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -19.904801\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -19.915753\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -19.926596\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -19.927330\n",
            "resetting env. episode 155.000000, reward total was -19.000000. running mean: -19.918057\n",
            "resetting env. episode 156.000000, reward total was -19.000000. running mean: -19.908876\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -19.909787\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -19.920689\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -19.921482\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -19.922268\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -19.933045\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -19.933715\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -19.944377\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -19.934934\n",
            "resetting env. episode 165.000000, reward total was -18.000000. running mean: -19.915584\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -19.916428\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -19.907264\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -19.918192\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -19.919010\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -19.909820\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -19.920721\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -19.911514\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -19.912399\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -19.923275\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -19.924042\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -19.934802\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -19.935454\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -19.936099\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -19.946738\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -19.957271\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -19.957698\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -19.958121\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -19.958540\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -19.948955\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -19.959465\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -19.969870\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -19.960172\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -19.960570\n",
            "resetting env. episode 189.000000, reward total was -18.000000. running mean: -19.940964\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -19.951555\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -19.962039\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -19.972419\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -19.982694\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -19.992868\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -19.992939\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.003009\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.002979\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -19.992950\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.003020\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.002990\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.002960\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.002930\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.012901\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.022772\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.012544\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.012419\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.022295\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.012072\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.021951\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.021732\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.031514\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.021199\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.030987\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.030677\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.030370\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.040067\n",
            "resetting env. episode 217.000000, reward total was -19.000000. running mean: -20.029666\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.019369\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.019176\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -20.008984\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.008894\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.018805\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.028617\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.028331\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.038048\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.037667\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.037291\n",
            "resetting env. episode 228.000000, reward total was -19.000000. running mean: -20.026918\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.026648\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.036382\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.036018\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.025658\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.035401\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.035047\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.024697\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.034450\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.044105\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.053664\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.053128\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.062596\n",
            "resetting env. episode 241.000000, reward total was -18.000000. running mean: -20.041970\n",
            "resetting env. episode 242.000000, reward total was -18.000000. running mean: -20.021551\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.031335\n",
            "resetting env. episode 244.000000, reward total was -19.000000. running mean: -20.021022\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.020812\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.020604\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.030398\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.030094\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.019793\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.019595\n",
            "resetting env. episode 251.000000, reward total was -17.000000. running mean: -19.989399\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -19.999505\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.009510\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.009415\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.019320\n",
            "resetting env. episode 256.000000, reward total was -18.000000. running mean: -19.999127\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -19.989136\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -19.989245\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -19.979352\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -19.989559\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -19.999663\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.009666\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.009570\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.019474\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.029279\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.038987\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -20.028597\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.038311\n",
            "resetting env. episode 269.000000, reward total was -19.000000. running mean: -20.027928\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.037648\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.047272\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.046799\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.046331\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.055868\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.065309\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.064656\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.064009\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.063369\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.062736\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.052108\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.061587\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.070971\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.070262\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.079559\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.088763\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.097876\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.106897\n",
            "resetting env. episode 288.000000, reward total was -19.000000. running mean: -20.095828\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.104870\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.113821\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.122683\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.121456\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.120242\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.129039\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.137749\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.146371\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.154908\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.153358\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.161825\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.170207\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.178505\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.186720\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.194852\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.192904\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.200975\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.198965\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.206975\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.214906\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.202757\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.210729\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.218622\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.226435\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.224171\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.211929\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.219810\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.217612\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.225436\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.213182\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.221050\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.228839\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.236551\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.234185\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.241843\n",
            "resetting env. episode 324.000000, reward total was -19.000000. running mean: -20.229425\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.237131\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.244759\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.242312\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.249889\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.257390\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.264816\n",
            "resetting env. episode 331.000000, reward total was -19.000000. running mean: -20.252168\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.259646\n",
            "resetting env. episode 333.000000, reward total was -19.000000. running mean: -20.247050\n",
            "resetting env. episode 334.000000, reward total was -17.000000. running mean: -20.214579\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.212433\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.210309\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.208206\n",
            "resetting env. episode 338.000000, reward total was -18.000000. running mean: -20.186124\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.194263\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.192320\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.190397\n",
            "resetting env. episode 342.000000, reward total was -18.000000. running mean: -20.168493\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.166808\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.165140\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -20.153488\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.151954\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.140434\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.139030\n",
            "resetting env. episode 349.000000, reward total was -19.000000. running mean: -20.127639\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.126363\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.125099\n",
            "resetting env. episode 352.000000, reward total was -17.000000. running mean: -20.093848\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.102910\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.091881\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.090962\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.090052\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.099152\n",
            "resetting env. episode 358.000000, reward total was -18.000000. running mean: -20.078160\n",
            "resetting env. episode 359.000000, reward total was -19.000000. running mean: -20.067379\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.076705\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.065938\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.075279\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.084526\n",
            "resetting env. episode 364.000000, reward total was -17.000000. running mean: -20.053680\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.063144\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.072512\n",
            "resetting env. episode 367.000000, reward total was -19.000000. running mean: -20.061787\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.071169\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.060458\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.069853\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.079154\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.088363\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.097479\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.096504\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.095539\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.094584\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.103638\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.102602\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.101576\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.110560\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.109454\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.118360\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.107176\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.106105\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.115043\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.123893\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.122654\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -20.111428\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.120313\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.119110\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.127919\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.116640\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.125473\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.124219\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.112977\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.111847\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.120728\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.129521\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.138226\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.136844\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.145475\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.154020\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.152480\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.160955\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.169346\n",
            "resetting env. episode 406.000000, reward total was -19.000000. running mean: -20.157652\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.156076\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.164515\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.162870\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.171241\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.169529\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.177834\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.176055\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.184295\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.182452\n",
            "resetting env. episode 416.000000, reward total was -18.000000. running mean: -20.160627\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.169021\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.177331\n",
            "resetting env. episode 419.000000, reward total was -18.000000. running mean: -20.155557\n",
            "resetting env. episode 420.000000, reward total was -18.000000. running mean: -20.134002\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.122662\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.121435\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.120221\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.119019\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.117828\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.116650\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.115484\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.114329\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.123186\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.121954\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.120734\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.129527\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.138232\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.146849\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -20.135381\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.144027\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.152587\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.161061\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.169450\n",
            "resetting env. episode 440.000000, reward total was -19.000000. running mean: -20.157756\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.166178\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.174516\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.182771\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.180943\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.189134\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.197243\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.195270\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.203318\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.201284\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.209272\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.207179\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.215107\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.222956\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.220726\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.228519\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.236234\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.243872\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.241433\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.239019\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.246628\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.254162\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.261620\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.269004\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.276314\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.273551\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.270816\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.258107\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.265526\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.262871\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.270242\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.277540\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.274765\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.272017\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.279297\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.286504\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.293639\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -20.280702\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.277895\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.275116\n",
            "resetting env. episode 480.000000, reward total was -18.000000. running mean: -20.252365\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.259842\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.267243\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -20.254571\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.252025\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.249505\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.257010\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.244440\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.241995\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.239575\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.237179\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.234808\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.242460\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.240035\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.247635\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.245158\n",
            "resetting env. episode 496.000000, reward total was -16.000000. running mean: -20.202707\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.210680\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.218573\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.206387\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.204323\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.202280\n",
            "resetting env. episode 502.000000, reward total was -19.000000. running mean: -20.190257\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -20.188355\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.196471\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.204506\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.212461\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.220337\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.218133\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.225952\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.223693\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.221456\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.229241\n",
            "resetting env. episode 513.000000, reward total was -18.000000. running mean: -20.206949\n",
            "resetting env. episode 514.000000, reward total was -20.000000. running mean: -20.204879\n",
            "resetting env. episode 515.000000, reward total was -19.000000. running mean: -20.192830\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.200902\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -20.198893\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.206904\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.204835\n",
            "resetting env. episode 520.000000, reward total was -19.000000. running mean: -20.192787\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.200859\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.208850\n",
            "resetting env. episode 523.000000, reward total was -21.000000. running mean: -20.216762\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.214594\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -20.222448\n",
            "resetting env. episode 526.000000, reward total was -18.000000. running mean: -20.200224\n",
            "resetting env. episode 527.000000, reward total was -19.000000. running mean: -20.188221\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.196339\n",
            "resetting env. episode 529.000000, reward total was -17.000000. running mean: -20.164376\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.172732\n",
            "resetting env. episode 531.000000, reward total was -19.000000. running mean: -20.161005\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.169395\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.177701\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.185924\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.194065\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.202124\n",
            "resetting env. episode 537.000000, reward total was -19.000000. running mean: -20.190103\n",
            "resetting env. episode 538.000000, reward total was -20.000000. running mean: -20.188202\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.196320\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.204356\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.212313\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -20.210190\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.218088\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -20.215907\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -20.213748\n",
            "resetting env. episode 546.000000, reward total was -20.000000. running mean: -20.211610\n",
            "resetting env. episode 547.000000, reward total was -20.000000. running mean: -20.209494\n",
            "resetting env. episode 548.000000, reward total was -20.000000. running mean: -20.207399\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.215325\n",
            "resetting env. episode 550.000000, reward total was -20.000000. running mean: -20.213172\n",
            "resetting env. episode 551.000000, reward total was -20.000000. running mean: -20.211040\n",
            "resetting env. episode 552.000000, reward total was -20.000000. running mean: -20.208930\n",
            "resetting env. episode 553.000000, reward total was -20.000000. running mean: -20.206841\n",
            "resetting env. episode 554.000000, reward total was -19.000000. running mean: -20.194772\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.202825\n",
            "resetting env. episode 556.000000, reward total was -21.000000. running mean: -20.210796\n",
            "resetting env. episode 557.000000, reward total was -20.000000. running mean: -20.208688\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -20.206601\n",
            "resetting env. episode 559.000000, reward total was -18.000000. running mean: -20.184535\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -20.182690\n",
            "resetting env. episode 561.000000, reward total was -17.000000. running mean: -20.150863\n",
            "resetting env. episode 562.000000, reward total was -20.000000. running mean: -20.149355\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.157861\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.166282\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.174620\n",
            "resetting env. episode 566.000000, reward total was -20.000000. running mean: -20.172873\n",
            "resetting env. episode 567.000000, reward total was -19.000000. running mean: -20.161145\n",
            "resetting env. episode 568.000000, reward total was -20.000000. running mean: -20.159533\n",
            "resetting env. episode 569.000000, reward total was -19.000000. running mean: -20.147938\n",
            "resetting env. episode 570.000000, reward total was -19.000000. running mean: -20.136458\n",
            "resetting env. episode 571.000000, reward total was -20.000000. running mean: -20.135094\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.143743\n",
            "resetting env. episode 573.000000, reward total was -19.000000. running mean: -20.132306\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.140982\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.149573\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.158077\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -20.156496\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.164931\n",
            "resetting env. episode 579.000000, reward total was -20.000000. running mean: -20.163282\n",
            "resetting env. episode 580.000000, reward total was -19.000000. running mean: -20.151649\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.160133\n",
            "resetting env. episode 582.000000, reward total was -20.000000. running mean: -20.158531\n",
            "resetting env. episode 583.000000, reward total was -19.000000. running mean: -20.146946\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.155476\n",
            "resetting env. episode 585.000000, reward total was -20.000000. running mean: -20.153922\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.162382\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.170759\n",
            "resetting env. episode 588.000000, reward total was -19.000000. running mean: -20.159051\n",
            "resetting env. episode 589.000000, reward total was -18.000000. running mean: -20.137461\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.146086\n",
            "resetting env. episode 591.000000, reward total was -20.000000. running mean: -20.144625\n",
            "resetting env. episode 592.000000, reward total was -20.000000. running mean: -20.143179\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.151747\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.160230\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.168627\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.176941\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.185172\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.183320\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.191487\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.199572\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.207576\n",
            "resetting env. episode 602.000000, reward total was -19.000000. running mean: -20.195500\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.203545\n",
            "resetting env. episode 604.000000, reward total was -20.000000. running mean: -20.201510\n",
            "resetting env. episode 605.000000, reward total was -20.000000. running mean: -20.199495\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.197500\n",
            "resetting env. episode 607.000000, reward total was -20.000000. running mean: -20.195525\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.203570\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -20.201534\n",
            "resetting env. episode 610.000000, reward total was -20.000000. running mean: -20.199519\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.207523\n",
            "resetting env. episode 612.000000, reward total was -19.000000. running mean: -20.195448\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.203494\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -20.201459\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.209444\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -20.207350\n",
            "resetting env. episode 617.000000, reward total was -19.000000. running mean: -20.195276\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -20.193323\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.191390\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.199476\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.207482\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -20.205407\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.213353\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.221219\n",
            "resetting env. episode 625.000000, reward total was -18.000000. running mean: -20.199007\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.207017\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.214947\n",
            "resetting env. episode 628.000000, reward total was -20.000000. running mean: -20.212797\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.220669\n",
            "resetting env. episode 630.000000, reward total was -17.000000. running mean: -20.188463\n",
            "resetting env. episode 631.000000, reward total was -19.000000. running mean: -20.176578\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.184812\n",
            "resetting env. episode 633.000000, reward total was -20.000000. running mean: -20.182964\n",
            "resetting env. episode 634.000000, reward total was -19.000000. running mean: -20.171134\n",
            "resetting env. episode 635.000000, reward total was -20.000000. running mean: -20.169423\n",
            "resetting env. episode 636.000000, reward total was -20.000000. running mean: -20.167729\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.176052\n",
            "resetting env. episode 638.000000, reward total was -19.000000. running mean: -20.164291\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.172648\n",
            "resetting env. episode 640.000000, reward total was -19.000000. running mean: -20.160922\n",
            "resetting env. episode 641.000000, reward total was -20.000000. running mean: -20.159312\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.167719\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -20.166042\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.174382\n",
            "resetting env. episode 645.000000, reward total was -19.000000. running mean: -20.162638\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.171011\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.179301\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.187508\n",
            "resetting env. episode 649.000000, reward total was -20.000000. running mean: -20.185633\n",
            "resetting env. episode 650.000000, reward total was -18.000000. running mean: -20.163777\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.172139\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.180418\n",
            "resetting env. episode 653.000000, reward total was -21.000000. running mean: -20.188614\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.196727\n",
            "resetting env. episode 655.000000, reward total was -20.000000. running mean: -20.194760\n",
            "resetting env. episode 656.000000, reward total was -21.000000. running mean: -20.202813\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.210784\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.218677\n",
            "resetting env. episode 659.000000, reward total was -20.000000. running mean: -20.216490\n",
            "resetting env. episode 660.000000, reward total was -19.000000. running mean: -20.204325\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -20.202282\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.200259\n",
            "resetting env. episode 663.000000, reward total was -19.000000. running mean: -20.188256\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.196374\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.204410\n",
            "resetting env. episode 666.000000, reward total was -18.000000. running mean: -20.182366\n",
            "resetting env. episode 667.000000, reward total was -20.000000. running mean: -20.180542\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.188737\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.196849\n",
            "resetting env. episode 670.000000, reward total was -19.000000. running mean: -20.184881\n",
            "resetting env. episode 671.000000, reward total was -20.000000. running mean: -20.183032\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.181202\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.189390\n",
            "resetting env. episode 674.000000, reward total was -20.000000. running mean: -20.187496\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -20.185621\n",
            "resetting env. episode 676.000000, reward total was -19.000000. running mean: -20.173765\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.172027\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.180307\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -20.178504\n",
            "resetting env. episode 680.000000, reward total was -18.000000. running mean: -20.156719\n",
            "resetting env. episode 681.000000, reward total was -20.000000. running mean: -20.155152\n",
            "resetting env. episode 682.000000, reward total was -18.000000. running mean: -20.133600\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.132264\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.130941\n",
            "resetting env. episode 685.000000, reward total was -20.000000. running mean: -20.129632\n",
            "resetting env. episode 686.000000, reward total was -20.000000. running mean: -20.128336\n",
            "resetting env. episode 687.000000, reward total was -18.000000. running mean: -20.107052\n",
            "resetting env. episode 688.000000, reward total was -20.000000. running mean: -20.105982\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.114922\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.113773\n",
            "resetting env. episode 691.000000, reward total was -18.000000. running mean: -20.092635\n",
            "resetting env. episode 692.000000, reward total was -18.000000. running mean: -20.071709\n",
            "resetting env. episode 693.000000, reward total was -20.000000. running mean: -20.070992\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.080282\n",
            "resetting env. episode 695.000000, reward total was -18.000000. running mean: -20.059479\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.068884\n",
            "resetting env. episode 697.000000, reward total was -19.000000. running mean: -20.058195\n",
            "resetting env. episode 698.000000, reward total was -20.000000. running mean: -20.057613\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.067037\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.066367\n",
            "resetting env. episode 701.000000, reward total was -18.000000. running mean: -20.045703\n",
            "resetting env. episode 702.000000, reward total was -19.000000. running mean: -20.035246\n",
            "resetting env. episode 703.000000, reward total was -20.000000. running mean: -20.034894\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.034545\n",
            "resetting env. episode 705.000000, reward total was -19.000000. running mean: -20.024199\n",
            "resetting env. episode 706.000000, reward total was -20.000000. running mean: -20.023957\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -20.033718\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -20.033380\n",
            "resetting env. episode 709.000000, reward total was -20.000000. running mean: -20.033047\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -20.032716\n",
            "resetting env. episode 711.000000, reward total was -20.000000. running mean: -20.032389\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.042065\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.051644\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -20.051128\n",
            "resetting env. episode 715.000000, reward total was -20.000000. running mean: -20.050617\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.060111\n",
            "resetting env. episode 717.000000, reward total was -20.000000. running mean: -20.059509\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -20.058914\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.068325\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.077642\n",
            "resetting env. episode 721.000000, reward total was -20.000000. running mean: -20.076866\n",
            "resetting env. episode 722.000000, reward total was -19.000000. running mean: -20.066097\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.075436\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -20.084682\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.093835\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -20.092896\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.101967\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -20.100948\n",
            "resetting env. episode 729.000000, reward total was -19.000000. running mean: -20.089938\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.099039\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -20.108049\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.116968\n",
            "resetting env. episode 733.000000, reward total was -20.000000. running mean: -20.115798\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.114640\n",
            "resetting env. episode 735.000000, reward total was -20.000000. running mean: -20.113494\n",
            "resetting env. episode 736.000000, reward total was -19.000000. running mean: -20.102359\n",
            "resetting env. episode 737.000000, reward total was -19.000000. running mean: -20.091335\n",
            "resetting env. episode 738.000000, reward total was -20.000000. running mean: -20.090422\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -20.089518\n",
            "resetting env. episode 740.000000, reward total was -20.000000. running mean: -20.088623\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.097736\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -20.106759\n",
            "resetting env. episode 743.000000, reward total was -19.000000. running mean: -20.095692\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.104735\n",
            "resetting env. episode 745.000000, reward total was -18.000000. running mean: -20.083687\n",
            "resetting env. episode 746.000000, reward total was -19.000000. running mean: -20.072850\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.082122\n",
            "resetting env. episode 748.000000, reward total was -20.000000. running mean: -20.081301\n",
            "resetting env. episode 749.000000, reward total was -20.000000. running mean: -20.080488\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.089683\n",
            "resetting env. episode 751.000000, reward total was -19.000000. running mean: -20.078786\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -20.077998\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.087218\n",
            "resetting env. episode 754.000000, reward total was -19.000000. running mean: -20.076346\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -20.075582\n",
            "resetting env. episode 756.000000, reward total was -20.000000. running mean: -20.074827\n",
            "resetting env. episode 757.000000, reward total was -20.000000. running mean: -20.074078\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.083338\n",
            "resetting env. episode 759.000000, reward total was -20.000000. running mean: -20.082504\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -20.091679\n",
            "resetting env. episode 761.000000, reward total was -20.000000. running mean: -20.090762\n",
            "resetting env. episode 762.000000, reward total was -20.000000. running mean: -20.089855\n",
            "resetting env. episode 763.000000, reward total was -20.000000. running mean: -20.088956\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -20.088067\n",
            "resetting env. episode 765.000000, reward total was -20.000000. running mean: -20.087186\n",
            "resetting env. episode 766.000000, reward total was -19.000000. running mean: -20.076314\n",
            "resetting env. episode 767.000000, reward total was -20.000000. running mean: -20.075551\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.084795\n",
            "resetting env. episode 769.000000, reward total was -20.000000. running mean: -20.083948\n",
            "resetting env. episode 770.000000, reward total was -19.000000. running mean: -20.073108\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -20.082377\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -20.081553\n",
            "resetting env. episode 773.000000, reward total was -20.000000. running mean: -20.080738\n",
            "resetting env. episode 774.000000, reward total was -19.000000. running mean: -20.069930\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.079231\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -20.088439\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.097554\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -20.106579\n",
            "resetting env. episode 779.000000, reward total was -19.000000. running mean: -20.095513\n",
            "resetting env. episode 780.000000, reward total was -20.000000. running mean: -20.094558\n",
            "resetting env. episode 781.000000, reward total was -20.000000. running mean: -20.093612\n",
            "resetting env. episode 782.000000, reward total was -18.000000. running mean: -20.072676\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.081949\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.091130\n",
            "resetting env. episode 785.000000, reward total was -19.000000. running mean: -20.080219\n",
            "resetting env. episode 786.000000, reward total was -18.000000. running mean: -20.059416\n",
            "resetting env. episode 787.000000, reward total was -18.000000. running mean: -20.038822\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.048434\n",
            "resetting env. episode 789.000000, reward total was -20.000000. running mean: -20.047950\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -20.047470\n",
            "resetting env. episode 791.000000, reward total was -18.000000. running mean: -20.026995\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -20.026726\n",
            "resetting env. episode 793.000000, reward total was -20.000000. running mean: -20.026458\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.036194\n",
            "resetting env. episode 795.000000, reward total was -20.000000. running mean: -20.035832\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.045473\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.055019\n",
            "resetting env. episode 798.000000, reward total was -20.000000. running mean: -20.054468\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.063924\n",
            "resetting env. episode 800.000000, reward total was -19.000000. running mean: -20.053285\n",
            "resetting env. episode 801.000000, reward total was -19.000000. running mean: -20.042752\n",
            "resetting env. episode 802.000000, reward total was -18.000000. running mean: -20.022324\n",
            "resetting env. episode 803.000000, reward total was -19.000000. running mean: -20.012101\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.021980\n",
            "resetting env. episode 805.000000, reward total was -20.000000. running mean: -20.021760\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.031543\n",
            "resetting env. episode 807.000000, reward total was -20.000000. running mean: -20.031227\n",
            "resetting env. episode 808.000000, reward total was -20.000000. running mean: -20.030915\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.040606\n",
            "resetting env. episode 810.000000, reward total was -17.000000. running mean: -20.010200\n",
            "resetting env. episode 811.000000, reward total was -21.000000. running mean: -20.020098\n",
            "resetting env. episode 812.000000, reward total was -19.000000. running mean: -20.009897\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.019798\n",
            "resetting env. episode 814.000000, reward total was -20.000000. running mean: -20.019600\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.029404\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.039110\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.048719\n",
            "resetting env. episode 818.000000, reward total was -20.000000. running mean: -20.048231\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.057749\n",
            "resetting env. episode 820.000000, reward total was -19.000000. running mean: -20.047172\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.056700\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.066133\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.075472\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.084717\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -20.093870\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -20.092931\n",
            "resetting env. episode 827.000000, reward total was -21.000000. running mean: -20.102002\n",
            "resetting env. episode 828.000000, reward total was -20.000000. running mean: -20.100982\n",
            "resetting env. episode 829.000000, reward total was -19.000000. running mean: -20.089972\n",
            "resetting env. episode 830.000000, reward total was -20.000000. running mean: -20.089072\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -20.098181\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.107200\n",
            "resetting env. episode 833.000000, reward total was -19.000000. running mean: -20.096128\n",
            "resetting env. episode 834.000000, reward total was -19.000000. running mean: -20.085166\n",
            "resetting env. episode 835.000000, reward total was -20.000000. running mean: -20.084315\n",
            "resetting env. episode 836.000000, reward total was -18.000000. running mean: -20.063472\n",
            "resetting env. episode 837.000000, reward total was -20.000000. running mean: -20.062837\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.072208\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.081486\n",
            "resetting env. episode 840.000000, reward total was -19.000000. running mean: -20.070671\n",
            "resetting env. episode 841.000000, reward total was -20.000000. running mean: -20.069965\n",
            "resetting env. episode 842.000000, reward total was -20.000000. running mean: -20.069265\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -20.078572\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.087787\n",
            "resetting env. episode 845.000000, reward total was -18.000000. running mean: -20.066909\n",
            "resetting env. episode 846.000000, reward total was -20.000000. running mean: -20.066240\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.075577\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -20.074822\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.084073\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -20.083233\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -20.082400\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.091576\n",
            "resetting env. episode 853.000000, reward total was -19.000000. running mean: -20.080661\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -20.089854\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.098955\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.107966\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -20.116886\n",
            "resetting env. episode 858.000000, reward total was -18.000000. running mean: -20.095717\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -20.104760\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.113713\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.122575\n",
            "resetting env. episode 862.000000, reward total was -18.000000. running mean: -20.101350\n",
            "resetting env. episode 863.000000, reward total was -20.000000. running mean: -20.100336\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.109333\n",
            "resetting env. episode 865.000000, reward total was -18.000000. running mean: -20.088240\n",
            "resetting env. episode 866.000000, reward total was -20.000000. running mean: -20.087357\n",
            "resetting env. episode 867.000000, reward total was -18.000000. running mean: -20.066484\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.075819\n",
            "resetting env. episode 869.000000, reward total was -19.000000. running mean: -20.065061\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -20.074410\n",
            "resetting env. episode 871.000000, reward total was -20.000000. running mean: -20.073666\n",
            "resetting env. episode 872.000000, reward total was -20.000000. running mean: -20.072929\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.082200\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.091378\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -20.090464\n",
            "resetting env. episode 876.000000, reward total was -18.000000. running mean: -20.069559\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.078864\n",
            "resetting env. episode 878.000000, reward total was -19.000000. running mean: -20.068075\n",
            "resetting env. episode 879.000000, reward total was -20.000000. running mean: -20.067394\n",
            "resetting env. episode 880.000000, reward total was -20.000000. running mean: -20.066721\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.076053\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.075293\n",
            "resetting env. episode 883.000000, reward total was -20.000000. running mean: -20.074540\n",
            "resetting env. episode 884.000000, reward total was -19.000000. running mean: -20.063794\n",
            "resetting env. episode 885.000000, reward total was -19.000000. running mean: -20.053157\n",
            "resetting env. episode 886.000000, reward total was -20.000000. running mean: -20.052625\n",
            "resetting env. episode 887.000000, reward total was -20.000000. running mean: -20.052099\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.061578\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.070962\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.080252\n",
            "resetting env. episode 891.000000, reward total was -20.000000. running mean: -20.079450\n",
            "resetting env. episode 892.000000, reward total was -20.000000. running mean: -20.078655\n",
            "resetting env. episode 893.000000, reward total was -19.000000. running mean: -20.067869\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.077190\n",
            "resetting env. episode 895.000000, reward total was -19.000000. running mean: -20.066418\n",
            "resetting env. episode 896.000000, reward total was -18.000000. running mean: -20.045754\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.055296\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.064743\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.074096\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.083355\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.092522\n",
            "resetting env. episode 902.000000, reward total was -15.000000. running mean: -20.041596\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.051180\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.060669\n",
            "resetting env. episode 905.000000, reward total was -19.000000. running mean: -20.050062\n",
            "resetting env. episode 906.000000, reward total was -20.000000. running mean: -20.049561\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -20.059066\n",
            "resetting env. episode 908.000000, reward total was -20.000000. running mean: -20.058475\n",
            "resetting env. episode 909.000000, reward total was -19.000000. running mean: -20.047890\n",
            "resetting env. episode 910.000000, reward total was -18.000000. running mean: -20.027411\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -20.037137\n",
            "resetting env. episode 912.000000, reward total was -19.000000. running mean: -20.026766\n",
            "resetting env. episode 913.000000, reward total was -20.000000. running mean: -20.026498\n",
            "resetting env. episode 914.000000, reward total was -20.000000. running mean: -20.026233\n",
            "resetting env. episode 915.000000, reward total was -19.000000. running mean: -20.015971\n",
            "resetting env. episode 916.000000, reward total was -20.000000. running mean: -20.015811\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.025653\n",
            "resetting env. episode 918.000000, reward total was -18.000000. running mean: -20.005397\n",
            "resetting env. episode 919.000000, reward total was -20.000000. running mean: -20.005343\n",
            "resetting env. episode 920.000000, reward total was -18.000000. running mean: -19.985289\n",
            "resetting env. episode 921.000000, reward total was -20.000000. running mean: -19.985436\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -19.985582\n",
            "resetting env. episode 923.000000, reward total was -18.000000. running mean: -19.965726\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -19.976069\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -19.976308\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -19.986545\n",
            "resetting env. episode 927.000000, reward total was -19.000000. running mean: -19.976680\n",
            "resetting env. episode 928.000000, reward total was -19.000000. running mean: -19.966913\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -19.977244\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -19.987471\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -19.997596\n",
            "resetting env. episode 932.000000, reward total was -20.000000. running mean: -19.997621\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.007644\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -20.017568\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.027392\n",
            "resetting env. episode 936.000000, reward total was -21.000000. running mean: -20.037118\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -20.036747\n",
            "resetting env. episode 938.000000, reward total was -19.000000. running mean: -20.026380\n",
            "resetting env. episode 939.000000, reward total was -21.000000. running mean: -20.036116\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.045755\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -20.055297\n",
            "resetting env. episode 942.000000, reward total was -19.000000. running mean: -20.044744\n",
            "resetting env. episode 943.000000, reward total was -19.000000. running mean: -20.034297\n",
            "resetting env. episode 944.000000, reward total was -18.000000. running mean: -20.013954\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -20.023814\n",
            "resetting env. episode 946.000000, reward total was -20.000000. running mean: -20.023576\n",
            "resetting env. episode 947.000000, reward total was -19.000000. running mean: -20.013340\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -20.013207\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.013075\n",
            "resetting env. episode 950.000000, reward total was -19.000000. running mean: -20.002944\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.012915\n",
            "resetting env. episode 952.000000, reward total was -19.000000. running mean: -20.002785\n",
            "resetting env. episode 953.000000, reward total was -19.000000. running mean: -19.992758\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.002830\n",
            "resetting env. episode 955.000000, reward total was -17.000000. running mean: -19.972802\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -19.983074\n",
            "resetting env. episode 957.000000, reward total was -20.000000. running mean: -19.983243\n",
            "resetting env. episode 958.000000, reward total was -20.000000. running mean: -19.983411\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -19.993576\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.003641\n",
            "resetting env. episode 961.000000, reward total was -20.000000. running mean: -20.003604\n",
            "resetting env. episode 962.000000, reward total was -19.000000. running mean: -19.993568\n",
            "resetting env. episode 963.000000, reward total was -20.000000. running mean: -19.993633\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.003696\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.013659\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.023523\n",
            "resetting env. episode 967.000000, reward total was -19.000000. running mean: -20.013287\n",
            "resetting env. episode 968.000000, reward total was -20.000000. running mean: -20.013155\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.023023\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.032793\n",
            "resetting env. episode 971.000000, reward total was -19.000000. running mean: -20.022465\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.032240\n",
            "resetting env. episode 973.000000, reward total was -18.000000. running mean: -20.011918\n",
            "resetting env. episode 974.000000, reward total was -19.000000. running mean: -20.001799\n",
            "resetting env. episode 975.000000, reward total was -19.000000. running mean: -19.991781\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.001863\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -20.001844\n",
            "resetting env. episode 978.000000, reward total was -20.000000. running mean: -20.001826\n",
            "resetting env. episode 979.000000, reward total was -17.000000. running mean: -19.971808\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -19.982089\n",
            "resetting env. episode 981.000000, reward total was -20.000000. running mean: -19.982269\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -19.992446\n",
            "resetting env. episode 983.000000, reward total was -20.000000. running mean: -19.992521\n",
            "resetting env. episode 984.000000, reward total was -20.000000. running mean: -19.992596\n",
            "resetting env. episode 985.000000, reward total was -18.000000. running mean: -19.972670\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -19.982944\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -19.993114\n",
            "resetting env. episode 988.000000, reward total was -18.000000. running mean: -19.973183\n",
            "resetting env. episode 989.000000, reward total was -20.000000. running mean: -19.973451\n",
            "resetting env. episode 990.000000, reward total was -20.000000. running mean: -19.973717\n",
            "resetting env. episode 991.000000, reward total was -20.000000. running mean: -19.973979\n",
            "resetting env. episode 992.000000, reward total was -20.000000. running mean: -19.974240\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -19.974497\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -19.984752\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -19.994905\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.004956\n",
            "resetting env. episode 997.000000, reward total was -19.000000. running mean: -19.994906\n",
            "resetting env. episode 998.000000, reward total was -20.000000. running mean: -19.994957\n",
            "resetting env. episode 999.000000, reward total was -20.000000. running mean: -19.995008\n",
            "resetting env. episode 1000.000000, reward total was -19.000000. running mean: -19.985057\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -19.995207\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -19.995255\n",
            "resetting env. episode 1003.000000, reward total was -19.000000. running mean: -19.985302\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -19.995449\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.005495\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -20.005440\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.015385\n",
            "resetting env. episode 1008.000000, reward total was -20.000000. running mean: -20.015232\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.025079\n",
            "resetting env. episode 1010.000000, reward total was -19.000000. running mean: -20.014828\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.024680\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -20.034433\n",
            "resetting env. episode 1013.000000, reward total was -20.000000. running mean: -20.034089\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.043748\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.053311\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.052778\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -20.062250\n",
            "resetting env. episode 1018.000000, reward total was -21.000000. running mean: -20.071627\n",
            "resetting env. episode 1019.000000, reward total was -20.000000. running mean: -20.070911\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.080202\n",
            "resetting env. episode 1021.000000, reward total was -19.000000. running mean: -20.069400\n",
            "resetting env. episode 1022.000000, reward total was -20.000000. running mean: -20.068706\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -20.068019\n",
            "resetting env. episode 1024.000000, reward total was -20.000000. running mean: -20.067339\n",
            "resetting env. episode 1025.000000, reward total was -20.000000. running mean: -20.066665\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.075999\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.085239\n",
            "resetting env. episode 1028.000000, reward total was -19.000000. running mean: -20.074386\n",
            "resetting env. episode 1029.000000, reward total was -19.000000. running mean: -20.063642\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -20.073006\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.082276\n",
            "resetting env. episode 1032.000000, reward total was -18.000000. running mean: -20.061453\n",
            "resetting env. episode 1033.000000, reward total was -20.000000. running mean: -20.060839\n",
            "resetting env. episode 1034.000000, reward total was -20.000000. running mean: -20.060230\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -20.059628\n",
            "resetting env. episode 1036.000000, reward total was -17.000000. running mean: -20.029032\n",
            "resetting env. episode 1037.000000, reward total was -19.000000. running mean: -20.018741\n",
            "resetting env. episode 1038.000000, reward total was -19.000000. running mean: -20.008554\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.018468\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.028284\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -20.038001\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.047621\n",
            "resetting env. episode 1043.000000, reward total was -20.000000. running mean: -20.047145\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -20.046673\n",
            "resetting env. episode 1045.000000, reward total was -19.000000. running mean: -20.036206\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.045844\n",
            "resetting env. episode 1047.000000, reward total was -20.000000. running mean: -20.045386\n",
            "resetting env. episode 1048.000000, reward total was -20.000000. running mean: -20.044932\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.054483\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.063938\n",
            "resetting env. episode 1051.000000, reward total was -19.000000. running mean: -20.053299\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -20.052766\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.062238\n",
            "resetting env. episode 1054.000000, reward total was -17.000000. running mean: -20.031616\n",
            "resetting env. episode 1055.000000, reward total was -19.000000. running mean: -20.021299\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.031086\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.040776\n",
            "resetting env. episode 1058.000000, reward total was -18.000000. running mean: -20.020368\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -20.020164\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.029962\n",
            "resetting env. episode 1061.000000, reward total was -20.000000. running mean: -20.029663\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.039366\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -20.048973\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -20.058483\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.067898\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -20.077219\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -20.086447\n",
            "resetting env. episode 1068.000000, reward total was -18.000000. running mean: -20.065582\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.074927\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -20.074177\n",
            "resetting env. episode 1071.000000, reward total was -18.000000. running mean: -20.053435\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -20.062901\n",
            "resetting env. episode 1073.000000, reward total was -20.000000. running mean: -20.062272\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.071649\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.080933\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.090124\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.099222\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.108230\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.117148\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -20.115976\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.124817\n",
            "resetting env. episode 1082.000000, reward total was -20.000000. running mean: -20.123568\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -20.122333\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.131109\n",
            "resetting env. episode 1085.000000, reward total was -19.000000. running mean: -20.119798\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.128600\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.137314\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.145941\n",
            "resetting env. episode 1089.000000, reward total was -19.000000. running mean: -20.134482\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.143137\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.151706\n",
            "resetting env. episode 1092.000000, reward total was -18.000000. running mean: -20.130189\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.138887\n",
            "resetting env. episode 1094.000000, reward total was -17.000000. running mean: -20.107498\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.116423\n",
            "resetting env. episode 1096.000000, reward total was -19.000000. running mean: -20.105259\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.114206\n",
            "resetting env. episode 1098.000000, reward total was -20.000000. running mean: -20.113064\n",
            "resetting env. episode 1099.000000, reward total was -19.000000. running mean: -20.101933\n",
            "resetting env. episode 1100.000000, reward total was -19.000000. running mean: -20.090914\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -20.100005\n",
            "resetting env. episode 1102.000000, reward total was -18.000000. running mean: -20.079005\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.088215\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -20.097333\n",
            "resetting env. episode 1105.000000, reward total was -19.000000. running mean: -20.086359\n",
            "resetting env. episode 1106.000000, reward total was -19.000000. running mean: -20.075496\n",
            "resetting env. episode 1107.000000, reward total was -19.000000. running mean: -20.064741\n",
            "resetting env. episode 1108.000000, reward total was -20.000000. running mean: -20.064093\n",
            "resetting env. episode 1109.000000, reward total was -19.000000. running mean: -20.053452\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.062918\n",
            "resetting env. episode 1111.000000, reward total was -19.000000. running mean: -20.052289\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -20.051766\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.061248\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -20.060636\n",
            "resetting env. episode 1115.000000, reward total was -19.000000. running mean: -20.050029\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.059529\n",
            "resetting env. episode 1117.000000, reward total was -20.000000. running mean: -20.058934\n",
            "resetting env. episode 1118.000000, reward total was -20.000000. running mean: -20.058344\n",
            "resetting env. episode 1119.000000, reward total was -20.000000. running mean: -20.057761\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -20.057183\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -20.066611\n",
            "resetting env. episode 1122.000000, reward total was -20.000000. running mean: -20.065945\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.075286\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.084533\n",
            "resetting env. episode 1125.000000, reward total was -19.000000. running mean: -20.073688\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.082951\n",
            "resetting env. episode 1127.000000, reward total was -18.000000. running mean: -20.062121\n",
            "resetting env. episode 1128.000000, reward total was -19.000000. running mean: -20.051500\n",
            "resetting env. episode 1129.000000, reward total was -18.000000. running mean: -20.030985\n",
            "resetting env. episode 1130.000000, reward total was -21.000000. running mean: -20.040675\n",
            "resetting env. episode 1131.000000, reward total was -16.000000. running mean: -20.000268\n",
            "resetting env. episode 1132.000000, reward total was -19.000000. running mean: -19.990266\n",
            "resetting env. episode 1133.000000, reward total was -20.000000. running mean: -19.990363\n",
            "resetting env. episode 1134.000000, reward total was -21.000000. running mean: -20.000460\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -20.000455\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -20.000450\n",
            "resetting env. episode 1137.000000, reward total was -19.000000. running mean: -19.990446\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -20.000541\n",
            "resetting env. episode 1139.000000, reward total was -19.000000. running mean: -19.990536\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.000631\n",
            "resetting env. episode 1141.000000, reward total was -19.000000. running mean: -19.990624\n",
            "resetting env. episode 1142.000000, reward total was -20.000000. running mean: -19.990718\n",
            "resetting env. episode 1143.000000, reward total was -20.000000. running mean: -19.990811\n",
            "resetting env. episode 1144.000000, reward total was -18.000000. running mean: -19.970903\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -19.981194\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -19.991382\n",
            "resetting env. episode 1147.000000, reward total was -20.000000. running mean: -19.991468\n",
            "resetting env. episode 1148.000000, reward total was -21.000000. running mean: -20.001553\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -20.001538\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -20.011522\n",
            "resetting env. episode 1151.000000, reward total was -20.000000. running mean: -20.011407\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -20.011293\n",
            "resetting env. episode 1153.000000, reward total was -18.000000. running mean: -19.991180\n",
            "resetting env. episode 1154.000000, reward total was -20.000000. running mean: -19.991268\n",
            "resetting env. episode 1155.000000, reward total was -20.000000. running mean: -19.991356\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.001442\n",
            "resetting env. episode 1157.000000, reward total was -20.000000. running mean: -20.001428\n",
            "resetting env. episode 1158.000000, reward total was -18.000000. running mean: -19.981413\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -19.981599\n",
            "resetting env. episode 1160.000000, reward total was -19.000000. running mean: -19.971783\n",
            "resetting env. episode 1161.000000, reward total was -18.000000. running mean: -19.952065\n",
            "resetting env. episode 1162.000000, reward total was -18.000000. running mean: -19.932545\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -19.933219\n",
            "resetting env. episode 1164.000000, reward total was -19.000000. running mean: -19.923887\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -19.934648\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -19.935302\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -19.945949\n",
            "resetting env. episode 1168.000000, reward total was -20.000000. running mean: -19.946489\n",
            "resetting env. episode 1169.000000, reward total was -19.000000. running mean: -19.937024\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -19.947654\n",
            "resetting env. episode 1171.000000, reward total was -20.000000. running mean: -19.948178\n",
            "resetting env. episode 1172.000000, reward total was -19.000000. running mean: -19.938696\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -19.949309\n",
            "resetting env. episode 1174.000000, reward total was -20.000000. running mean: -19.949816\n",
            "resetting env. episode 1175.000000, reward total was -20.000000. running mean: -19.950318\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -19.960814\n",
            "resetting env. episode 1177.000000, reward total was -19.000000. running mean: -19.951206\n",
            "resetting env. episode 1178.000000, reward total was -20.000000. running mean: -19.951694\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -19.962177\n",
            "resetting env. episode 1180.000000, reward total was -19.000000. running mean: -19.952556\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -19.963030\n",
            "resetting env. episode 1182.000000, reward total was -21.000000. running mean: -19.973400\n",
            "resetting env. episode 1183.000000, reward total was -16.000000. running mean: -19.933666\n",
            "resetting env. episode 1184.000000, reward total was -19.000000. running mean: -19.924329\n",
            "resetting env. episode 1185.000000, reward total was -17.000000. running mean: -19.895086\n",
            "resetting env. episode 1186.000000, reward total was -17.000000. running mean: -19.866135\n",
            "resetting env. episode 1187.000000, reward total was -20.000000. running mean: -19.867474\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -19.878799\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -19.890011\n",
            "resetting env. episode 1190.000000, reward total was -19.000000. running mean: -19.881111\n",
            "resetting env. episode 1191.000000, reward total was -19.000000. running mean: -19.872300\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -19.883577\n",
            "resetting env. episode 1193.000000, reward total was -18.000000. running mean: -19.864741\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -19.866093\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -19.877433\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -19.878658\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -19.889872\n",
            "resetting env. episode 1198.000000, reward total was -19.000000. running mean: -19.880973\n",
            "resetting env. episode 1199.000000, reward total was -21.000000. running mean: -19.892163\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -19.903242\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -19.904209\n",
            "resetting env. episode 1202.000000, reward total was -20.000000. running mean: -19.905167\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -19.916115\n",
            "resetting env. episode 1204.000000, reward total was -20.000000. running mean: -19.916954\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -19.927785\n",
            "resetting env. episode 1206.000000, reward total was -19.000000. running mean: -19.918507\n",
            "resetting env. episode 1207.000000, reward total was -18.000000. running mean: -19.899322\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -19.910329\n",
            "resetting env. episode 1209.000000, reward total was -19.000000. running mean: -19.901225\n",
            "resetting env. episode 1210.000000, reward total was -17.000000. running mean: -19.872213\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -19.883491\n",
            "resetting env. episode 1212.000000, reward total was -17.000000. running mean: -19.854656\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -19.866109\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -19.867448\n",
            "resetting env. episode 1215.000000, reward total was -20.000000. running mean: -19.868774\n",
            "resetting env. episode 1216.000000, reward total was -21.000000. running mean: -19.880086\n",
            "resetting env. episode 1217.000000, reward total was -19.000000. running mean: -19.871285\n",
            "resetting env. episode 1218.000000, reward total was -20.000000. running mean: -19.872572\n",
            "resetting env. episode 1219.000000, reward total was -17.000000. running mean: -19.843847\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -19.845408\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -19.856954\n",
            "resetting env. episode 1222.000000, reward total was -20.000000. running mean: -19.858385\n",
            "resetting env. episode 1223.000000, reward total was -19.000000. running mean: -19.849801\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -19.861303\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -19.862690\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -19.874063\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -19.885322\n",
            "resetting env. episode 1228.000000, reward total was -19.000000. running mean: -19.876469\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -19.887704\n",
            "resetting env. episode 1230.000000, reward total was -16.000000. running mean: -19.848827\n",
            "resetting env. episode 1231.000000, reward total was -19.000000. running mean: -19.840339\n",
            "resetting env. episode 1232.000000, reward total was -19.000000. running mean: -19.831936\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -19.843616\n",
            "resetting env. episode 1234.000000, reward total was -20.000000. running mean: -19.845180\n",
            "resetting env. episode 1235.000000, reward total was -19.000000. running mean: -19.836728\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -19.848361\n",
            "resetting env. episode 1237.000000, reward total was -19.000000. running mean: -19.839877\n",
            "resetting env. episode 1238.000000, reward total was -19.000000. running mean: -19.831479\n",
            "resetting env. episode 1239.000000, reward total was -19.000000. running mean: -19.823164\n",
            "resetting env. episode 1240.000000, reward total was -20.000000. running mean: -19.824932\n",
            "resetting env. episode 1241.000000, reward total was -20.000000. running mean: -19.826683\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -19.838416\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -19.850032\n",
            "resetting env. episode 1244.000000, reward total was -21.000000. running mean: -19.861531\n",
            "resetting env. episode 1245.000000, reward total was -20.000000. running mean: -19.862916\n",
            "resetting env. episode 1246.000000, reward total was -19.000000. running mean: -19.854287\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -19.865744\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -19.877087\n",
            "resetting env. episode 1249.000000, reward total was -20.000000. running mean: -19.878316\n",
            "resetting env. episode 1250.000000, reward total was -21.000000. running mean: -19.889533\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -19.900637\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -19.911631\n",
            "resetting env. episode 1253.000000, reward total was -20.000000. running mean: -19.912515\n",
            "resetting env. episode 1254.000000, reward total was -20.000000. running mean: -19.913390\n",
            "resetting env. episode 1255.000000, reward total was -13.000000. running mean: -19.844256\n",
            "resetting env. episode 1256.000000, reward total was -20.000000. running mean: -19.845813\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -19.857355\n",
            "resetting env. episode 1258.000000, reward total was -20.000000. running mean: -19.858781\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -19.870194\n",
            "resetting env. episode 1260.000000, reward total was -19.000000. running mean: -19.861492\n",
            "resetting env. episode 1261.000000, reward total was -20.000000. running mean: -19.862877\n",
            "resetting env. episode 1262.000000, reward total was -18.000000. running mean: -19.844248\n",
            "resetting env. episode 1263.000000, reward total was -20.000000. running mean: -19.845805\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -19.857347\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -19.868774\n",
            "resetting env. episode 1266.000000, reward total was -20.000000. running mean: -19.870086\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -19.881385\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -19.882571\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -19.893746\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -19.904808\n",
            "resetting env. episode 1271.000000, reward total was -19.000000. running mean: -19.895760\n",
            "resetting env. episode 1272.000000, reward total was -21.000000. running mean: -19.906803\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -19.907735\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -19.918657\n",
            "resetting env. episode 1275.000000, reward total was -18.000000. running mean: -19.899471\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -19.910476\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -19.921371\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -19.932158\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -19.942836\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -19.943408\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -19.953974\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -19.964434\n",
            "resetting env. episode 1283.000000, reward total was -19.000000. running mean: -19.954789\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -19.955242\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -19.965689\n",
            "resetting env. episode 1286.000000, reward total was -20.000000. running mean: -19.966032\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -19.976372\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -19.976608\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -19.986842\n",
            "resetting env. episode 1290.000000, reward total was -18.000000. running mean: -19.966974\n",
            "resetting env. episode 1291.000000, reward total was -19.000000. running mean: -19.957304\n",
            "resetting env. episode 1292.000000, reward total was -20.000000. running mean: -19.957731\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -19.968154\n",
            "resetting env. episode 1294.000000, reward total was -18.000000. running mean: -19.948472\n",
            "resetting env. episode 1295.000000, reward total was -20.000000. running mean: -19.948987\n",
            "resetting env. episode 1296.000000, reward total was -17.000000. running mean: -19.919497\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -19.930302\n",
            "resetting env. episode 1298.000000, reward total was -20.000000. running mean: -19.930999\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -19.941689\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -19.952273\n",
            "resetting env. episode 1301.000000, reward total was -19.000000. running mean: -19.942750\n",
            "resetting env. episode 1302.000000, reward total was -17.000000. running mean: -19.913322\n",
            "resetting env. episode 1303.000000, reward total was -19.000000. running mean: -19.904189\n",
            "resetting env. episode 1304.000000, reward total was -20.000000. running mean: -19.905147\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -19.906096\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -19.917035\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -19.927864\n",
            "resetting env. episode 1308.000000, reward total was -20.000000. running mean: -19.928586\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -19.939300\n",
            "resetting env. episode 1310.000000, reward total was -20.000000. running mean: -19.939907\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -19.950508\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -19.961003\n",
            "resetting env. episode 1313.000000, reward total was -20.000000. running mean: -19.961393\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -19.971779\n",
            "resetting env. episode 1315.000000, reward total was -18.000000. running mean: -19.952061\n",
            "resetting env. episode 1316.000000, reward total was -20.000000. running mean: -19.952540\n",
            "resetting env. episode 1317.000000, reward total was -19.000000. running mean: -19.943015\n",
            "resetting env. episode 1318.000000, reward total was -20.000000. running mean: -19.943585\n",
            "resetting env. episode 1319.000000, reward total was -20.000000. running mean: -19.944149\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -19.954708\n",
            "resetting env. episode 1321.000000, reward total was -21.000000. running mean: -19.965160\n",
            "resetting env. episode 1322.000000, reward total was -19.000000. running mean: -19.955509\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -19.965954\n",
            "resetting env. episode 1324.000000, reward total was -20.000000. running mean: -19.966294\n",
            "resetting env. episode 1325.000000, reward total was -17.000000. running mean: -19.936631\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -19.947265\n",
            "resetting env. episode 1327.000000, reward total was -18.000000. running mean: -19.927792\n",
            "resetting env. episode 1328.000000, reward total was -18.000000. running mean: -19.908514\n",
            "resetting env. episode 1329.000000, reward total was -16.000000. running mean: -19.869429\n",
            "resetting env. episode 1330.000000, reward total was -19.000000. running mean: -19.860735\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -19.862128\n",
            "resetting env. episode 1332.000000, reward total was -17.000000. running mean: -19.833506\n",
            "resetting env. episode 1333.000000, reward total was -17.000000. running mean: -19.805171\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -19.817120\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -19.828948\n",
            "resetting env. episode 1336.000000, reward total was -21.000000. running mean: -19.840659\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -19.842252\n",
            "resetting env. episode 1338.000000, reward total was -20.000000. running mean: -19.843830\n",
            "resetting env. episode 1339.000000, reward total was -20.000000. running mean: -19.845391\n",
            "resetting env. episode 1340.000000, reward total was -20.000000. running mean: -19.846938\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -19.858468\n",
            "resetting env. episode 1342.000000, reward total was -21.000000. running mean: -19.869884\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -19.881185\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -19.882373\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -19.893549\n",
            "resetting env. episode 1346.000000, reward total was -20.000000. running mean: -19.894614\n",
            "resetting env. episode 1347.000000, reward total was -19.000000. running mean: -19.885667\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -19.896811\n",
            "resetting env. episode 1349.000000, reward total was -21.000000. running mean: -19.907843\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -19.918764\n",
            "resetting env. episode 1351.000000, reward total was -19.000000. running mean: -19.909577\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -19.920481\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -19.931276\n",
            "resetting env. episode 1354.000000, reward total was -18.000000. running mean: -19.911963\n",
            "resetting env. episode 1355.000000, reward total was -19.000000. running mean: -19.902844\n",
            "resetting env. episode 1356.000000, reward total was -20.000000. running mean: -19.903815\n",
            "resetting env. episode 1357.000000, reward total was -19.000000. running mean: -19.894777\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -19.905829\n",
            "resetting env. episode 1359.000000, reward total was -17.000000. running mean: -19.876771\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -19.888003\n",
            "resetting env. episode 1361.000000, reward total was -19.000000. running mean: -19.879123\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -19.890332\n",
            "resetting env. episode 1363.000000, reward total was -19.000000. running mean: -19.881429\n",
            "resetting env. episode 1364.000000, reward total was -20.000000. running mean: -19.882614\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -19.893788\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -19.904850\n",
            "resetting env. episode 1367.000000, reward total was -19.000000. running mean: -19.895802\n",
            "resetting env. episode 1368.000000, reward total was -19.000000. running mean: -19.886844\n",
            "resetting env. episode 1369.000000, reward total was -17.000000. running mean: -19.857975\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -19.859396\n",
            "resetting env. episode 1371.000000, reward total was -20.000000. running mean: -19.860802\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -19.872194\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -19.883472\n",
            "resetting env. episode 1374.000000, reward total was -20.000000. running mean: -19.884637\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -19.895791\n",
            "resetting env. episode 1376.000000, reward total was -17.000000. running mean: -19.866833\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -19.878164\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -19.889383\n",
            "resetting env. episode 1379.000000, reward total was -20.000000. running mean: -19.890489\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -19.891584\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -19.892668\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -19.903742\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -19.914704\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -19.925557\n",
            "resetting env. episode 1385.000000, reward total was -20.000000. running mean: -19.926302\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -19.937039\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -19.937668\n",
            "resetting env. episode 1388.000000, reward total was -20.000000. running mean: -19.938291\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -19.948909\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -19.949419\n",
            "resetting env. episode 1391.000000, reward total was -19.000000. running mean: -19.939925\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -19.950526\n",
            "resetting env. episode 1393.000000, reward total was -20.000000. running mean: -19.951021\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -19.951511\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -19.961995\n",
            "resetting env. episode 1396.000000, reward total was -18.000000. running mean: -19.942375\n",
            "resetting env. episode 1397.000000, reward total was -20.000000. running mean: -19.942952\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -19.953522\n",
            "resetting env. episode 1399.000000, reward total was -20.000000. running mean: -19.953987\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -19.964447\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -19.964803\n",
            "resetting env. episode 1402.000000, reward total was -20.000000. running mean: -19.965155\n",
            "resetting env. episode 1403.000000, reward total was -20.000000. running mean: -19.965503\n",
            "resetting env. episode 1404.000000, reward total was -20.000000. running mean: -19.965848\n",
            "resetting env. episode 1405.000000, reward total was -18.000000. running mean: -19.946190\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -19.956728\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -19.967160\n",
            "resetting env. episode 1408.000000, reward total was -21.000000. running mean: -19.977489\n",
            "resetting env. episode 1409.000000, reward total was -20.000000. running mean: -19.977714\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -19.987937\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -19.998057\n",
            "resetting env. episode 1412.000000, reward total was -18.000000. running mean: -19.978077\n",
            "resetting env. episode 1413.000000, reward total was -19.000000. running mean: -19.968296\n",
            "resetting env. episode 1414.000000, reward total was -19.000000. running mean: -19.958613\n",
            "resetting env. episode 1415.000000, reward total was -19.000000. running mean: -19.949027\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -19.959537\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -19.959941\n",
            "resetting env. episode 1418.000000, reward total was -20.000000. running mean: -19.960342\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -19.970738\n",
            "resetting env. episode 1420.000000, reward total was -21.000000. running mean: -19.981031\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -19.981221\n",
            "resetting env. episode 1422.000000, reward total was -20.000000. running mean: -19.981409\n",
            "resetting env. episode 1423.000000, reward total was -20.000000. running mean: -19.981594\n",
            "resetting env. episode 1424.000000, reward total was -19.000000. running mean: -19.971779\n",
            "resetting env. episode 1425.000000, reward total was -18.000000. running mean: -19.952061\n",
            "resetting env. episode 1426.000000, reward total was -19.000000. running mean: -19.942540\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -19.953115\n",
            "resetting env. episode 1428.000000, reward total was -20.000000. running mean: -19.953584\n",
            "resetting env. episode 1429.000000, reward total was -19.000000. running mean: -19.944048\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -19.944607\n",
            "resetting env. episode 1431.000000, reward total was -19.000000. running mean: -19.935161\n",
            "resetting env. episode 1432.000000, reward total was -20.000000. running mean: -19.935810\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -19.946452\n",
            "resetting env. episode 1434.000000, reward total was -20.000000. running mean: -19.946987\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -19.957517\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -19.957942\n",
            "resetting env. episode 1437.000000, reward total was -18.000000. running mean: -19.938363\n",
            "resetting env. episode 1438.000000, reward total was -20.000000. running mean: -19.938979\n",
            "resetting env. episode 1439.000000, reward total was -19.000000. running mean: -19.929589\n",
            "resetting env. episode 1440.000000, reward total was -19.000000. running mean: -19.920293\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -19.931090\n",
            "resetting env. episode 1442.000000, reward total was -20.000000. running mean: -19.931779\n",
            "resetting env. episode 1443.000000, reward total was -20.000000. running mean: -19.932462\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -19.943137\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -19.943706\n",
            "resetting env. episode 1446.000000, reward total was -20.000000. running mean: -19.944269\n",
            "resetting env. episode 1447.000000, reward total was -18.000000. running mean: -19.924826\n",
            "resetting env. episode 1448.000000, reward total was -21.000000. running mean: -19.935578\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -19.936222\n",
            "resetting env. episode 1450.000000, reward total was -20.000000. running mean: -19.936860\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -19.947491\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -19.958016\n",
            "resetting env. episode 1453.000000, reward total was -20.000000. running mean: -19.958436\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -19.968852\n",
            "resetting env. episode 1455.000000, reward total was -20.000000. running mean: -19.969163\n",
            "resetting env. episode 1456.000000, reward total was -20.000000. running mean: -19.969471\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -19.979777\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -19.989979\n",
            "resetting env. episode 1459.000000, reward total was -17.000000. running mean: -19.960079\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -19.970478\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -19.980774\n",
            "resetting env. episode 1462.000000, reward total was -19.000000. running mean: -19.970966\n",
            "resetting env. episode 1463.000000, reward total was -20.000000. running mean: -19.971256\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -19.981544\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -19.981728\n",
            "resetting env. episode 1466.000000, reward total was -18.000000. running mean: -19.961911\n",
            "resetting env. episode 1467.000000, reward total was -19.000000. running mean: -19.952292\n",
            "resetting env. episode 1468.000000, reward total was -19.000000. running mean: -19.942769\n",
            "resetting env. episode 1469.000000, reward total was -20.000000. running mean: -19.943341\n",
            "resetting env. episode 1470.000000, reward total was -18.000000. running mean: -19.923908\n",
            "resetting env. episode 1471.000000, reward total was -20.000000. running mean: -19.924669\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -19.935422\n",
            "resetting env. episode 1473.000000, reward total was -18.000000. running mean: -19.916068\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -19.926907\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -19.937638\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -19.948262\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -19.958779\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -19.969191\n",
            "resetting env. episode 1479.000000, reward total was -20.000000. running mean: -19.969499\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -19.969804\n",
            "resetting env. episode 1481.000000, reward total was -20.000000. running mean: -19.970106\n",
            "resetting env. episode 1482.000000, reward total was -20.000000. running mean: -19.970405\n",
            "resetting env. episode 1483.000000, reward total was -20.000000. running mean: -19.970701\n",
            "resetting env. episode 1484.000000, reward total was -20.000000. running mean: -19.970994\n",
            "resetting env. episode 1485.000000, reward total was -19.000000. running mean: -19.961284\n",
            "resetting env. episode 1486.000000, reward total was -19.000000. running mean: -19.951671\n",
            "resetting env. episode 1487.000000, reward total was -19.000000. running mean: -19.942155\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -19.942733\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -19.943306\n",
            "resetting env. episode 1490.000000, reward total was -18.000000. running mean: -19.923873\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -19.924634\n",
            "resetting env. episode 1492.000000, reward total was -20.000000. running mean: -19.925388\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -19.936134\n",
            "resetting env. episode 1494.000000, reward total was -20.000000. running mean: -19.936772\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -19.937405\n",
            "resetting env. episode 1496.000000, reward total was -20.000000. running mean: -19.938031\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -19.948650\n",
            "resetting env. episode 1498.000000, reward total was -20.000000. running mean: -19.949164\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -19.949672\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -19.960176\n",
            "CPU times: user 1h 51min 59s, sys: 35min 41s, total: 2h 27min 41s\n",
            "Wall time: 1h 16min 4s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "23db194e-5577-43ea-c2cb-ce59969527f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}